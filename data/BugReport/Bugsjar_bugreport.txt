ACCUMULO-9396979b$$bulk imported files showing up in metadata after bulk import fails$$Bulk import fails.  The file is moved to the failures directory.  But references in the !METADATA table remain.
ACCUMULO-ea2f9856$$bulk imported files showing up in metadata after bulk import fails$$Bulk import fails.  The file is moved to the failures directory.  But references in the !METADATA table remain.
ACCUMULO-25cf3ccd$$Authorizations has inconsistent serialization$$The same set of authorizations may not serialize to the same value each time, if specified in a different order when constructed (like new Authorizations("a", "b") and new Authorizations("b", "a")), because serialization reproducibility depends on the insert order in the underlying HashSet.  So, one could get the following to happen: {code:java} true == auths1.equals(auths2) && !auths1.serialize().equals(auths2.serialize()); {code}
ACCUMULO-474b2577$$stop-all doesn't work: Error BAD_CREDENTIALS for user root$${noformat}   bin/accumulo admin stopAll 2013-02-27 14:56:14,072 [util.Admin] ERROR: org.apache.accumulo.core.client.AccumuloSecurityException: Error BAD_CREDENTIALS for user root - Username or Password is Invalid org.apache.accumulo.core.client.AccumuloSecurityException: Error BAD_CREDENTIALS for user root - Username or Password is Invalid 	at org.apache.accumulo.core.client.impl.MasterClient.execute(MasterClient.java:119) 	at org.apache.accumulo.server.util.Admin.stopServer(Admin.java:107) 	at org.apache.accumulo.server.util.Admin.main(Admin.java:95) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:597) 	at org.apache.accumulo.start.Main 1.run(Main.java:97) 	at java.lang.Thread.run(Thread.java:662) Caused by: ThriftSecurityException(user:root, code:BAD_CREDENTIALS) 	at org.apache.accumulo.core.master.thrift.MasterClientService shutdown_result shutdown_resultStandardScheme.read(MasterClientService.java:8424) 	at org.apache.accumulo.core.master.thrift.MasterClientService shutdown_result shutdown_resultStandardScheme.read(MasterClientService.java:8410) 	at org.apache.accumulo.core.master.thrift.MasterClientService shutdown_result.read(MasterClientService.java:8360) 	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78) 	at org.apache.accumulo.core.master.thrift.MasterClientService Client.recv_shutdown(MasterClientService.java:312) 	at org.apache.accumulo.core.master.thrift.MasterClientService Client.shutdown(MasterClientService.java:297) 	at org.apache.accumulo.server.util.Admin 1.execute(Admin.java:110) 	at org.apache.accumulo.server.util.Admin 1.execute(Admin.java:107) 	at org.apache.accumulo.core.client.impl.MasterClient.execute(MasterClient.java:113) 	... 8 more  {noformat}
ACCUMULO-742960f1$$ProxyServer does not set column information on BatchScanner$$The createScanner method uses the options from the thrift request to call fetchColumn() and fetchColumnFamily(). The createBatchScanner should be doing have the same feature, though the statements are absent from the code.
ACCUMULO-cfbf5999$$ProxyServer does not set column information on BatchScanner$$The createScanner method uses the options from the thrift request to call fetchColumn() and fetchColumnFamily(). The createBatchScanner should be doing have the same feature, though the statements are absent from the code.
ACCUMULO-e29dc4f5$$The update() method on the ProxyServer should throw a MutationsRejectedException$$None
ACCUMULO-9476b877$$"du" on a table without files does not report$${noformat} shell> createtable t shell> du t shell> {noformat}  expected:  {noformat} shell> du t              0 t shell> {noformat}
ACCUMULO-c489d866$$"du" on a table without files does not report$${noformat} shell> createtable t shell> du t shell> {noformat}  expected:  {noformat} shell> du t              0 t shell> {noformat}
ACCUMULO-813109d7$$Verify all methods in the ProxyService that take table names actually throw TableNotFoundException when the table is missing.$$None
ACCUMULO-d9ab8449$$Don't cache credentials in client-side Connector$$AuthenticationToken objects are Destroyable. However, this cannot be exercised properly in the client code, because the Connector immediately serializes the credentials and stores them as long as the Connector lives.  It should be possible to destroy a token after creating a Connector, and thereby forcing any further RPC calls initiated by that Connector to fail to authenticate. This means that serialization on the client side to a TCredentials object needs to occur just before the RPC call.
ACCUMULO-6ff92b12$$Accumulo Shell does not respect 'exit' when executing file$$If there is an {{exit}} statement in the file given via {{accumulo shell -f file}}, the execution seems to skip it and go on to the next command instead of terminating.  To recreate: {noformat} [mike@home ~] cat bug.accumulo exit scan -np -t !METADATA [mike@home ~] bin/accumulo shell -f /home/mike/bug.accumulo {noformat}  Expected output: None Actual output: A full scan of the !METADATA
ACCUMULO-ef0f6ddc$$Accumulo Shell does not respect 'exit' when executing file$$If there is an {{exit}} statement in the file given via {{accumulo shell -f file}}, the execution seems to skip it and go on to the next command instead of terminating.  To recreate: {noformat} [mike@home ~] cat bug.accumulo exit scan -np -t !METADATA [mike@home ~] bin/accumulo shell -f /home/mike/bug.accumulo {noformat}  Expected output: None Actual output: A full scan of the !METADATA
ACCUMULO-4d10c92f$$Shell's setiter is not informative when using a bad class name$$In the shell, I did setiter using a class that wasn't found. Rather then a message about it not being found, I just get told that I have an invalid argument. Even turning on debug, I had to use the stack trace to figure out why it was erroring.
ACCUMULO-6c565dfb$$Shell's setiter is not informative when using a bad class name$$In the shell, I did setiter using a class that wasn't found. Rather then a message about it not being found, I just get told that I have an invalid argument. Even turning on debug, I had to use the stack trace to figure out why it was erroring.
ACCUMULO-994df698$$MockTable's addMutation does not check for empty mutation$$When calling addMutation or addMutations on a MockBatchWriter, the updates stored in the mutation are iterated over then committed in the MockTable class.   When this occurs in the TabletServerBatchWriter (eventually called from the BatchWriterImpl), however, the mutation size is first checked and if the mutation size is 0, an IllegalArgumentException is thrown.  In practice, if you have code that tries to submit an empty mutation to a BatchWriter, it will fail and throw an exception in the real world, but this will not be caught in tests against MockAccumulo.
ACCUMULO-b082fc1e$$MockTable's addMutation does not check for empty mutation$$When calling addMutation or addMutations on a MockBatchWriter, the updates stored in the mutation are iterated over then committed in the MockTable class.   When this occurs in the TabletServerBatchWriter (eventually called from the BatchWriterImpl), however, the mutation size is first checked and if the mutation size is 0, an IllegalArgumentException is thrown.  In practice, if you have code that tries to submit an empty mutation to a BatchWriter, it will fail and throw an exception in the real world, but this will not be caught in tests against MockAccumulo.
ACCUMULO-fb25913c$$AccumuloVFSClassloader incorrectly treats folders as folders of jar files$$Specifying a directory of classes is incorrectly interpreted as a directory of jars in the general.dynamic.classpaths configuration property.  Example: adding a path such as *_ ACCUMULO_HOME/core/target/classes_* gets incorrectly interpreted as *_ ACCUMULO_HOME/core/target/classes/\*_* and evaluates to *_ ACCUMULO_HOME/core/target/classes/org_* and *_ ACCUMULO_HOME/core/target/classes/META-INF_*, but *NOT* to *_ ACCUMULO_HOME/core/target/classes_* as expected.
ACCUMULO-dc95cb69$$FileOperations expects RFile filenames to contain only 1 dot.$$If I attempt to create or read an RFile that contains more than 1 dot in the filename, FileOperations throws an IllegalArgumentException("File name " + name + " has no extension"). Please allow creation/import of RFiles that have more than 1 dot in the filename.
ACCUMULO-df4b1985$$FileOperations expects RFile filenames to contain only 1 dot.$$If I attempt to create or read an RFile that contains more than 1 dot in the filename, FileOperations throws an IllegalArgumentException("File name " + name + " has no extension"). Please allow creation/import of RFiles that have more than 1 dot in the filename.
ACCUMULO-b007b22e$$Combiner default behavior is dangerous$$Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.    Also classes that extend combiner should call super.validateOptions().
ACCUMULO-0cf2ff72$$Remove username from initialization$$This is an artifact from a brief transition area during the 1.5 development. We have a flag for the user to set what the root username is, except it's never used. We should remove both the variable and the flag for it.
ACCUMULO-13eb19c2$$AccumuloInputFormat cannot fetch empty column family$$The following fails: {code:java} Job job = new Job(); HashSet<Pair<Text,Text>> cols = new HashSet<Pair<Text,Text>>(); cols.add(new Pair<Text,Text>(new Text(""), null)); AccumuloInputFormat.fetchColumns(job, cols); Set<Pair<Text,Text>> setCols = AccumuloInputFormat.getFetchedColumns(job); assertEquals(cols.size(), setCols.size()); {code}
ACCUMULO-872b6db3$$ColumnVisibility parse tree nodes do not have correct location offsets for AND and OR nodes$$Trying to do some transformations on visibility strings and running into issues working with the parse tree:  Clojure 1.5.1 user=> (import [org.apache.accumulo.core.security ColumnVisibility]) org.apache.accumulo.core.security.ColumnVisibility user=> (def vis (ColumnVisibility. "(W)|(U|V)")) #'user/vis user=> (.getTermStart (first (.getChildren (.getParseTree vis)))) 1 user=> (.getTermEnd (first (.getChildren (.getParseTree vis)))) 2 user=> (.getTermStart (second (.getChildren (.getParseTree vis)))) 0 user=> (.getTermEnd (second (.getChildren (.getParseTree vis)))) 8  Shouldn't those last two be 5 and 8?
ACCUMULO-941e3cb1$$Resolve table name to table id once in Accumulo input format$$AccumuloInputFormat (and I suspect AccumuloOutputFormat) sends the table name to each mapper.  The mapper uses this table name to create a scanner.  In the case of the following events a map reduce job could read from two different table ids.      # start M/R job reading table A  # rename table A (tableId=1) to table C  # rename table B (tableId=2) to table A  If the input format passed table id 1 to the mappers, then the renames would not cause a problem.
ACCUMULO-2f0643a9$$Off-by-one error in FamilyIntersectingIterator$$In the buildDocKey() function within the FamilyIntersectingIterator there is a bug that shortens the docID by 1.  This causes the wrong doc's data to be returned in the results of a query using this Iterator.
ACCUMULO-efef09b0$$Off-by-one error in FamilyIntersectingIterator$$In the buildDocKey() function within the FamilyIntersectingIterator there is a bug that shortens the docID by 1.  This causes the wrong doc's data to be returned in the results of a query using this Iterator.
ACCUMULO-3143b9c5$$delete mutations not working through the Proxy$$Aru Sahni writes:  {quote} I'm new to Accumulo and am still trying to wrap my head around its ways. To further that challenge, I'm using Pyaccumulo, which doesn't present much in terms of available reference material.  Right now I'm trying to understand how Accumulo manages record (key-value pair) deletions.  conn = Accumulo(host, port, user, password) table = 'test_table' conn.create_table(table) writer = conn.create_batch_writer(table) mut = Mutation('mut_01') mut.put(cf='item', cq='name', value='car') writer.add_mutation(mut) writer.close() conn.close()  Will generate a record (found via a shell scan):  mut_01 item:name []    car  However the subsequent mutation...  writer = conn.create_batch_writer(table) mut = Mutation('mut_01') mut.put(cf='item', cq='name', is_delete=True) writer.add_mutation(mut) writer.close()  Results in:  mut_01 item:name []  How should one expect the deleted row to be represented? That record sticks around even after I force a compaction of the table.  I was expecting it to not show up in any iterators, or at least provide an easy way to see if the cell has been deleted. {quote}  [~ecn] has confirmed the problem.
ACCUMULO-8ec4cb84$$delete mutations not working through the Proxy$$Aru Sahni writes:  {quote} I'm new to Accumulo and am still trying to wrap my head around its ways. To further that challenge, I'm using Pyaccumulo, which doesn't present much in terms of available reference material.  Right now I'm trying to understand how Accumulo manages record (key-value pair) deletions.  conn = Accumulo(host, port, user, password) table = 'test_table' conn.create_table(table) writer = conn.create_batch_writer(table) mut = Mutation('mut_01') mut.put(cf='item', cq='name', value='car') writer.add_mutation(mut) writer.close() conn.close()  Will generate a record (found via a shell scan):  mut_01 item:name []    car  However the subsequent mutation...  writer = conn.create_batch_writer(table) mut = Mutation('mut_01') mut.put(cf='item', cq='name', is_delete=True) writer.add_mutation(mut) writer.close()  Results in:  mut_01 item:name []  How should one expect the deleted row to be represented? That record sticks around even after I force a compaction of the table.  I was expecting it to not show up in any iterators, or at least provide an easy way to see if the cell has been deleted. {quote}  [~ecn] has confirmed the problem.
ACCUMULO-6dbbdc21$$RegExFilter deepCopy NullPointerException$$If any of the regex matcher objects are null (i.e. for example, if you only specify a regex for the column family), the deepCopy call will throw a NullPointerException.
ACCUMULO-cd7feb4d$$RegExFilter deepCopy NullPointerException$$If any of the regex matcher objects are null (i.e. for example, if you only specify a regex for the column family), the deepCopy call will throw a NullPointerException.
ACCUMULO-8ad5a888$$key.followingKey(PartialKey.ROW_COLFAM_COLQUAL_COLVIS) can produce a key with an invalid COLVIS$$Need a new algorithm for calculating the next biggest column visibility, because tagging \0 to the end creates an invalid column visibility. We might be able to minimize the timestamp for this (i.e. set timestamp to Long.MIN_VALUE, but keep column and row elements the same).
ACCUMULO-c831e44d$$key.followingKey(PartialKey.ROW_COLFAM_COLQUAL_COLVIS) can produce a key with an invalid COLVIS$$Need a new algorithm for calculating the next biggest column visibility, because tagging \0 to the end creates an invalid column visibility. We might be able to minimize the timestamp for this (i.e. set timestamp to Long.MIN_VALUE, but keep column and row elements the same).
ACCUMULO-2d97b875$$Validity checks missing for readFields and Thrift deserialization$$Classes in o.a.a.core.data (and potentially elsewhere) that support construction from a Thrift object and/or population from a {{DataInput}} (via a {{readFields()}} method) often lack data validity checks that the classes' constructors enforce. The missing checks make it possible for an attacker to create invalid objects by manipulating the bytes being read. The situation is analogous to the need to check objects deserialized from their Java serialized form within the {{readObject()}} method.
ACCUMULO-a5e3ed3b$$Validity checks missing for readFields and Thrift deserialization$$Classes in o.a.a.core.data (and potentially elsewhere) that support construction from a Thrift object and/or population from a {{DataInput}} (via a {{readFields()}} method) often lack data validity checks that the classes' constructors enforce. The missing checks make it possible for an attacker to create invalid objects by manipulating the bytes being read. The situation is analogous to the need to check objects deserialized from their Java serialized form within the {{readObject()}} method.
ACCUMULO-adee0f12$$Validity checks missing for readFields and Thrift deserialization$$Classes in o.a.a.core.data (and potentially elsewhere) that support construction from a Thrift object and/or population from a {{DataInput}} (via a {{readFields()}} method) often lack data validity checks that the classes' constructors enforce. The missing checks make it possible for an attacker to create invalid objects by manipulating the bytes being read. The situation is analogous to the need to check objects deserialized from their Java serialized form within the {{readObject()}} method.
ACCUMULO-397f86f6$$RegExFilter does not properly regex when using multi-byte characters$$The current RegExFilter class uses a ByteArrayBackedCharSequence to set the data to match against. The ByteArrayBackedCharSequence contains a line of code that prevents the matcher from properly matching multi-byte characters.  Line 49 of ByteArrayBackedCharSequence.java is: return (char) (0xff & data[offset + index]);                                                                                                This incorrectly casts a single byte from the byte array to a char, which is 2 bytes in Java. This prevents the RegExFilter from properly performing Regular Expressions on multi-byte character encoded values.  A patch for the RegExFilter.java file has been created and will be submitted.
ACCUMULO-76d727f0$$RegExFilter does not properly regex when using multi-byte characters$$The current RegExFilter class uses a ByteArrayBackedCharSequence to set the data to match against. The ByteArrayBackedCharSequence contains a line of code that prevents the matcher from properly matching multi-byte characters.  Line 49 of ByteArrayBackedCharSequence.java is: return (char) (0xff & data[offset + index]);                                                                                                This incorrectly casts a single byte from the byte array to a char, which is 2 bytes in Java. This prevents the RegExFilter from properly performing Regular Expressions on multi-byte character encoded values.  A patch for the RegExFilter.java file has been created and will be submitted.
ACCUMULO-46f62443$$MockAccumulo doesn't throw informative errors$$Users are unable to tell if an error has occurred and whether it is due to unimplemented features in MockAccumulo.
ACCUMULO-add180fb$$MockAccumulo doesn't throw informative errors$$Users are unable to tell if an error has occurred and whether it is due to unimplemented features in MockAccumulo.
ACCUMULO-15476a0d$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.
ACCUMULO-3d55560a$$Mock Accumulo Inverts order of mutations w/ same timestamp$$Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.
ACCUMULO-28294266$$TraceProxy.trace should not throw InvocationTargetException$$In {{TraceProxy.trace}} there is the following code snippet: {code}         try {           return method.invoke(instance, args);         } catch (Throwable ex) {           ex.printStackTrace();           throw ex;         } {code} When this is an InvocationTargetException, it can really mess with the calling code's exception handling logic.
ACCUMULO-f2920c26$$Value implementation provides conflicting statements$$The javadoc for the no-arg constructor for {{Value}} states that it "Creates a zero-size sequence." However, the implementation of get will error in this case. {code} public byte[] get() {     if (this.value == null) {       throw new IllegalStateException("Uninitialized. Null constructor " + "called w/o accompanying readFields invocation");     } {code}  Either we need to change the javadoc to be more explicit or change the behaviour of various accessors in the class. I would consider both solutions to be breaking of the API contract since we are changing what clients can expect from us.
ACCUMULO-0dc92ca1$$Stat calculation of STDEV may be inaccurate$$The math is sound, but it is susceptible to rounding errors. We should address that.  See http://www.strchr.com/standard_deviation_in_one_pass and http://www.cs.berkeley.edu/~mhoemmen/cs194/Tutorials/variance.pdf
ACCUMULO-a64151e6$$Garbage collector deleted everything when given bad input$$Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.  {noformat} root@testUp> table !METADATA root@testUp !METADATA> grant Table.WRITE -u root  root@testUp !METADATA> insert ~del testDel test valueTest {noformat}  This is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.  {noformat} 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables 2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf 2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf {noformat}  GC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.
ACCUMULO-7ec60f1b$$Incorrect boundry matching for MockTableOperations.deleteRows$$The api for deleteRows specifies: Delete rows between (start, end] but the current implementation for MockTableOperations.deleteRows is implemented as (start, end)  Here is the failing test case  {code:java} public class TestDelete {   private static final String INSTANCE = "mock";   private static final String TABLE = "foo";   private static final String USER = "user";   private static final String PASS = "password";   private static final Authorizations AUTHS = new Authorizations();    @Test   public void testDelete() throws TableNotFoundException, AccumuloException,       AccumuloSecurityException, TableExistsException {      MockInstance mockAcc = new MockInstance(INSTANCE);     Connector conn = mockAcc.getConnector(USER, new PasswordToken(PASS));     conn.tableOperations().create(TABLE);     conn.securityOperations().grantTablePermission(USER, TABLE, TablePermission.READ);     conn.securityOperations().grantTablePermission(USER, TABLE, TablePermission.WRITE);      Mutation mut = new Mutation("2");     mut.put("colfam", "colqual", "value");     BatchWriter writer = conn.createBatchWriter(TABLE, new BatchWriterConfig());     writer.addMutation(mut);      Scanner scan = conn.createScanner(TABLE, AUTHS);     scan.setRange(new Range("2", "2"));      assertEquals(1, countRecords(scan));          // this should delete (1,2]      conn.tableOperations().deleteRows(TABLE, new Text("1"), new Text("2"));      scan = conn.createScanner(TABLE, AUTHS);     scan.setRange(new Range("2", "2"));          // this will fail if row 2 exists     assertEquals(0, countRecords(scan));   }    private int countRecords(Scanner scan) {     int cnt = 0;     for (Entry<Key, Value> entry : scan) {       cnt++;     }     scan.close();     return cnt;   } } {code}
ACCUMULO-019edb16$$Incompatible API changes in 1.6.0$$While examining API changes for 1.6.0 I noticed some non-deprecated methods were removed.  I am not sure how important these are, but technically these methods are in the public API.  Opening this issue to document what I found.  I compared 1.6.0 to 1.5.0.  In ACCUMULO-1674 the following methods were removed  {noformat} package org.apache.accumulo.core.client.mapreduce.lib.util ConfiguratorBase.getToken ( Class<?>, Configuration ) [static]  :  byte[ ] ConfiguratorBase.getTokenClass ( Class<?> ,Configuration) [static]  :  String {noformat}  In ACCUMULO-391 the following method was removed  {noformat} package org.apache.accumulo.core.client.mapreduce.lib.util InputConfigurator.getTabletLocator ( Class<?>, Configuration ) [static]  : TabletLocator  {noformat}  In ACCUMULO-391 the following method was removed and not properly fixed in ACCUMULO-2586  {noformat} accumulo-core.jar, RangeInputSplit.class package org.apache.accumulo.core.client.mapred InputFormatBase.RangeInputSplit.InputFormatBase.RangeInputSplit ( String table, Range range, String[ ] locations ) package org.apache.accumulo.core.client.mapreduce InputFormatBase.RangeInputSplit.InputFormatBase.RangeInputSplit ( String table, Range range, String[ ] locations )  {noformat}   It seems like the following were removed in ACCUMULO-1854   {noformat} package org.apache.accumulo.core.client.mapred InputFormatBase.RecordReaderBase<K.setupIterators (JobConf job, Scanner scanner )  :  void package org.apache.accumulo.core.client.mapreduce InputFormatBase.RecordReaderBase<K.setupIterators (TaskAttemptContext context, Scanner scanner)  :  void {noformat}  In ACCUMULO-1018 the following method was removed  {noformat} package org.apache.accumulo.core.client MutationsRejectedException.MutationsRejectedException ( List, HashMap, Set, Collection, int cause, Throwable cvsList )  {noformat}
ACCUMULO-17344890$$BlockedOutputStream can hit a StackOverflowError$$This issue mostly came up after a resolution to ACCUMULO-2668 that allows a byte[] to be passed directly to the underlying stream from the NoFlushOutputStream.  The problem appears to be due to the BlockedOutputStream.write(byte[], int, int) implementation that recursively writes out blocks/buffers out. When the stream is passed a large mutation (128MB was sufficient to trigger the error for me), this will cause a StackOverflowError.   This is appears to be specifically with encryption at rest turned on.  A simple fix would be to unroll the recursion.
ACCUMULO-6138a80f$$Instance secret written out with other configuration items to RFiles and WALogs when encryption is turned on$$The encryption at rest feature records configuration information in order to encrypted RFiles and WALogs so that if the configuration changes, the files can be read back.  The code that does this recording hovers up all the "instance.*" entries, and does not pick out the instance.secret as a special one not to write.  Thus the instance secret goes into each file in the clear, which is non-ideal to say the least.  Patch forthcoming.
ACCUMULO-1f7dd2d5$$History command incorrectly numbers commands$$When you use the history command, it will provide you with a list of previous commands that have been executed, each with a command number. However, if you try to use history expansion by number to invoke one of those commands, you will be off by one.  I think this is because the history command in added to the list after it shows you the list, and pushes everything else up by one. Uncertain if this is something we do wrong, or if this is an upstream JLine bug.
ACCUMULO-ff8c2383$$MockTableOperations.deleteRow does not handle null for start or end keys$$The deleteRow function does not check for null values for start or end keys. These null values are passed down into key constructor which will throw a NullPointerException: java.lang.NullPointerException 	at org.apache.accumulo.core.data.Key.<init>(Key.java:103) 	at org.apache.accumulo.core.client.mock.MockTableOperations.deleteRows(MockTableOperations.java:315)  The API semantics dictate: if (start == null ) then start == Text() if (end == null ) then end == maxKey()
ACCUMULO-9fcca2ed$$MockTableOperations.tableIdMap always returns tableName as ID$$Noticed and fixed this during ACCUMULO-378.  An exception was thrown unexpectedly when trying to use tableIdMap with a MockInstance. Lift fix from 93c8bddc71d1ee190649eeab263205185d75421c into main tree.
ACCUMULO-31aea2ad$$WAL handling fails to deal with 1.4 -> 1.5 -> 1.6$$After doing a 1.4 -> 1.5 -> 1.6 upgrade that still has WALs for some tables, the 1.6 instance fails to correctly handle the 1.4 recovered WALs.  This can happen either through not waiting long enough after the upgrade to 1.5 or because of an offline table brought online on 1.6 (ala ACCUMULO-2816).
ACCUMULO-f99b5654$$Missing toString, hashCode and equals methods on BatchWriterConfig$$Tried to test equality of two BatchWriterConfig objects, found they're missing all of the methods from Object that they should be implementing.
ACCUMULO-11d11e0d$$DefaultLoadBalancer takes a long time when tablets are highly unbalanced$$After creating a thousand splits on a large cluster, I noticed the master was only moving tablets to one server at a time.
ACCUMULO-023be574$$RangeInputSplit Writable methods don't serialize IteratorSettings$$Was trying to figure out why some information was getting lost on a RangeInputSplit after serialization, and found out it was because the serialization and deserialization of the class didn't include the configured IteratorSettings.  This likely isn't a big problem for normal users as, when no IteratorSettings are configured on the RangeInputSplit, it falls back to pulling from the Configuration, but it's possible, with "non-standard" uses of mapreduce, that information could be missing in the Configuration that the mappers receive, and would subsequently error.
ACCUMULO-2fd7633f$$RangeInputSplit Writable methods don't serialize IteratorSettings$$Was trying to figure out why some information was getting lost on a RangeInputSplit after serialization, and found out it was because the serialization and deserialization of the class didn't include the configured IteratorSettings.  This likely isn't a big problem for normal users as, when no IteratorSettings are configured on the RangeInputSplit, it falls back to pulling from the Configuration, but it's possible, with "non-standard" uses of mapreduce, that information could be missing in the Configuration that the mappers receive, and would subsequently error.
ACCUMULO-5eceb10e$$Unable to assign single tablet table migrated to 1.6.0$$Sorry for the screen caps, no copy/paste from machines.  Background- several tables migrated from 1.5.1 to 1.6.0. Only one of which was a single tablet. Upon starting, we noticed that that single table was not loading and the master was reporting an unassigned tablet. Had a stack trace in the monitor (attached).  Also attached is a a metadata scan of the table in question (ID: 12). I was able to get a functional copy of the table by offlining 12 and cloning it. It functioned without issues. Attached is a copy of it's metadata scan as well (ID: 9o)  The stack trace leads me to it being a specific issue with the contents of srv:dir, and the only difference is the relative vs. absolute file names. This cluster was not changed to multiple namenodes and ../tables/default_tablet does not exist. There are other tables which still use the relative naming scheme, and the system does not seem to be having issues with them.
ACCUMULO-d6472040$$Don't allow viewfs in instance.volumes$$I think one of our folks put viewfs into instance.volumes on accident. File references in accumulo.root and accumulo.metadata were then written with viewfs in the path. The garbage collector then throws errors as compactions occur and it tries delete and move the files to the hdfs users trash directory.  viewfs should never be allowed in instance.volumes property. It should fail.
ACCUMULO-f848178e$$RangeInputSplit doesn't serialize table name$$Found another missed member in the serialization of RangeInputSplit: the table name.  Not a huge deal because the table information should still be in the Configuration for most users, but this does break in "advanced" uses of mapreduce. Work around is to re-set the table in the RangeInputSplit in your overridden InputFormat.getRecordReader or make sure the Configuration is consistent from getRecordReader and getSplits.
ACCUMULO-94c2a31f$$calling MiniAccumuloCluster.stop multiple times fails with NPE$$On the mailing list [~ctubbsii] mentioned seeing some NPEs in the stderr for {{mvn verify}}.  I see one here when running mvn verify with either hadoop profile:  {quote} Exception in thread "Thread-0" java.lang.NullPointerException 	at org.apache.accumulo.minicluster.MiniAccumuloCluster.stopProcessWithTimeout(MiniAccumuloCluster.java:449) 	at org.apache.accumulo.minicluster.MiniAccumuloCluster.stop(MiniAccumuloCluster.java:376) 	at org.apache.accumulo.minicluster.MiniAccumuloCluster 1.run(MiniAccumuloCluster.java:318) {quote}  The relevant piece of code (in 1.5.2-SNAP) is the {{executor.execute}} below  {code}   private int stopProcessWithTimeout(final Process proc, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException {     FutureTask<Integer> future = new FutureTask<Integer>(new Callable<Integer>() {         @Override         public Integer call() throws InterruptedException {           proc.destroy();           return proc.waitFor();         }     });      executor.execute(future);      return future.get(timeout, unit);   } {code}  Reading through the code for stop, it nulls out executor when it's done. So the easy way to get an NPE is calling stop() multiple times on a MAC instance. Since we have a shutdown hook that calls stop, that means that a single user invocation of stop should result in a NPE later.  Since start() doesn't allow multiple starts, we probably shouldn't allow multiple stops. That would mean adding logic to the shutdown hook to check if we're already stopped or making a private unguarded version of stop that allows multiple calls and using that from the hook.  criteria for closing this issue:  * MAC should document wether calling stop() multiple times is allowed * fix MAC.stop to either guard against multiple calls or handle them gracefully * find out why this only gets an NPE in one place. Do we rely on the shutdown hook everywhere?
ACCUMULO-17654199$$File never picked up for replication$$I was running some tests and noticed that a single file was getting ignored. The logs were warning that the Status message that was written to {{accumulo.metadata}} didn't have a createdTime on the Status record.  The odd part is that all other Status messages had a createdTime and were successfully replicated. Looking at the writes from the TabletServer logs, the expected record *was* written by the TabletServer, and writing a test with the full series of Status records written does net the correct Status (which was different than what was observed in the actual table).  Looking into it, the log which was subject to this error was the first WAL that was used when the instance was started. Because the table configurations are lazily configured when they are actually used, I believe that the StatusCombiner that is set on {{accumulo.metadata}} was not seen by the TabletServer, and the VersioningIterator "ate" the first record.  I need to come up with a way that I can be sure that all tservers will have seen the Combiner set on accumulo.metadata before any data is written to it to avoid losing a record like this.
ACCUMULO-ddd2c3bc$$InputTableConfig missing isOfflineScan field in Serializer$$InputTableConfig write(DataOutput dataOutput) forgets to write out the isOfflineScan field, which makes it always false when it gets unserialized.
ACCUMULO-72fd6bec$$MiniAccumuloConfig doesn't set 0 for monitor log4j port$$MonitorLoggingIT will fail on a host if the monitor is already running because MAC doesn't configure itself to use an ephemeral port. We haven't really noticed this because MAC doesn't start a monitor by default.
ACCUMULO-1b35d263$$ZooKeeperInstance only uses first ZooKeeper in list of quorum$$Had tests running which had a quorum of 3 ZooKeeper servers. One appears to have died and the test was then unable to connect to the Accumulo shell, hanging on trying to connect to ZooKeeper.  There was no client.conf file present, so a ClientConfiguration was constructed from accumulo-site.xml.  {code} this.zooKeepers = clientConf.get(ClientProperty.INSTANCE_ZK_HOST); {code}  When the commons configuration AbstractConfiguration class is used with the get() method, only the first element in the value is returned, as the implementation treats the other items as a list because of the default separator of a comma.  It's easily reproduced with the following:  {code}     ZooKeeperInstance inst = new ZooKeeperInstance("accumulo", "localhost,127.0.0.1");     System.out.println(inst.getZooKeepers()); {code}  The above will print  {noformat} localhost {noformat}  instead of the expected  {noformat} localhost,127.0.0.1 {noformat}
ACCUMULO-891584fb$$Shell displays authTimeout poorly$$The authTimeout in the shell is displayed badly when executing {{about -v}}. Even though it is configured in integer minutes, it is converted to seconds for display as a floating point number with 2 decimals. This makes no sense, since the decimals will always be {{.00}}.  We can keep the units in seconds, I guess, but this needs to be displayed with {{%ds}} not {{%.2fs}}. This was broken in ACCUMULO-3224 by using TimeUnit to convert the number, instead of dividing by 1000.0 as we were doing manually before.
ACCUMULO-15e83709$$Consolidate ZK code WRT retries$$A couple of general ZK things that should be fixed up:  # Multiple means of automatic retrying of recoverable ZooKeeper errors through use of an InvocationHandler and a Proxy around IZooReader(Writer) # Encapsulate retry logic # Switch over callers to use the retrying instance instead of the non-retrying instance
ACCUMULO-9d8cc45d$$Bulk random walk test failed$$The bulk random walk test failed while running on a 10 node cluster w/ the following error message.  {noformat} 18 23:36:05,167 [bulk.Setup] INFO : Starting bulk test on 459a04a0   19 00:24:33,950 [randomwalk.Framework] ERROR: Error during random walk java.lang.Exception: Error running node Bulk.xml         at org.apache.accumulo.server.test.randomwalk.Module.visit(Module.java:253)         at org.apache.accumulo.server.test.randomwalk.Framework.run(Framework.java:61)         at org.apache.accumulo.server.test.randomwalk.Framework.main(Framework.java:114)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)         at java.lang.reflect.Method.invoke(Method.java:597)         at org.apache.accumulo.start.Main 1.run(Main.java:89)         at java.lang.Thread.run(Thread.java:662) Caused by: java.lang.Exception: Error running node bulk.Verify         at org.apache.accumulo.server.test.randomwalk.Module.visit(Module.java:253)         at org.apache.accumulo.server.test.randomwalk.Module.visit(Module.java:249)         ... 8 more Caused by: java.lang.Exception: Bad key at r0d646 cf:000 [] 1326932285943 false -1         at org.apache.accumulo.server.test.randomwalk.bulk.Verify.visit(Verify.java:51)         at org.apache.accumulo.server.test.randomwalk.Module.visit(Module.java:249)         ... 9 more {noformat}  Looking at the table the rows [r0d646, r0edd9] and [r0f056, r10467] all had -1 values.  There was a tablet that overlapped the first range of -1 rows exactly 268;r0edd9;r0d645.  This tablet had only the following activity on a tablet server and was then merged out of existence.  The merge operation was 268;r10eff;r093b1.  {noformat} 19 00:05:10,966 [tabletserver.Tablet] DEBUG: Files for low split 268;r0edd9;r0d645  [/b-0001azp/I0001azt.rf, /b-0001azp/I0001azu.rf, /t-0001ale/A0001an3.rf] 19 00:05:10,974 [tabletserver.Tablet] TABLET_HIST: 268;r0f055;r0d645 split 268;r0edd9;r0d645 268;r0f055;r0edd9 19 00:05:10,975 [tabletserver.Tablet] TABLET_HIST: 268;r0edd9;r0d645 opened  19 00:05:15,029 [tabletserver.Tablet] TABLET_HIST: 268;r0edd9;r0d645 import /b-0001azi/I0001azm.rf 17138 0 19 00:05:15,103 [tabletserver.Tablet] DEBUG: Starting MajC 268;r0edd9;r0d645 [/b-0001azi/I0001azm.rf, /b-0001azp/I0001azt.rf, /b-0001azp/I0001azu.rf, /t-0001ale/A0001an3.rf] --> /t-0001apj/A0001bri.rf_tmp 19 00:05:15,339 [tabletserver.Tablet] TABLET_HIST: 268;r0edd9;r0d645 import /b-0001azx/I0001azy.rf 16620 0 19 00:05:15,651 [tabletserver.Compactor] DEBUG: Compaction 268;r0edd9;r0d645 181,080 read | 60,360 written | 553,761 entries/sec |  0.327 secs 19 00:05:15,661 [tabletserver.Tablet] TABLET_HIST: 268;r0edd9;r0d645 MajC [/b-0001azi/I0001azm.rf, /b-0001azp/I0001azt.rf, /b-0001azp/I0001azu.rf, /t-0001ale/A0001an3.rf] --> /t-0001apj/A0001bri.rf 19 00:05:30,672 [tabletserver.Tablet] DEBUG: Starting MajC 268;r0edd9;r0d645 [/b-0001azx/I0001azy.rf] --> /t-0001apj/C0001brn.rf_tmp 19 00:05:30,810 [tabletserver.Compactor] DEBUG: Compaction 268;r0edd9;r0d645 60,360 read | 60,360 written | 534,159 entries/sec |  0.113 secs 19 00:05:30,824 [tabletserver.Tablet] TABLET_HIST: 268;r0edd9;r0d645 MajC [/b-0001azx/I0001azy.rf] --> /t-0001apj/C0001brn.rf 19 00:05:30,943 [tabletserver.Tablet] DEBUG: initiateClose(saveState=true queueMinC=false disableWrites=false) 268;r0edd9;r0d645 19 00:05:30,943 [tabletserver.Tablet] DEBUG: completeClose(saveState=true completeClose=true) 268;r0edd9;r0d645 19 00:05:30,947 [tabletserver.Tablet] TABLET_HIST: 268;r0edd9;r0d645 closed 19 00:05:30,947 [tabletserver.TabletServer] DEBUG: Unassigning 268;r0edd9;r0d645@(null,xxx.xxx.xxx.xxx:9997[134d7425fc59413],null) 19 00:05:30,949 [tabletserver.TabletServer] INFO : unloaded 268;r0edd9;r0d645 19 00:05:30,949 [tabletserver.TabletServer] INFO : unloaded 268;r0edd9;r0d645  {noformat}   For the second range of -1 values [r0f056, r10467], r0f056 corresponds to the split point r0f055.  Howerver, there is no split point corresponding to r10467. All of the tablets w/ a split of r0f055 lived on one tablet server.    {noformat} 19 00:02:21,262 [tabletserver.Tablet] TABLET_HIST: 268<;r0d645 split 268;r0f055;r0d645 268<;r0f055 19 00:02:21,263 [tabletserver.Tablet] TABLET_HIST: 268;r0f055;r0d645 opened  19 00:02:21,264 [tabletserver.Tablet] TABLET_HIST: 268<;r0f055 opened  19 00:02:44,504 [tabletserver.Tablet] TABLET_HIST: 268<;r0f055 split 268;r11da6;r0f055 268<;r11da6 19 00:02:44,505 [tabletserver.Tablet] TABLET_HIST: 268;r11da6;r0f055 opened  19 00:05:10,974 [tabletserver.Tablet] TABLET_HIST: 268;r0f055;r0d645 split 268;r0edd9;r0d645 268;r0f055;r0edd9 19 00:05:10,975 [tabletserver.Tablet] TABLET_HIST: 268;r0f055;r0edd9 opened  19 00:05:15,023 [tabletserver.Tablet] TABLET_HIST: 268;r11da6;r0f055 split 268;r0f622;r0f055 268;r11da6;r0f622 19 00:05:15,024 [tabletserver.Tablet] TABLET_HIST: 268;r0f622;r0f055 opened  {noformat}  All of the tablets mentioned so far were all merged away in the same merge operation, making this operation a possible place were data loss occurred.  However, I can not pinpoint the issue at this point in time.  Below is a little info about the merge from the master logs showing which tablets were involved in the merge.  {noformat} 19 00:05:30,616 [master.EventCoordinator] INFO : Merge state of 268;r10eff;r093b1 set to WAITING_FOR_CHOPPED 19 00:05:30,677 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc5940c] to chop 268;r09927;r0903a 19 00:05:30,678 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc5940c] to chop 268;r0ca9e;r09927 19 00:05:30,678 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc5940a] to chop 268;r0d2b5;r0ca9e 19 00:05:30,678 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc59412] to chop 268;r0d645;r0d2b5 19 00:05:30,678 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc59413] to chop 268;r0edd9;r0d645 19 00:05:30,678 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc59413] to chop 268;r0f055;r0edd9 19 00:05:30,678 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc59413] to chop 268;r0f622;r0f055 19 00:05:30,678 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc59413] to chop 268;r0f68b;r0f622 19 00:05:30,678 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc59413] to chop 268;r10c14;r0f68b 19 00:05:30,678 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc59413] to chop 268;r110f7;r10c14 {noformat}  When this test verifies its data and detects data loss, there is no easy way to determine at what time the data loss occurred.  It might be useful to modify the data in the bulk test such that it is easier to determine the time when data was lost.  For example the continuous ingest test creates linked list and it is possible to determine tight time bounds when a node was ingested.  However that may change the nature of this test and the bugs that it might find.
ACCUMULO-97f16db4$$AccumuloVFSClassloader creates conflicting local cache directory names when vfs.cache.dir property is set.$$When the vfs.cache.dir property is not set, the AccumuloVFSClassloader will use java.io.tmpdir as a base directory for the local cache of jars and then generate a unique directory name using a combination of the processid, hostname and userid executing the JVM.  When the vfs.cache.dir property is set, that value is used as the base directory and  an attempt to generate a unique directory is made using an AtomicInteger. This isn't suitable because for non-long lived processes, this will always be 1 - and there's a good chance that directory already exists and is owned by another user, and not writable to by the user in question.   This leads to a failure of the invoked accumulo component to start.  Modify the behavior of the unique directory creation when vfs.cache.dir is set so that it employs the same mechanism for unique directory naming that is used when it is not set.
ACCUMULO-a3267d3e$$DateLexicoder fails to correctly order dates prior to 1970$$DateLexicoder incorrectly orders dates before 1970 at the end of all other dates.  Therefore, the order was correct for all dates if the user only wrote dates before 1970, or only dates after 1970, but not if they did both.  The DateLexicoder should be fixed to store using a signed LongLexicoder internally, instead of the ULongLexicoder that it used before.
ACCUMULO-81d25bc2$$display the exact number of tablet servers$$This is a regression of ACCUMULO-1140
ACCUMULO-27d4ee21$$Token class option always requires token property$$In testing out ACCUMULO-2815, I attempted to manually provide a KerberosToken to authenticate myself and then launch the shell, but ran into an issue. The KerberosToken (in its current state) needs no options: it's wholly functional on its own.  {{accumulo shell -tc org.apache.accumulo.core.client.security.tokens.KerberosToken}}  gives an error  {noformat} 2014-12-16 11:41:09,712 [shell.Shell] ERROR: com.beust.jcommander.ParameterException: Must supply either both or neither of '--tokenClass' and '--tokenProperty' {noformat}  And providing an empty option just prints the help message {{accumulo shell -tc org.apache.accumulo.core.client.security.tokens.KerberosToken -l ""}}  I'm guessing the latter is just how the JCommander DynamicParameter is implemented, but I don't see a reason why every authentication *must* have some properties provided to it.
ACCUMULO-cfb832a1$$ProxyServer ignores value of isDeleted on ColumnUpdate$$The ProxyServer ignores the actual boolean value of the isDeleted flag on a ColumnUpdate.  If the isDeleted value is set, regardless of the actual boolean value, the ProxyServer marks the update as a delete.  The ProxyServer should be updated to check the value of the flag.
ACCUMULO-7651b777$$Shell.config()'s return value is ignored.$${{Shell.config()}} returns a boolean which is true if there was an error configuring the shell, but the value is never observed. This can result in other unintended errors (like trying to use the ConsoleReader member when it's not initialized).
ACCUMULO-9339ecf8$$AuthenticationTokenSecretManager might delete key while ZooAuthenticationKeyWatcher enumerates existing keys$$Noticed the following race condition.  The secret manager (in the master) on startup will enumerate the old keys used for creating delegation tokens and delete the keys that are expired.  At the same time, the watcher (in each tserver) might see some updates to these keys and update the secret manager. There's a race condition there that the watcher might try to read a key that the secret manager just deleted.  Need to catch the NoNodeException in the watcher and just accept that it's ok if one of these children are deleted to avoid a scary error in the monitor.
ACCUMULO-db4a291f$$master killed a tablet server$$Master killed a tablet server for having long hold times.  The tablet server had this error during minor compaction:  {noformat} 01 23:57:20,073 [security.ZKAuthenticator] ERROR: org.apache.zookeeper.KeeperException NoNodeException: KeeperErrorCode = NoNode for /accumulo/88cd0f63-a36a-4218-86b1-9ba1d2cccf08/users/user004 org.apache.zookeeper.KeeperException NoNodeException: KeeperErrorCode = NoNode for /accumulo/88cd0f63-a36a-4218-86b1-9ba1d2cccf08/users/user004         at org.apache.zookeeper.KeeperException.create(KeeperException.java:102)         at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)         at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1243)         at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1271)         at org.apache.accumulo.core.zookeeper.ZooUtil.recursiveDelete(ZooUtil.java:103)         at org.apache.accumulo.core.zookeeper.ZooUtil.recursiveDelete(ZooUtil.java:117)         at org.apache.accumulo.server.zookeeper.ZooReaderWriter.recursiveDelete(ZooReaderWriter.java:67)         at sun.reflect.GeneratedMethodAccessor53.invoke(Unknown Source)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)         at java.lang.reflect.Method.invoke(Method.java:597)         at org.apache.accumulo.server.zookeeper.ZooReaderWriter 1.invoke(ZooReaderWriter.java:169)         at  Proxy4.recursiveDelete(Unknown Source)         at org.apache.accumulo.server.security.ZKAuthenticator.dropUser(ZKAuthenticator.java:252)         at org.apache.accumulo.server.security.Auditor.dropUser(Auditor.java:104)         at org.apache.accumulo.server.client.ClientServiceHandler.dropUser(ClientServiceHandler.java:136)         at sun.reflect.GeneratedMethodAccessor52.invoke(Unknown Source)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)         at java.lang.reflect.Method.invoke(Method.java:597)         at cloudtrace.instrument.thrift.TraceWrap 1.invoke(TraceWrap.java:58)         at  Proxy2.dropUser(Unknown Source)         at org.apache.accumulo.core.client.impl.thrift.ClientService Processor dropUser.process(ClientService.java:2257)         at org.apache.accumulo.core.tabletserver.thrift.TabletClientService Processor.process(TabletClientService.java:2037)         at org.apache.accumulo.server.util.TServerUtils TimedProcessor.process(TServerUtils.java:151)         at org.apache.thrift.server.TNonblockingServer FrameBuffer.invoke(TNonblockingServer.java:631)         at org.apache.accumulo.server.util.TServerUtils THsHaServer Invocation.run(TServerUtils.java:199)         at java.util.concurrent.ThreadPoolExecutor Worker.runTask(ThreadPoolExecutor.java:886)         at java.util.concurrent.ThreadPoolExecutor Worker.run(ThreadPoolExecutor.java:908)         at org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)         at java.lang.Thread.run(Thread.java:662)  {noformat}  This tablet was the result of a split that occurred during a delete.  The master missed this tablet when taking tablets offline.  We need to do a consistency check on the offline tablets before deleting the table information in zookeeper.
ACCUMULO-73ce9cfb$$not possible to create a Mutation object from scala w/o some extra helper code$$issue:   it's not possible to create a Mutation object from scala without employing a standalone java jar wrapper. the preferred method for creating the object has you do it in two stages: create with table row, then employ Mutation.put() to populate the object with the actual mutation data. when you do this in scala, you get a  java.lang.IllegalStateException: Can not add to mutation after serializing it at org.apache.accumulo.core.data.Mutation.put(Mutation.java:168) at org.apache.accumulo.core.data.Mutation.put(Mutation.java:163) at org.apache.accumulo.core.data.Mutation.put(Mutation.java:211)  error. I *think* this has something to do with the byte array going out of scope in Scala but somehow not in Java. If you concat the operations (constuctor().put(data, data, ...) you don't run into the error, but scala sees a Unit return type, so you can't actually add the mutation to a BatchWriter. The only way I was able to get around this was to create a stand-alone jar with a method that created then returned a populated mutation object.   I wasn't sure whether or not to call this a bug or an enhancement. given that you probably want Accumulo to play nice with Scala I decided to call it a bug.   below is a link to the stack overflow thread I created whilst figuring all this out:   http://stackoverflow.com/questions/29497547/odd-error-when-populating-accumulo-1-6-mutation-object-via-spark-notebook/29527189#29527189
ACCUMULO-47c64d9a$$ClientConfiguration.getAllPropertiesWithPrefix doesn't work$$I think I introduced this method for trace.span.receiver.*, and didn't write a test for it.  My mistake.
ACCUMULO-699b8bf0$$ShutdownTServer never sets requestedShutdown$$ACCUMULO-1259 made ShutdownTServer a bit more sane WRT to what it was doing and the FATE repo interface.  One attempt it makes it to not repeatedly invoke shutdownTServer on the master..  Except {{requestedShutdown}} is never set to {{true}}.
ACCUMULO-36225565$$In Accumulo 1.7.0, connecting to a minicluster started via bin/accumulo minicluster doesn't work$$In Accumulo 1.7.0, connecting to a minicluster started via "bin/accumulo minicluster" doesn't work.  When connecting, it appears to ignore the ZK port supplied in the command and is attempting to listen to ZK on 2181.  For example: accumulo-1.7.0 > bin/accumulo minicluster … Mini Accumulo Cluster    Directory:            /var/folders/rv/44k88tps4ql0dc1f68ck4d2w0000gn/T/1437925819514-0   Logs:                 /var/folders/rv/44k88tps4ql0dc1f68ck4d2w0000gn/T/1437925819514-0/logs   Instance Name:        miniInstance   Root Password:        secret   ZooKeeper:            localhost:56783   Shutdown Port:        4445    To connect with shell, use the following command :     accumulo shell -zh localhost:56783 -zi miniInstance -u root  Successfully started on Sun Jul 26 11:50:28 EDT 2015 ===================  From a new terminal:  accumulo-1.7.0 > accumulo shell -zh localhost:56783 -zi miniInstance -u root Password: ******* …. 60 seconds later …. 2015-07-26 11:52:44,436 [tracer.ZooTraceClient] ERROR: Unabled to get destination tracer hosts in ZooKeeper, will retry in 5000 milliseconds java.lang.RuntimeException: Failed to connect to zookeeper (localhost:2181) within 2x zookeeper timeout period 30000 	at org.apache.accumulo.fate.zookeeper.ZooSession.connect(ZooSession.java:124)  Shell - Apache Accumulo Interactive Shell - - version: 1.7.0 - instance name: miniInstance - instance id: a371d4ac-8bc7-4a6a-865f-5f3c8e27fbe1 - - type 'help' for a list of available commands - root@miniInstance>
ACCUMULO-5ca779a0$$hashCode for Mutation has an unfortunate implementation$$While looking at how a tablet server processes constraint violations, I happened to look into Mutation's hashCode implementation:  {code}   @Override   public int hashCode() {     return toThrift(false).hashCode();   } {code}  Clicking through to TMutation hashCode finds this gem:  {code}   @Override   public int hashCode() {     return 0;   } {code}
ACCUMULO-a2c2d38a$$ConditionalWriterIT is failing$$I noticed that the ConditionalWriterIT was failing in master.   Using the following command with {{git bisect}} I tracked it down to commit {{3af75fc}} for ACCUMULO-4077 as the change which broke the IT.  Have not looked into why its failing yet.  {noformat} mvn clean verify -Dit.test=ConditionalWriterIT -Dfindbugs.skip -Dcheckstyle.skip -Dtest=foo -DfailIfNoTests=false {noformat}
ACCUMULO-27300d81$$Fix incorrect usage of ByteBuffer$$While working on ACCUMULO-4098 I found one place where ByteBuffer was being used incorrectly.   Looking around the code, I have found other places that are using ByteBuffer incorrectly.  Some of the problems I found are as follows :   * Calling {{ByteBuffer.array()}} without calling {{ByteBuffer.hasArray()}}.  * Using {{ByteBuffer.position()}} or {{ByteBuffer.limit()}} without adding {{ByteBuffer.arrayOffset()}} when dealing with an array returned by {{ByteBuffer.array()}}.  * Using {{ByteBuffer.arrayOffset()}} without adding {{ByteBuffer.position()}} when dealing with an array returned by {{ByteBuffer.array()}}.
ACCUMULO-5594b2e0$$importdirectory failing on split table$$bulk import for the wikisearch example isn't working properly: files are not being assigned to partitions if there are splits.
ACCUMULO-be2fdba7$$importdirectory failing on split table$$bulk import for the wikisearch example isn't working properly: files are not being assigned to partitions if there are splits.
ACCUMULO-4d23d784$$CompactCommand description is incorrect$$The compact command has the following description  {code} root@accumulo> compact -? usage: compact [<table>{ <table>}] [-?] [-b <begin-row>] [--cancel] [-e <end-row>] [-nf] [-ns <namespace> | -p <pattern> | -t <tableName>]  [-pn <profile>]  [-w] description: sets all tablets for a table to major compact as soon as possible (based on current time)   -?,--help                       display this help   -b,--begin-row <begin-row>      begin row (inclusive)      --cancel                     cancel user initiated compactions   -e,--end-row <end-row>          end row (inclusive)   -nf,--noFlush                   do not flush table data in memory before compacting.   -ns,--namespace <namespace>     name of a namespace to operate on   -p,--pattern <pattern>          regex pattern of table names to operate on   -pn,--profile <profile>         iterator profile name   -t,--table <tableName>          name of a table to operate on   -w,--wait                       wait for compact to finish {code}  However, the --begin-row is not inclusive.  Here is a simple demonstration. {code} createtable compacttest addsplits a b c insert "a" "1" "" "" insert "a" "2" "" "" insert "b" "3" "" "" insert "b" "4" "" "" insert "c" "5" "" "" insert "c" "6" "" "" flush -w scan -t accumulo.metadata -np compact -b a -e c -t compacttest -w scan -t accumulo.metadata -np deletetable compacttest -f {code}  You will see that file associated with the 'a' split is still a F flush file, which the files in the 'b' and 'c' split are A files.  Not sure if the fix is to update the commands description, which would be easy, or to make the begin row actually inclusive.
ACCUMULO-50db442b$$CompactCommand description is incorrect$$The compact command has the following description  {code} root@accumulo> compact -? usage: compact [<table>{ <table>}] [-?] [-b <begin-row>] [--cancel] [-e <end-row>] [-nf] [-ns <namespace> | -p <pattern> | -t <tableName>]  [-pn <profile>]  [-w] description: sets all tablets for a table to major compact as soon as possible (based on current time)   -?,--help                       display this help   -b,--begin-row <begin-row>      begin row (inclusive)      --cancel                     cancel user initiated compactions   -e,--end-row <end-row>          end row (inclusive)   -nf,--noFlush                   do not flush table data in memory before compacting.   -ns,--namespace <namespace>     name of a namespace to operate on   -p,--pattern <pattern>          regex pattern of table names to operate on   -pn,--profile <profile>         iterator profile name   -t,--table <tableName>          name of a table to operate on   -w,--wait                       wait for compact to finish {code}  However, the --begin-row is not inclusive.  Here is a simple demonstration. {code} createtable compacttest addsplits a b c insert "a" "1" "" "" insert "a" "2" "" "" insert "b" "3" "" "" insert "b" "4" "" "" insert "c" "5" "" "" insert "c" "6" "" "" flush -w scan -t accumulo.metadata -np compact -b a -e c -t compacttest -w scan -t accumulo.metadata -np deletetable compacttest -f {code}  You will see that file associated with the 'a' split is still a F flush file, which the files in the 'b' and 'c' split are A files.  Not sure if the fix is to update the commands description, which would be easy, or to make the begin row actually inclusive.
ACCUMULO-eb0f9b41$$CompactCommand description is incorrect$$The compact command has the following description  {code} root@accumulo> compact -? usage: compact [<table>{ <table>}] [-?] [-b <begin-row>] [--cancel] [-e <end-row>] [-nf] [-ns <namespace> | -p <pattern> | -t <tableName>]  [-pn <profile>]  [-w] description: sets all tablets for a table to major compact as soon as possible (based on current time)   -?,--help                       display this help   -b,--begin-row <begin-row>      begin row (inclusive)      --cancel                     cancel user initiated compactions   -e,--end-row <end-row>          end row (inclusive)   -nf,--noFlush                   do not flush table data in memory before compacting.   -ns,--namespace <namespace>     name of a namespace to operate on   -p,--pattern <pattern>          regex pattern of table names to operate on   -pn,--profile <profile>         iterator profile name   -t,--table <tableName>          name of a table to operate on   -w,--wait                       wait for compact to finish {code}  However, the --begin-row is not inclusive.  Here is a simple demonstration. {code} createtable compacttest addsplits a b c insert "a" "1" "" "" insert "a" "2" "" "" insert "b" "3" "" "" insert "b" "4" "" "" insert "c" "5" "" "" insert "c" "6" "" "" flush -w scan -t accumulo.metadata -np compact -b a -e c -t compacttest -w scan -t accumulo.metadata -np deletetable compacttest -f {code}  You will see that file associated with the 'a' split is still a F flush file, which the files in the 'b' and 'c' split are A files.  Not sure if the fix is to update the commands description, which would be easy, or to make the begin row actually inclusive.
ACCUMULO-116d5928$$Make sure iterators handle deletion entries properly$$In minor compaction scope and in non-full major compaction scopes the iterator may see deletion entries. These entries should be preserved by all iterators except ones that are strictly scan-time iterators that will never be configured for the minc or majc scopes. Deletion entries are only removed during full major compactions.
ACCUMULO-ebf22df0$$Make sure iterators handle deletion entries properly$$In minor compaction scope and in non-full major compaction scopes the iterator may see deletion entries. These entries should be preserved by all iterators except ones that are strictly scan-time iterators that will never be configured for the minc or majc scopes. Deletion entries are only removed during full major compactions.
ACCUMULO-8dad5e0f$$FirstEntryInRowIterator is broken and has no test$$In 1.4 and trunk, the iterator throws a NullPointerException when seeked.  In 1.3 the iterator runs, but there is a question as to what it should do when it is seeked to the middle of a row.  Currently, it returns the first key found within the range.  I believe this should be changed to ignore the remaining portion of that row and return the first key of the next row.  Should this change be made in 1.3, or should I leave it as is and just change it in 1.4 and greater?
ACCUMULO-dc9f23d9$$TimestampFilter should serialize start and end as longs in the IteratorSetting$$Although the TimestampFilter supports using longs to set the start or end timestamp, it formats them as strings using SimpleDateFormat when storing or retrieving them in the IteratorSetting.  This results in exceptions when the timestamps being used aren't able to be formatted as _yyyyMMddHHmmssz_. For example, try {{setEnd(253402300800001,true)}}  Instead, {{setStart()}} and {{setEnd()}} could just as easily use {{String.valueOf(long i)}} to store the values, and {{init()}} could retrieve them using {{Long.valueOf(String s)}}.
ACCUMULO-9453bcfa$$MockTable doesn't obey useVersions parameter$$The constructor for {{MockTable}} will call {{IteratorUtil.generateInitialTableProperties()}}, and thus set a versioning iterator on itself regardless of whether the useVersion parameter is set to true or false.   I believe {{MockTable}}'s constructor should call IteratorUtil.generateInitialTableProperties() only if useVersions is true, otherwise, it should populate {{settings}} with a new {{TreeMap}}
ACCUMULO-a450ac2f$$MockBatchScanner inappropriately filters on ranges$$I believe I have a legitimate case where an iterator will return something outside of the seeked-to range.  This appears to work in a live system, but fails to work in test cases using the MockBatchScanner.  I believe this is because the MockBatchScanner filters on the supplied ranges in addition to seeking the iterators to each range.  Either we need to remove this range filter, or fix the real system to do the same thing.  I prefer the former of course.
ACCUMULO-65390f8c$$Mock does not implement locality groups or merging$$The Mock Instance does not implement locality groups and throws an exception if one attempts to set them. It would be useful for the unit tests that I am writing for the Accumulo proxy to have at least minimal locality group functionality in the Mock instance, for example simply storing the groups and returning the stored groups when asked for.  *Edit: Tablet merging would be useful as well.
ACCUMULO-692efde2$$VisibilityFilter does not catch BadArgumentException$$If an invalid column visibility makes it into the system, then the VisibilityFilter may not handle it properly.   The accept method handles VisibilityParseException, but some of the parse code throws a BadArgumentException which is not handled.
ACCUMULO-4aeaeb2a$$stacking combiners produces a strange result$$Paste the following into your shell:  {noformat} deletetable test createtable test setiter -t test -p 16 -scan -n test_1 -class org.apache.accumulo.core.iterators.user.SummingCombiner  count:a  STRING setiter -t test -p 17 -scan -n test_2 -class org.apache.accumulo.core.iterators.user.SummingCombiner  count:a  STRING setiter -t test -p 18 -scan -n test_3 -class org.apache.accumulo.core.iterators.user.SummingCombiner  count:a  STRING setiter -t test -p 10 -scan -n test_4 -class org.apache.accumulo.core.iterators.user.SummingCombiner  count  STRING insert row count a 1 insert row count a 1 insert row count b 1 insert row count b 1 insert row count b 1 insert row count c 1 scan {noformat}  I expect:  {noformat} row count:a []    2 row count:b []    3 row count:c []    1 {noformat}  But instead, I get this: {noformat} row count:a []    12 {noformat}
FLINK-02c08456$$Type extractor cannot determine type of function$$This function fails in the type extractor.  {code} public static final class DuplicateValue<T> implements MapFunction<Tuple1<T>, Tuple2<T, T>> { 		 	@Override 	public Tuple2<T, T> map(Tuple1<T> vertex) { 		return new Tuple2<T, T>(vertex.f0, vertex.f0); 	} } {code}
FLINK-27e40205$$Type extractor cannot determine type of function$$This function fails in the type extractor.  {code} public static final class DuplicateValue<T> implements MapFunction<Tuple1<T>, Tuple2<T, T>> { 		 	@Override 	public Tuple2<T, T> map(Tuple1<T> vertex) { 		return new Tuple2<T, T>(vertex.f0, vertex.f0); 	} } {code}
FLINK-22c370d9$$POJO Type extractor bug with type variables$$The following program incorrectly states that there are duplicate getters/setters.  {code} 	public static class Vertex<K, V> { 		 		private K key1; 		private K key2; 		private V value; 		 		public Vertex() {} 		 		public Vertex(K key, V value) { 			this.key1 = key; 			this.key2 = key; 			this.value = value; 		} 		 		public Vertex(K key1, K key2, V value) { 			this.key1 = key1; 			this.key2 = key2; 			this.value = value; 		}  		public void setKey1(K key1) { 			this.key1 = key1; 		} 		 		public void setKey2(K key2) { 			this.key2 = key2; 		} 		 		public K getKey1() { 			return key1; 		} 		 		public K getKey2() { 			return key2; 		} 		 		public void setValue(V value) { 			this.value = value; 		} 		 		public V getValue() { 			return value; 		} 	} 	 	public static void main(String[] args) throws Exception { 		ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); 		 		DataSet<Vertex<Long, Double>> set = env.fromElements(new Vertex<Long, Double>(0L, 3.0), new Vertex<Long, Double>(1L, 1.0)); 		 		set.print(); 		 		env.execute(); 	} {code}  The exception is {code} Exception in thread "main" java.lang.IllegalStateException: Detected more than one getters 	at org.apache.flink.api.java.typeutils.TypeExtractor.isValidPojoField(TypeExtractor.java:981) 	at org.apache.flink.api.java.typeutils.TypeExtractor.analyzePojo(TypeExtractor.java:1025) 	at org.apache.flink.api.java.typeutils.TypeExtractor.privateGetForClass(TypeExtractor.java:937) 	at org.apache.flink.api.java.typeutils.TypeExtractor.privateGetForClass(TypeExtractor.java:863) 	at org.apache.flink.api.java.typeutils.TypeExtractor.privateGetForObject(TypeExtractor.java:1146) 	at org.apache.flink.api.java.typeutils.TypeExtractor.getForObject(TypeExtractor.java:1116) 	at org.apache.flink.api.java.ExecutionEnvironment.fromElements(ExecutionEnvironment.java:466) 	at test.Test.main(Test.java:74)  {code}
FLINK-259f10c0$$CompilerException caused by NullPointerException$$Run into it during working on my code. Seems not caused by my plan, or anyway the compiler should have a NullPointer isssue:  org.apache.flink.compiler.CompilerException: An error occurred while translating the optimized plan to a nephele JobGraph: Error translating node 'Union "Union" : UNION [[ GlobalProperties [partitioning=HASH_PARTITIONED, on fields [0]] ]] [[ LocalProperties [ordering=null, grouped=null, unique=null] ]]': null 	at org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.postVisit(NepheleJobGraphGenerator.java:543) 	at org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.postVisit(NepheleJobGraphGenerator.java:95) 	at org.apache.flink.compiler.plan.DualInputPlanNode.accept(DualInputPlanNode.java:170) 	at org.apache.flink.compiler.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:196) 	at org.apache.flink.compiler.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:196) 	at org.apache.flink.compiler.plan.OptimizedPlan.accept(OptimizedPlan.java:165) 	at org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.compileJobGraph(NepheleJobGraphGenerator.java:163) 	at org.apache.flink.client.program.Client.getJobGraph(Client.java:218) 	at org.apache.flink.client.program.Client.run(Client.java:290) 	at org.apache.flink.client.program.Client.run(Client.java:285) 	at org.apache.flink.client.program.Client.run(Client.java:230) 	at org.apache.flink.client.CliFrontend.executeProgram(CliFrontend.java:347) 	at org.apache.flink.client.CliFrontend.run(CliFrontend.java:334) 	at org.apache.flink.client.CliFrontend.parseParameters(CliFrontend.java:1001) 	at org.apache.flink.client.CliFrontend.main(CliFrontend.java:1025) Caused by: org.apache.flink.compiler.CompilerException: Error translating node 'Union "Union" : UNION [[ GlobalProperties [partitioning=HASH_PARTITIONED, on fields [0]] ]] [[ LocalProperties [ordering=null, grouped=null, unique=null] ]]': null 	at org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.preVisit(NepheleJobGraphGenerator.java:338) 	at org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.preVisit(NepheleJobGraphGenerator.java:95) 	at org.apache.flink.compiler.plan.DualInputPlanNode.accept(DualInputPlanNode.java:162) 	at org.apache.flink.compiler.plan.WorksetIterationPlanNode.acceptForStepFunction(WorksetIterationPlanNode.java:196) 	at org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.postVisit(NepheleJobGraphGenerator.java:398) 	... 14 more Caused by: java.lang.NullPointerException 	at org.apache.flink.runtime.operators.util.TaskConfig.setDriver(TaskConfig.java:307) 	at org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.createDualInputVertex(NepheleJobGraphGenerator.java:793) 	at org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.preVisit(NepheleJobGraphGenerator.java:286) 	... 18 more
FLINK-6ecd0f82$$Prevent partitioning pushdown unless partitions fields match exactly$$Consider an operation grouped on fields (A, B), followed by an operation grouped on field (A).  Right now, the optimizer can push down the partitioning on (A), which serves both operations (the first step locally still groups by A and B). This may however by a bad idea for the cases where the field A has a low cardinality, or the value distribution is skewed.  Since we cannot determine that robustly yet, I suggest to disable this optimization for now.
FLINK-45fb6d82$$Optimizer prunes all candidates when unable to reuse sort properties$$Programs fail with an exception that no plan could be created. The bug can be reproduced by the following code:  {code} val data : DataSet[(Long, Long)] = ...  data.distinct(0, 1).groupBy(0).reduceGroup(...) {code}
FLINK-94c8e3fa$$Auxiliary nodes in iterations are not correctly identified as "dynamic" or "static"$$The static/dynamic path tagger starts on the original roots of the step functions, ignoring possible auxiliary nodes that we need to attach to the root (such as NoOps, when the root is a union)
FLINK-63ef8e86$$Getter/Setter recognition for POJO fields with generics is not working$$Fields like {code} private List<Contributors> contributors; {code}  Are not recognized correctly, even if they have getters and setters. Workaround: make them public.
FLINK-9cd96df7$$Void is not added to TypeInfoParser$$List l = Arrays.asList(new Tuple2<Void,Long>(null, 1L)); TypeInformation t = TypeInfoParser.parse("Tuple2<Void,Long>"); DataSet<Tuple2<Void,Long>> data = env.fromCollection(l, t); data.print(); Throws: Exception in thread "main" java.lang.IllegalArgumentException: String could not be parsed: Class 'Void' could not be found for use as custom object. Please note that inner classes must be declared static. at org.apache.flink.api.java.typeutils.TypeInfoParser.parse(TypeInfoParser.java:90) at org.apache.flink.hadoopcompatibility.mapreduce.example.ParquetOutput.main(ParquetOutput.java:92) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134) Caused by: java.lang.IllegalArgumentException: Class 'Void' could not be found for use as custom object. Please note that inner classes must be declared static. at org.apache.flink.api.java.typeutils.TypeInfoParser.parse(TypeInfoParser.java:290) at org.apache.flink.api.java.typeutils.TypeInfoParser.parse(TypeInfoParser.java:133) at org.apache.flink.api.java.typeutils.TypeInfoParser.parse(TypeInfoParser.java:88) ... 6 more
FLINK-fb7ce0e3$$Bug in PojoSerializer's copy() method$$The PojoSerializer's {{copy()}} method does not work properly with {{null}} values. An exception could look like:  {code} Caused by: java.io.IOException: Thread 'SortMerger spilling thread' terminated due to an exception: null 	at org.apache.flink.runtime.operators.sort.UnilateralSortMerger ThreadBase.run(UnilateralSortMerger.java:792) Caused by: java.io.EOFException 	at org.apache.flink.runtime.io.disk.RandomAccessInputView.nextSegment(RandomAccessInputView.java:83) 	at org.apache.flink.runtime.memorymanager.AbstractPagedInputView.advance(AbstractPagedInputView.java:159) 	at org.apache.flink.runtime.memorymanager.AbstractPagedInputView.readByte(AbstractPagedInputView.java:270) 	at org.apache.flink.runtime.memorymanager.AbstractPagedInputView.readUnsignedByte(AbstractPagedInputView.java:277) 	at org.apache.flink.types.StringValue.copyString(StringValue.java:839) 	at org.apache.flink.api.common.typeutils.base.StringSerializer.copy(StringSerializer.java:83) 	at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.copy(PojoSerializer.java:261) 	at org.apache.flink.runtime.operators.sort.NormalizedKeySorter.writeToOutput(NormalizedKeySorter.java:449) 	at org.apache.flink.runtime.operators.sort.UnilateralSortMerger SpillingThread.go(UnilateralSortMerger.java:1303) 	at org.apache.flink.runtime.operators.sort.UnilateralSortMerger ThreadBase.run(UnilateralSortMerger.java:788) {code}  I'm working on a fix for that...
FLINK-91f9bfc7$$Interfaces and abstract classes are not valid types$$I don't know whether this is by design or is a bug, but I am having trouble working with DataSet and traits in scala which is a major limitation.  A simple example is shown below.    Compile time warning is 'Type Main.SimpleTrait has no fields that are visible from Scala Type analysis. Falling back to Java Type Analysis...'  Run time error is 'Interfaces and abstract classes are not valid types: interface Main SimpleTrait'  Regards, John    val env = ExecutionEnvironment.getExecutionEnvironment    trait SimpleTrait {     def contains(x: String): Boolean   }    class SimpleClass extends SimpleTrait {     def contains(x: String) = true   }    val data: DataSet[Double] = env.fromElements(1.0, 2.0, 3.0, 4.0)    def f(data: DataSet[Double]): DataSet[SimpleTrait] = {      data.mapPartition(iterator => {       Iterator(new SimpleClass)     })   }     val g = f(data)   g.print()     env.execute("Simple example")
FLINK-d033fa8f$$Allow KeySelectors to implement ResultTypeQueryable$$See https://github.com/apache/flink/pull/354
FLINK-0a4c7694$$Events at unitialized input channels are lost$$If a program sends an event backwards to the producer task, it might happen that some of it input channels have not been initialized yet (UnknownInputChannel). In that case, the events are lost and will never be received at the producer.
FLINK-21f47d9c$$Custom Kryo Serializer fails in itertation scenario$$When using iterations with a custom serializer for a domain object, the iteration will fail.  {code:java} org.apache.flink.runtime.client.JobExecutionException: com.esotericsoftware.kryo.KryoException: Buffer underflow 	at org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.require(NoFetchingInput.java:76) 	at com.esotericsoftware.kryo.io.Input.readVarInt(Input.java:355) 	at com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:109) 	at com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:641) 	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:752) 	at org.apache.flink.api.java.typeutils.runtime.KryoSerializer.deserialize(KryoSerializer.java:198) 	at org.apache.flink.api.java.typeutils.runtime.KryoSerializer.deserialize(KryoSerializer.java:203) 	at org.apache.flink.runtime.io.disk.InputViewIterator.next(InputViewIterator.java:43) 	at org.apache.flink.runtime.iterative.task.IterationHeadPactTask.streamOutFinalOutputBulk(IterationHeadPactTask.java:404) 	at org.apache.flink.runtime.iterative.task.IterationHeadPactTask.run(IterationHeadPactTask.java:377) 	at org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:360) 	at org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:204) 	at java.lang.Thread.run(Thread.java:745) {code}
FLINK-8f321c72$$FileOutputFormat writes to wrong path if path ends with '/'$$The FileOutputFormat duplicates the last directory of a path, if the path ends  with a slash '/'. For example, if the output path is specified as {{/home/myuser/outputPath/}} the output is written to {{/home/myuser/outputPath/outputPath/}}.  This bug was introduced by commit 8fc04e4da8a36866e10564205c3f900894f4f6e0
FLINK-1f726e48$$Streaming iteration heads cannot be instantiated$$It looks that streaming jobs with iterations and dop > 1 do not work currently. From what I see, when the TaskManager tries to instantiate a new RuntimeEnvironment for the iteration head tasks it fails since the following exception is being thrown:  java.lang.Exception: Failed to deploy the task Map (2/8) - execution #0 to slot SimpleSlot (0)(1) - 0e39fcabcab3e8543cc2d8320f9de783 - ALLOCATED/ALIVE: java.lang.Exception: Error setting up runtime environment: java.lang.RuntimeException: Could not register the given element, broker slot is already occupied. 	at org.apache.flink.runtime.execution.RuntimeEnvironment.<init>(RuntimeEnvironment.java:174) 	at org.apache.flink.runtime.taskmanager.TaskManager.org apache flink runtime taskmanager TaskManager  submitTask(TaskManager.scala:432) ..... ..... Caused by: java.lang.RuntimeException: java.lang.RuntimeException: Could not register the given element, broker slot is already occupied. 	at org.apache.flink.streaming.api.streamvertex.StreamIterationHead.setInputsOutputs(StreamIterationHead.java:64) 	at org.apache.flink.streaming.api.streamvertex.StreamVertex.registerInputOutput(StreamVertex.java:86) 	at org.apache.flink.runtime.execution.RuntimeEnvironment.<init>(RuntimeEnvironment.java:171) 	... 20 more Caused by: java.lang.RuntimeException: Could not register the given element, broker slot is already occupied. 	at org.apache.flink.runtime.iterative.concurrent.Broker.handIn(Broker.java:39) 	at org.apache.flink.streaming.api.streamvertex.StreamIterationHead.setInputsOutputs(StreamIterationHead.java:62)  The IterateTest passed since it is using a dop of 1 but for higher parallelism it fails. Also, the IterateExample fails as well if you try to run it.   I will debug this once I find some time so any ideas of what could possible cause this are more than welcome.
FLINK-5308ac83$$InstanceConnectionInfo returns wrong hostname when no DNS entry exists$$If there is no DNS entry for an address (like 10.4.122.43), then the {{InstanceConnectionInfo}} returns the first octet ({{10}}) as the hostame.
FLINK-380ef878$$IndexOutOfBoundsException when receiving empty buffer at remote channel$$Receiving buffers from remote input channels with size 0 results in an {{IndexOutOfBoundsException}}.  {code} Caused by: java.lang.IndexOutOfBoundsException: index: 30 (expected: range(0, 30)) 	at io.netty.buffer.AbstractByteBuf.checkIndex(AbstractByteBuf.java:1123) 	at io.netty.buffer.PooledUnsafeDirectByteBuf.getBytes(PooledUnsafeDirectByteBuf.java:156) 	at io.netty.buffer.PooledUnsafeDirectByteBuf.getBytes(PooledUnsafeDirectByteBuf.java:151) 	at io.netty.buffer.SlicedByteBuf.getBytes(SlicedByteBuf.java:179) 	at io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:717) 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.decodeBufferOrEvent(PartitionRequestClientHandler.java:205) 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.decodeMsg(PartitionRequestClientHandler.java:164) 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.channelRead(PartitionRequestClientHandler.java:118) {code}
FLINK-39d526e6$$Bug in DoubleParser and FloatParser - empty String is not casted to 0$$Hi,  I found the bug, when I wanted to read a csv file, which had a line like: "||\n"  If I treat it as a Tuple2<Long,Long>, I get as expected a tuple (0L,0L).  But if I want to read it into a Double-Tuple or a Float-Tuple, I get the following error:  java.lang.AssertionError: Test failed due to a org.apache.flink.api.common.io.ParseException: Line could not be parsed: '||' ParserError NUMERIC_VALUE_FORMAT_ERROR   This error can be solved by adding an additional condition for empty strings in the FloatParser / DoubleParser.  We definitely need the CSVReader to be able to read "empty values".  I can fix it like described if there are no better ideas :)
FLINK-7164b2b6$$Paths containing a Windows drive letter cannot be used in FileOutputFormats$$Paths that contain a Windows drive letter such as {{file:///c:/my/directory}} cannot be used as output path for {{FileOutputFormat}}.  If done, the following exception is thrown:  {code} Caused by: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:c:         at org.apache.flink.core.fs.Path.initialize(Path.java:242)         at org.apache.flink.core.fs.Path.<init>(Path.java:225)         at org.apache.flink.core.fs.Path.<init>(Path.java:138)         at org.apache.flink.core.fs.local.LocalFileSystem.pathToFile(LocalFileSystem.java:147)         at org.apache.flink.core.fs.local.LocalFileSystem.mkdirs(LocalFileSystem.java:232)         at org.apache.flink.core.fs.local.LocalFileSystem.mkdirs(LocalFileSystem.java:233)         at org.apache.flink.core.fs.local.LocalFileSystem.mkdirs(LocalFileSystem.java:233)         at org.apache.flink.core.fs.local.LocalFileSystem.mkdirs(LocalFileSystem.java:233)         at org.apache.flink.core.fs.local.LocalFileSystem.mkdirs(LocalFileSystem.java:233)         at org.apache.flink.core.fs.FileSystem.initOutPathLocalFS(FileSystem.java:603)         at org.apache.flink.api.common.io.FileOutputFormat.open(FileOutputFormat.java:233)         at org.apache.flink.api.java.io.CsvOutputFormat.open(CsvOutputFormat.java:158)         at org.apache.flink.runtime.operators.DataSinkTask.invoke(DataSinkTask.java:183)         at org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:217)         at java.lang.Thread.run(Unknown Source) Caused by: java.net.URISyntaxException: Relative path in absolute URI: file:c:         at java.net.URI.checkPath(Unknown Source)         at java.net.URI.<init>(Unknown Source)         at org.apache.flink.core.fs.Path.initialize(Path.java:240)         ... 14 more {code}
FLINK-ccd574a4$$Failed task deployment causes NPE on input split assignment$$The input split assignment code is returning {null} if the Task has failed, which is causing a NPE.  We should improve our error handling / reporting in that situation.  {code} 13:12:31,002 INFO  org.apache.flink.yarn.ApplicationMaster  anonfun 2  anon 1    - Status of job c0b47ce41e9a85a628a628a3977705ef (Flink Java Job at Tue Apr 21 13:10:36 UTC 2015) changed to FAILING Cannot deploy task - TaskManager not responding.. .... 13:12:47,591 ERROR org.apache.flink.runtime.operators.RegularPactTask            - Error in task code:  CHAIN DataSource (at userMethod (org.apache.flink.api.java.io.AvroInputFormat)) -> FlatMap (FlatMap at main(UserClass.java:111)) (20/50) java.lang.RuntimeException: Requesting the next InputSplit failed. 	at org.apache.flink.runtime.taskmanager.TaskInputSplitProvider.getNextInputSplit(TaskInputSplitProvider.java:88) 	at org.apache.flink.runtime.operators.DataSourceTask 1.hasNext(DataSourceTask.java:337) 	at org.apache.flink.runtime.operators.DataSourceTask.invoke(DataSourceTask.java:136) 	at org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:217) 	at java.lang.Thread.run(Thread.java:744) Caused by: java.lang.NullPointerException 	at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106) 	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:301) 	at org.apache.flink.runtime.taskmanager.TaskInputSplitProvider.getNextInputSplit(TaskInputSplitProvider.java:83) 	... 4 more 13:12:47,595 INFO  org.apache.flink.runtime.taskmanager.Task                     - CHAIN DataSource (at SomeMethod (org.apache.flink.api.java.io.AvroInputFormat)) -> FlatMap (FlatMap at main(SomeClass.java:111)) (20/50) switched to FAILED : java.lang.RuntimeException: Requesting the next InputSplit failed. 	at org.apache.flink.runtime.taskmanager.TaskInputSplitProvider.getNextInputSplit(TaskInputSplitProvider.java:88) 	at org.apache.flink.runtime.operators.DataSourceTask 1.hasNext(DataSourceTask.java:337) 	at org.apache.flink.runtime.operators.DataSourceTask.invoke(DataSourceTask.java:136) 	at org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:217) 	at java.lang.Thread.run(Thread.java:744) Caused by: java.lang.NullPointerException 	at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106) 	at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:301) 	at org.apache.flink.runtime.taskmanager.TaskInputSplitProvider.getNextInputSplit(TaskInputSplitProvider.java:83) 	... 4 more {code}
FLINK-4dbf030a$$NullPointerException in vertex-centric iteration$$Hello to my Squirrels,  I came across this exception when having a vertex-centric iteration output followed by a group by.  I'm not sure if what is causing it, since I saw this error in a rather large pipeline, but I managed to reproduce it with [this code example | https://github.com/vasia/flink/commit/1b7bbca1a6130fbcfe98b4b9b43967eb4c61f309] and a sufficiently large dataset, e.g. [this one | http://snap.stanford.edu/data/com-DBLP.html] (I'm running this locally). It seems like a null Buffer in RecordWriter.  The exception message is the following:  Exception in thread "main" org.apache.flink.runtime.client.JobExecutionException: Job execution failed. at org.apache.flink.runtime.jobmanager.JobManager anonfun receiveWithLogMessages 1.applyOrElse(JobManager.scala:319) at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) at org.apache.flink.runtime.ActorLogMessages anon 1.apply(ActorLogMessages.scala:37) at org.apache.flink.runtime.ActorLogMessages anon 1.apply(ActorLogMessages.scala:30) at scala.PartialFunction class.applyOrElse(PartialFunction.scala:118) at org.apache.flink.runtime.ActorLogMessages anon 1.applyOrElse(ActorLogMessages.scala:30) at akka.actor.Actor class.aroundReceive(Actor.scala:465) at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:94) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.ActorCell.invoke(ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) at akka.dispatch.Mailbox.run(Mailbox.scala:221) at akka.dispatch.Mailbox.exec(Mailbox.scala:231) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) Caused by: java.lang.NullPointerException at org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializer.setNextBuffer(SpanningRecordSerializer.java:93) at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:92) at org.apache.flink.runtime.operators.shipping.OutputCollector.collect(OutputCollector.java:65) at org.apache.flink.runtime.iterative.task.IterationHeadPactTask.streamSolutionSetToFinalOutput(IterationHeadPactTask.java:405) at org.apache.flink.runtime.iterative.task.IterationHeadPactTask.run(IterationHeadPactTask.java:365) at org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:360) at org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:221) at java.lang.Thread.run(Thread.java:745)
FLINK-adb321d6$$NullPointerException in DeltaIteration when no ForwardedFileds$$The following exception is thrown by the Connected Components example, if the @ForwardedFieldsFirst("*") annotation from the ComponentIdFilter join is removed:  Caused by: java.lang.NullPointerException 	at org.apache.flink.examples.java.graph.ConnectedComponents ComponentIdFilter.join(ConnectedComponents.java:186) 	at org.apache.flink.examples.java.graph.ConnectedComponents ComponentIdFilter.join(ConnectedComponents.java:1) 	at org.apache.flink.runtime.operators.JoinWithSolutionSetSecondDriver.run(JoinWithSolutionSetSecondDriver.java:198) 	at org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:496) 	at org.apache.flink.runtime.iterative.task.AbstractIterativePactTask.run(AbstractIterativePactTask.java:139) 	at org.apache.flink.runtime.iterative.task.IterationIntermediatePactTask.run(IterationIntermediatePactTask.java:92) 	at org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:362) 	at org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:217) 	at java.lang.Thread.run(Thread.java:745)  [Code | https://github.com/vasia/flink/tree/cc-test] and [dataset | http://snap.stanford.edu/data/com-DBLP.html] to reproduce.
FLINK-0078c44e$$POJO serialization NPE$$NullPointer on serialization of a Date field:  Caused by: java.lang.RuntimeException: Error obtaining the sorted input: Thread 'SortMerger Reading Thread' terminated due to an exception: null 	at org.apache.flink.runtime.operators.sort.UnilateralSortMerger.getIterator(UnilateralSortMerger.java:607) 	at org.apache.flink.runtime.operators.RegularPactTask.getInput(RegularPactTask.java:1132) 	at org.apache.flink.runtime.operators.CoGroupDriver.prepare(CoGroupDriver.java:98) 	at org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:464) 	... 3 more Caused by: java.io.IOException: Thread 'SortMerger Reading Thread' terminated due to an exception: null 	at org.apache.flink.runtime.operators.sort.UnilateralSortMerger ThreadBase.run(UnilateralSortMerger.java:784) Caused by: java.lang.NullPointerException 	at org.apache.flink.api.common.typeutils.base.DateSerializer.deserialize(DateSerializer.java:72) 	at org.apache.flink.api.common.typeutils.base.DateSerializer.deserialize(DateSerializer.java:1) 	at org.apache.flink.api.java.typeutils.runtime.PojoSerializer.deserialize(PojoSerializer.java:487) 	at org.apache.flink.api.java.typeutils.runtime.TupleSerializer.deserialize(TupleSerializer.java:136) 	at org.apache.flink.api.java.typeutils.runtime.TupleSerializer.deserialize(TupleSerializer.java:30) 	at org.apache.flink.runtime.plugable.ReusingDeserializationDelegate.read(ReusingDeserializationDelegate.java:57) 	at org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.getNextRecord(SpillingAdaptiveSpanningRecordDeserializer.java:111) 	at org.apache.flink.runtime.io.network.api.reader.AbstractRecordReader.getNextRecord(AbstractRecordReader.java:64) 	at org.apache.flink.runtime.io.network.api.reader.MutableRecordReader.next(MutableRecordReader.java:34) 	at org.apache.flink.runtime.operators.util.ReaderIterator.next(ReaderIterator.java:59) 	at org.apache.flink.runtime.operators.sort.UnilateralSortMerger ReadingThread.go(UnilateralSortMerger.java:958) 	at org.apache.flink.runtime.operators.sort.UnilateralSortMerger ThreadBase.run(UnilateralSortMerger.java:781)
FLINK-495a5c3c$$Streaming does not correctly forward ExecutionConfig to runtime$$When running streaming jobs you see this log entry: "Environment did not contain an ExecutionConfig - using a default config."  Some parts of the code use an ExecutionConfig at runtime. This will be a default config without registered serializers and other user settings.
FLINK-6bc6dbec$$Sliding Window Keeps Emitting Elements After Source Stops Producing$$This happens when the source produces some elements, then the source stops for a while and then produces again some elements before stopping again. After this, the window will just keep emitting the last emitted element indefinitely.
FLINK-0cfa43d7$$Chained stream tasks share the same RuntimeContext$$Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future.   We should create separate runtime contexts for each operator in the chain.
FLINK-d594d024$$CancelTaskException leads to FAILED task state$$The {{CancelTaskException}} is thrown to trigger canceling of the executing task. It is intended to cause a cancelled status, rather than a failed status.  Currently, it leads to a {{FAILED}} state instead of the expected {{CANCELED}} state.
FLINK-03340919$$FileInputFormat.addFilesInDir miscalculates total size$$In FileInputFormat.addFilesInDir, the length variable should start from 0, because the return value is always used by adding it to the length (instead of just assigning). So with the current version, the length before the call will be seen twice in the result.  mvn verify caught this for me now. The reason why this hasn't been seen yet, is because testGetStatisticsMultipleNestedFiles catches this only if it gets the listings of the outer directory in a certain order. Concretely, if the inner directory is seen before the other file in the outer directory, then length is 0 at that point, so the bug doesn't show. But if the other file is seen first, then its size is added twice to the total result.
FLINK-fef9f115$$Keyed State does not work with DOP=1$$When changing the DOP from 3 to 1 in StatefulOperatorTest.apiTest() the test fails. The reason seems to be that the element is not properly set when chaining is happening.  Also, requiring this: {code} headContext.setNextInput(nextRecord); streamOperator.processElement(nextRecord); {code}  to be called seems rather fragile. Why not set the element in {{processElement()}}. This would also make for cleaner encapsulation, since now all outside code must assume that operators have a {{StreamingRuntimeContext}} on which they set the next element.  The state/keyed state machinery seems dangerously undertested.
FLINK-a56aad74$$Race leading to IndexOutOfBoundsException when querying for buffer while releasing SpillablePartition$$When running a code as simple as:   {noformat} 		ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();  		DataSet<Edge<String, NullValue>> edges = getEdgesDataSet(env); 		Graph<String, NullValue, NullValue> graph = Graph.fromDataSet(edges, env);  		DataSet<Tuple2<String, Long>> degrees = graph.getDegrees(); degrees.writeAsCsv(outputPath, "\n", " "); 			env.execute();  on the Freindster data set: https://snap.stanford.edu/data/com-Friendster.html; on 30 Wally nodes   I get the following exception: java.lang.Exception: The data preparation for task 'CoGroup (CoGroup at inDegrees(Graph.java:701))' , caused an error: Error obtaining the sorted input: Thread 'SortMerger Reading Thread' terminated due to an exception: Fatal error at remote task manager 'wally028.cit.tu-berlin.de/130.149.249.38:53730'. 	at org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:471) 	at org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:362) 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:559) 	at java.lang.Thread.run(Thread.java:722) Caused by: java.lang.RuntimeException: Error obtaining the sorted input: Thread 'SortMerger Reading Thread' terminated due to an exception: Fatal error at remote task manager 'wally028.cit.tu-berlin.de/130.149.249.38:53730'. 	at org.apache.flink.runtime.operators.sort.UnilateralSortMerger.getIterator(UnilateralSortMerger.java:607) 	at org.apache.flink.runtime.operators.RegularPactTask.getInput(RegularPactTask.java:1145) 	at org.apache.flink.runtime.operators.CoGroupDriver.prepare(CoGroupDriver.java:98) 	at org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:466) 	... 3 more Caused by: java.io.IOException: Thread 'SortMerger Reading Thread' terminated due to an exception: Fatal error at remote task manager 'wally028.cit.tu-berlin.de/130.149.249.38:53730'. 	at org.apache.flink.runtime.operators.sort.UnilateralSortMerger ThreadBase.run(UnilateralSortMerger.java:784) Caused by: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Fatal error at remote task manager 'wally028.cit.tu-berlin.de/130.149.249.38:53730'. 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.decodeMsg(PartitionRequestClientHandler.java:227) 	at org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.channelRead(PartitionRequestClientHandler.java:162) 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339) 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324) 	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339) 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324) 	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:242) 	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339) 	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324) 	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:847) 	at io.netty.channel.nio.AbstractNioByteChannel NioByteUnsafe.read(AbstractNioByteChannel.java:131) 	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511) 	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) 	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) 	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) 	at io.netty.util.concurrent.SingleThreadEventExecutor 2.run(SingleThreadEventExecutor.java:111) 	at java.lang.Thread.run(Thread.java:722) Caused by: java.io.IOException: Index: 133, Size: 0  {noformat}  Code works fine for the twitter data set, for instance, which is bigger in size, but contains less vertices.
FLINK-a41bc8cc$$TypeExtractor.analyzePojo has some problems around the default constructor detection$$If a class does have a default constructor, but the user forgot to make it public, then TypeExtractor.analyzePojo still thinks everything is OK, so it creates a PojoTypeInfo. Then PojoSerializer.createInstance blows up.  Furthermore, a "return null" seems to be missing from the then case of the if after catching the NoSuchMethodException which would also cause a headache for PojoSerializer.  An additional minor issue is that the word "class" is printed twice in several places, because class.toString also prepends it to the class name.
FLINK-30761572$$PojoType fields not supported by field position keys$$Tuple fields which are Pojos (or any other non-tuple composite type) cannot be selected as keys by field position keys.  Something like   {code} DataSet<Tuple2<Integer, MyPojo>> data = ... data.groupBy(1).reduce(...) {code}  fails with an exception.
FLINK-5546a1ef$$TypeExtractor returns wrong type info when a Tuple has two fields of the same POJO type$$Consider the following code:  DataSet<FooBarPojo> d1 = env.fromElements(new FooBarPojo()); 		DataSet<Tuple2<FooBarPojo, FooBarPojo>> d2 = d1.map(new MapFunction<FooBarPojo, Tuple2<FooBarPojo, FooBarPojo>>() { 			@Override 			public Tuple2<FooBarPojo, FooBarPojo> map(FooBarPojo value) throws Exception { 				return null; 			} 		});  where FooBarPojo is the following type: public class FooBarPojo { 	public int foo, bar; 	public FooBarPojo() {} }  This should print a tuple type with two identical fields: Java Tuple2<PojoType<FooBarPojo, fields = [bar: Integer, foo: Integer]>, PojoType<FooBarPojo, fields = [bar: Integer, foo: Integer]>>  But it prints the following instead: Java Tuple2<PojoType<FooBarPojo, fields = [bar: Integer, foo: Integer]>, GenericType<FooBarPojo>>  Note, that this problem causes some co-groups in Gelly to crash with "org.apache.flink.api.common.InvalidProgramException: The pair of co-group keys are not compatible with each other" when the vertex ID type is a POJO, because the second field of the Edge type gets to be a generic type, but the POJO gets recognized in the Vertex type, and getNumberOfKeyFields returns different numbers for the POJO and the generic type.  The source of the problem is the mechanism in TypeExtractor that would detect recursive types (see the "alreadySeen" field in TypeExtractor), as it mistakes the second appearance of FooBarPojo with a recursive field.  Specifically the following happens: createTypeInfoWithTypeHierarchy starts to process the Tuple2<FooBarPojo, FooBarPojo> type, and in line 434 it calls itself for the first field, which proceeds into the privateGetForClass case which correctly detects that it is a POJO, and correctly returns a PojoTypeInfo; but in the meantime in line 1191, privateGetForClass adds PojoTypeInfo to "alreadySeen". Then the outer createTypeInfoWithTypeHierarchy approaches the second field, goes into privateGetForClass, which mistakenly returns a GenericTypeInfo, as it thinks in line 1187, that a recursive type is being processed.  (Note, that if we comment out the recursive type detection (the lines that do their thing with the alreadySeen field), then the output is correct.)
FLINK-a17d4e82$$ReduceOnNeighborsWithExceptionITCase failure$$I noticed a build error due to failure on this case. It was on a branch of my fork, which didn't actually have anything to do with the failed test or the runtime system at all.  Here's the error log: https://s3.amazonaws.com/archive.travis-ci.org/jobs/73695554/log.txt
FLINK-d738430c$$BarrierBuffer does not properly clean up temp files$$None
FLINK-06e2da35$$CheckpointCoordinator triggers checkpoints even if not all sources are running any more$$When some sources finish early, they will not emit checkpoint barriers any more. That means that pending checkpoint alignments will never be able to complete, locking the flow.
FLINK-948b6e05$$CsvParser: Quotes cannot be escaped inside quoted fields$$We should allow users to escape the quote character inside a quoted field.  Quoting could be realized through the \ character like in: {{"This is an \"escaped\" quotation."}}  Mailing list thread: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/jira-Created-FLINK-2567-CsvParser-Quotes-cannot-be-escaped-inside-quoted-fields-td7654.html
FLINK-ce68cbd9$$fieldsGrouping for multiple output streams fails$$If a Spout or Bolt declares multiple output streams and another Bolt connects to one of those streams via "fieldsGrouping",  the call to {{FlinkTopologyBuilder.createTopology()}} fails with the following exception:  {noformat} org.apache.flink.api.common.InvalidProgramException: Specifying keys via field positions is only valid for tuple data types. Type: PojoType<org.apache.flink.stormcompatibility.util.SplitStreamType, fields = [streamId: String, value: GenericType<java.lang.Object>]> 	at org.apache.flink.api.java.operators.Keys ExpressionKeys.<init>(Keys.java:209) 	at org.apache.flink.api.java.operators.Keys ExpressionKeys.<init>(Keys.java:203) 	at org.apache.flink.streaming.api.datastream.DataStream.groupBy(DataStream.java:285) 	at org.apache.flink.stormcompatibility.api.FlinkTopologyBuilder.createTopology(FlinkTopologyBuilder.java:200) 	at org.apache.flink.stormcompatibility.api.FlinkTopologyBuilderTest.testFieldsGroupingOnMultipleBoltOutputStreams(FlinkTopologyBuilderTest.java:73) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:606) 	at org.junit.runners.model.FrameworkMethod 1.runReflectiveCall(FrameworkMethod.java:47) 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50) 	at org.junit.runners.ParentRunner 3.run(ParentRunner.java:238) 	at org.junit.runners.ParentRunner 1.schedule(ParentRunner.java:63) 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236) 	at org.junit.runners.ParentRunner.access 000(ParentRunner.java:53) 	at org.junit.runners.ParentRunner 2.evaluate(ParentRunner.java:229) 	at org.junit.runners.ParentRunner.run(ParentRunner.java:309) 	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50) 	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197) {noformat}  Fix: either introduce a mapper, that "flattens" the {{SplitStreamType}} in to regular tuple type that is nested inside or provide a custom {{KeySelector}}.
FLINK-3e233a38$$Set state checkpointer before default state for PartitionedStreamOperatorState$$Currently the default state is set before the passed StateCheckpointer instance for operator states.  What currently happens because of this is that the default value is serialized with Java serialization and then deserialized on the opstate.value() call using the StateCheckpointer most likely causing a failure.  This can be trivially fixed by swaping the order of the 2 calls.
FLINK-63d9800e$$Custom StateCheckpointers should be included in the snapshots$$Currently the restoreInitialState call fails when the user uses a custom StateCheckpointer to create the snapshot, because the state is restored before the StateCheckpointer is set for the StreamOperatorState. (because the restoreInitialState() call precedes the open() call)  To avoid this issue, the custom StateCheckpointer instance should be stored within the snapshot and should be set in the StreamOperatorState before calling restoreState(..).  To reduce the overhead induced by this we can do 2 optimizations:  - We only include custom StateCheckpointers (the default java serializer one is always available)  - We only serialize the checkpointer once and store the byte array in the snapshot
FLINK-8b40bb7a$$ArrayKeySelector returns wrong positions (or fails)$$The {{ArrayKeySelector}} is broken and returns wrong values in all cases except for [0] as a single only key position.
FLINK-68912126$$FixedLengthRecordSorter can not write to output cross MemorySegments.$$FixedLengthRecordSorter can not write to output cross MemorySegments, it works well as it's only called to write a single record before. Should fix it and add more unit test.
FLINK-af477563$$Bug in Hybrid Hash Join: Request to spill a partition with less than two buffers.$$The following exception is thrown when running the example triangle listing with an unmodified master build (4cadc3d6).  {noformat} ./bin/flink run ~/flink-examples/flink-java-examples/target/flink-java-examples-0.10-SNAPSHOT-EnumTrianglesOpt.jar ~/rmat/undirected/s19_e8.ssv output {noformat}  The only changes to {{flink-conf.yaml}} are {{taskmanager.numberOfTaskSlots: 8}} and {{parallelism.default: 8}}.  I have confirmed with input files [s19_e8.ssv|https://drive.google.com/file/d/0B6TrSsnHj2HxR2lnMHR4amdyTnM/view?usp=sharing] (40 MB) and [s20_e8.ssv|https://drive.google.com/file/d/0B6TrSsnHj2HxNi1HbmptU29MTm8/view?usp=sharing] (83 MB). On a second machine only the larger file caused the exception.  {noformat} org.apache.flink.client.program.ProgramInvocationException: The program execution failed: Job execution failed. 	at org.apache.flink.client.program.Client.runBlocking(Client.java:407) 	at org.apache.flink.client.program.Client.runBlocking(Client.java:386) 	at org.apache.flink.client.program.Client.runBlocking(Client.java:353) 	at org.apache.flink.client.program.ContextEnvironment.execute(ContextEnvironment.java:64) 	at org.apache.flink.examples.java.graph.EnumTrianglesOpt.main(EnumTrianglesOpt.java:125) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:497) 	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:434) 	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:350) 	at org.apache.flink.client.program.Client.runBlocking(Client.java:290) 	at org.apache.flink.client.CliFrontend.executeProgramBlocking(CliFrontend.java:675) 	at org.apache.flink.client.CliFrontend.run(CliFrontend.java:324) 	at org.apache.flink.client.CliFrontend.parseParameters(CliFrontend.java:977) 	at org.apache.flink.client.CliFrontend.main(CliFrontend.java:1027) Caused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed. 	at org.apache.flink.runtime.jobmanager.JobManager  anonfun handleMessage 1.applyOrElse(JobManager.scala:425) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) 	at org.apache.flink.runtime.LeaderSessionMessageFilter  anonfun receive 1.applyOrElse(LeaderSessionMessageFilter.scala:36) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) 	at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:33) 	at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:28) 	at scala.PartialFunction class.applyOrElse(PartialFunction.scala:118) 	at org.apache.flink.runtime.LogMessages  anon 1.applyOrElse(LogMessages.scala:28) 	at akka.actor.Actor class.aroundReceive(Actor.scala:465) 	at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:107) 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) 	at akka.actor.ActorCell.invoke(ActorCell.scala:487) 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) 	at akka.dispatch.Mailbox.run(Mailbox.scala:221) 	at akka.dispatch.Mailbox.exec(Mailbox.scala:231) 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) 	at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) Caused by: java.lang.RuntimeException: Bug in Hybrid Hash Join: Request to spill a partition with less than two buffers. 	at org.apache.flink.runtime.operators.hash.HashPartition.spillPartition(HashPartition.java:288) 	at org.apache.flink.runtime.operators.hash.MutableHashTable.spillPartition(MutableHashTable.java:1108) 	at org.apache.flink.runtime.operators.hash.MutableHashTable.insertBucketEntry(MutableHashTable.java:934) 	at org.apache.flink.runtime.operators.hash.MutableHashTable.insertIntoTable(MutableHashTable.java:859) 	at org.apache.flink.runtime.operators.hash.MutableHashTable.buildTableFromSpilledPartition(MutableHashTable.java:819) 	at org.apache.flink.runtime.operators.hash.MutableHashTable.prepareNextPartition(MutableHashTable.java:517) 	at org.apache.flink.runtime.operators.hash.MutableHashTable.nextRecord(MutableHashTable.java:556) 	at org.apache.flink.runtime.operators.hash.NonReusingBuildFirstHashMatchIterator.callWithNextKey(NonReusingBuildFirstHashMatchIterator.java:104) 	at org.apache.flink.runtime.operators.JoinDriver.run(JoinDriver.java:208) 	at org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:489) 	at org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:354) 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:579) 	at java.lang.Thread.run(Thread.java:745) {noformat}
FLINK-b654e989$$kryo serialization problem$$Performing a cross of two dataset of POJOs I have got the exception below. The first time I run the process, there was no problem. When I run it the second time, I have got the exception. My guess is that it could be a race condition related to the reuse of the Kryo serializer object. However, it could also be "a bug where type registrations are not properly forwarded to all Serializers", as suggested by Stephan.  ------------------------------------------------------------------------ 2015-10-01 18:18:21 INFO  JobClient:161 - 10/01/2015 18:18:21	Cross(Cross at main(FlinkMongoHadoop2LinkPOI2CDA.java:160))(3/4) switched to FAILED  com.esotericsoftware.kryo.KryoException: Encountered unregistered class ID: 114 	at com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:119) 	at com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:641) 	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:752) 	at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:210) 	at org.apache.flink.api.java.typeutils.runtime.TupleSerializer.deserialize(TupleSerializer.java:127) 	at org.apache.flink.api.java.typeutils.runtime.TupleSerializer.deserialize(TupleSerializer.java:30) 	at org.apache.flink.runtime.operators.resettable.AbstractBlockResettableIterator.getNextRecord(AbstractBlockResettableIterator.java:180) 	at org.apache.flink.runtime.operators.resettable.BlockResettableMutableObjectIterator.next(BlockResettableMutableObjectIterator.java:111) 	at org.apache.flink.runtime.operators.CrossDriver.runBlockedOuterSecond(CrossDriver.java:309) 	at org.apache.flink.runtime.operators.CrossDriver.run(CrossDriver.java:162) 	at org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:489) 	at org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:354) 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:581) 	at java.lang.Thread.run(Thread.java:745)
FLINK-88a97768$$Watermark triggered operators cannot progress with cyclic flows$$The problem is that we can easily create a cyclic watermark (time) dependency in the stream graph which will result in a deadlock for watermark triggered operators such as  the `WindowOperator`.  A solution to this could be to emit a Long.MAX_VALUE watermark from the iteration sources.
FLINK-e494c279$$KeySelectorUtil.getSelectorForKeys and TypeExtractor.getKeySelectorTypes are incompatible$$The following code snippet fails, because {{KeySelectorUtil.getSelectorForKeys}} returns the base {{Tuple}} type.  ```java TypeInformation<Tuple2<Integer, Integer>> typeInfo = TypeExtractor .getForObject(Tuple2.of(0, 0));  ExecutionConfig config = new ExecutionConfig();  KeySelector<Tuple2<Integer, Integer>, ?> keySelector = KeySelectorUtil.getSelectorForKeys( new Keys.ExpressionKeys<>(new int[]{0}, typeInfo), typeInfo, config);  // fails with InvalidTypesException TypeExtractor.getKeySelectorTypes(keySelector, typeInfo);  ```  However if I manually define the key selector as follows the snippet works fine due to the key type being an integer.  ```java KeySelector<Tuple2<Integer, Integer>, Integer> keySelector =  new KeySelector<Tuple2<Integer, Integer>, Integer>() { 	@Override 	public Integer getKey(Tuple2<Integer, Integer> value) throws Exception { 		return value.f0; 	} }; ```  The error message looks like this: org.apache.flink.api.common.functions.InvalidTypesException: Usage of class Tuple as a type is not allowed. Use a concrete subclass (e.g. Tuple1, Tuple2, etc.) instead. 	at org.apache.flink.api.java.typeutils.TypeExtractor.createTypeInfoWithTypeHierarchy(TypeExtractor.java:401) 	at org.apache.flink.api.java.typeutils.TypeExtractor.privateCreateTypeInfo(TypeExtractor.java:379) 	at org.apache.flink.api.java.typeutils.TypeExtractor.getUnaryOperatorReturnType(TypeExtractor.java:279) 	at org.apache.flink.api.java.typeutils.TypeExtractor.getKeySelectorTypes(TypeExtractor.java:229) 	at org.apache.flink.api.java.typeutils.TypeExtractor.getKeySelectorTypes(TypeExtractor.java:223)
FLINK-5dfc897b$$FileMonitoring function throws NPE when location is empty$${{StreamExecutionEnvironment.readFileStream()}} does not handle a missing location properly. I would suggest to log that the location is empty and continue running the job.  A test covering the correct behavior is also needed.
FLINK-17e7b423$$Certain Avro generated getters/setters not recognized$$For Avro schemas where value null is not allowed, the field is unboxed e.g. int but the getter/setter methods provide the boxed Integer as interface:  {code} {  "fields": [   {    "type": "double",     "name": "time"   },  } {code}  This results in Java  {code}   private double time;    public java.lang.Double getTime() {     return time;   }    public void setTime(java.lang.Double value) {     this.time = value;   } {code}  There is also a problem when there is an underscore in the Avro schema, e.g.:  {code}   {    "default": null,     "type": [     "null",      "long"    ],     "name": "conn_id"   },  {code}  This results in Java:  {code} private java.lang.Long conn_id;    public java.lang.Long getConnId() {     return conn_id;   }    public void setConnId(java.lang.Long value) {     this.conn_id = value;   } {code}
FLINK-76bebd42$$MutableHashTable fails when spilling partitions without overflow segments$$When one performs a join operation with many and large records then the join operation fails with the following exception when it tries to spill a {{HashPartition}}.  {code} java.lang.RuntimeException: Bug in Hybrid Hash Join: Request to spill a partition with less than two buffers. 	at org.apache.flink.runtime.operators.hash.HashPartition.spillPartition(HashPartition.java:302) 	at org.apache.flink.runtime.operators.hash.MutableHashTable.spillPartition(MutableHashTable.java:1108) 	at org.apache.flink.runtime.operators.hash.MutableHashTable.nextSegment(MutableHashTable.java:1277) 	at org.apache.flink.runtime.operators.hash.HashPartition BuildSideBuffer.nextSegment(HashPartition.java:524) 	at org.apache.flink.runtime.memory.AbstractPagedOutputView.advance(AbstractPagedOutputView.java:140) 	at org.apache.flink.runtime.memory.AbstractPagedOutputView.write(AbstractPagedOutputView.java:201) 	at org.apache.flink.runtime.memory.AbstractPagedOutputView.write(AbstractPagedOutputView.java:178) 	at org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.serialize(BytePrimitiveArraySerializer.java:74) 	at org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.serialize(BytePrimitiveArraySerializer.java:30) 	at org.apache.flink.runtime.operators.hash.HashPartition.insertIntoBuildBuffer(HashPartition.java:257) 	at org.apache.flink.runtime.operators.hash.MutableHashTable.insertIntoTable(MutableHashTable.java:856) 	at org.apache.flink.runtime.operators.hash.MutableHashTable.buildInitialTable(MutableHashTable.java:685) 	at org.apache.flink.runtime.operators.hash.MutableHashTable.open(MutableHashTable.java:443) 	at org.apache.flink.runtime.operators.hash.HashTableTest.testSpillingWhenBuildingTableWithoutOverflow(HashTableTest.java:234) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at org.junit.runners.model.FrameworkMethod 1.runReflectiveCall(FrameworkMethod.java:47) 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50) 	at org.junit.runners.ParentRunner 3.run(ParentRunner.java:238) 	at org.junit.runners.ParentRunner 1.schedule(ParentRunner.java:63) 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236) 	at org.junit.runners.ParentRunner.access 000(ParentRunner.java:53) 	at org.junit.runners.ParentRunner 2.evaluate(ParentRunner.java:229) 	at org.junit.runners.ParentRunner.run(ParentRunner.java:309) 	at org.junit.runner.JUnitCore.run(JUnitCore.java:160) 	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:78) 	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:212) 	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:68) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140) {code}  The reason is that the {{HashPartition}} does not include the number of used memory segments by the {{BuildSideBuffer}} when it counts the currently occupied memory segments.
FLINK-59685903$$Windowed fold operation fails because the initial value was not serialized$$The windowed fold operation currently fails because the initial value was not serialized. The reason for this is that the fold operation is realized as a {{WindowFunction}} within an {{AbstractUdfStreamOperator}} and does not get the output type information forwarded (which is necessary for the serialization).   The solution is to let the {{AbstractUdfStreamOperator}} forward the output type information to the {{WindowFunction}} if it implements the {{OutputTypeConfigurable}} interface.
FLINK-5a86a0a1$$Cannot cancel failing/restarting streaming job from the command line$$I cannot seem to be able to cancel a failing/restarting job from the command line client. The job cannot be rescheduled so it keeps failing:  The exception I get: 13:58:11,240 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Status of job 0c895d22c632de5dfe16c42a9ba818d5 (player-id) changed to RESTARTING. 13:58:25,234 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Trying to cancel job with ID 0c895d22c632de5dfe16c42a9ba818d5. 13:58:25,561 WARN  akka.remote.ReliableDeliverySupervisor                        - Association with remote system [akka.tcp://flink@127.0.0.1:42012] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].
FLINK-a402002d$$Cannot cancel failing/restarting streaming job from the command line$$I cannot seem to be able to cancel a failing/restarting job from the command line client. The job cannot be rescheduled so it keeps failing:  The exception I get: 13:58:11,240 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Status of job 0c895d22c632de5dfe16c42a9ba818d5 (player-id) changed to RESTARTING. 13:58:25,234 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Trying to cancel job with ID 0c895d22c632de5dfe16c42a9ba818d5. 13:58:25,561 WARN  akka.remote.ReliableDeliverySupervisor                        - Association with remote system [akka.tcp://flink@127.0.0.1:42012] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].
FLINK-8dc70f2e$$Optimizer does not push properties out of bulk iterations$$Flink's optimizer should be able to reuse interesting properties from outside the loop. In order to do that it is sometimes necessary to append a NoOp node to the step function which recomputes the required properties.  This is currently not working for {{BulkIterations}}, because the plans with the appended NoOp nodes are not added to the overall list of candidates.  This not only leads to sub-optimal plan selection but sometimes to the rejection of valid jobs. The following job, for example, will be falsely rejected by flink.  {code} ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();  		DataSet<Tuple1<Long>> input1 = env.generateSequence(1, 10).map(new MapFunction<Long, Tuple1<Long>>() { 			@Override 			public Tuple1<Long> map(Long value) throws Exception { 				return new Tuple1<>(value); 			} 		});  		DataSet<Tuple1<Long>> input2 = env.generateSequence(1, 10).map(new MapFunction<Long, Tuple1<Long>>() { 			@Override 			public Tuple1<Long> map(Long value) throws Exception { 				return new Tuple1<>(value); 			} 		});  		DataSet<Tuple1<Long>> distinctInput = input1.distinct();  		IterativeDataSet<Tuple1<Long>> iteration = distinctInput.iterate(10);  		DataSet<Tuple1<Long>> iterationStep = iteration 				.coGroup(input2) 				.where(0) 				.equalTo(0) 				.with(new CoGroupFunction<Tuple1<Long>, Tuple1<Long>, Tuple1<Long>>() { 					@Override 					public void coGroup( 							Iterable<Tuple1<Long>> first, 							Iterable<Tuple1<Long>> second, 							Collector<Tuple1<Long>> out) throws Exception { 						Iterator<Tuple1<Long>> it = first.iterator();  						if (it.hasNext()) { 							out.collect(it.next()); 						} 					} 				});  		DataSet<Tuple1<Long>> iterationResult = iteration.closeWith(iterationStep);  		iterationResult.output(new DiscardingOutputFormat<Tuple1<Long>>()); {code}
FLINK-937963e3$$ZooKeeperCheckpointIDCounter.start() can block JobManager actor$$In HA mode, the job manager enables checkpoints during submission of streaming programs.  This leads to call to ZooKeeperCheckpointIDCounter.start(), which communicates with ZooKeeper. This can block the job manager actor.  A solution is to start the counter later instead of the CheckpointCoordinator constructor.
FLINK-a5b05566$$Error while parsing job arguments passed by CLI$$Flink CLI treats job arguments provided in format "-<char>" as its own parameters, which results in errors in execution.  Example 1: call: >bin/flink info myJarFile.jar -f flink -i <filepath> -m 1 error: Unrecognized option: -f  Example 2: Job myJarFile.jar is uploaded to web submission client, flink parameter box is empty program arguments box: -f flink -i <filepath> -m 1 error:  An unexpected error occurred: Unrecognized option: -f org.apache.flink.client.cli.CliArgsException: Unrecognized option: -f 	at org.apache.flink.client.cli.CliFrontendParser.parseInfoCommand(CliFrontendParser.java:296) 	at org.apache.flink.client.CliFrontend.info(CliFrontend.java:376) 	at org.apache.flink.client.CliFrontend.parseParameters(CliFrontend.java:983) 	at org.apache.flink.client.web.JobSubmissionServlet.doGet(JobSubmissionServlet.java:171) 	at javax.servlet.http.HttpServlet.service(HttpServlet.java:734) 	at javax.servlet.http.HttpServlet.service(HttpServlet.java:847) 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:532) 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:453) 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:227) 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:965) 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:388) 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:187) 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:901) 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117) 	at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:47) 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:113) 	at org.eclipse.jetty.server.Server.handle(Server.java:348) 	at org.eclipse.jetty.server.HttpConnection.handleRequest(HttpConnection.java:596) 	at org.eclipse.jetty.server.HttpConnection RequestHandler.headerComplete(HttpConnection.java:1048) 	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:549) 	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:211) 	at org.eclipse.jetty.server.HttpConnection.handle(HttpConnection.java:425) 	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:489) 	at org.eclipse.jetty.util.thread.QueuedThreadPool 2.run(QueuedThreadPool.java:436) 	at java.lang.Thread.run(Thread.java:745)  Execution of  >bin/flink run myJarFile.jar -f flink -i <filepath> -m 1   works perfectly fine
FLINK-117ba95f$$Checkpoint stats show ghost numbers$$[~StephanEwen] reported an issue with the display of checkpoint stats. A pipeline with a stateful source and stateless intermediate operator shows stats for the stateless intermediate operator. The numbers are most likely the same as for the source operator.
FLINK-44061882$$Invalid execution graph cleanup for jobs with colocation groups$$Currently, upon restarting an execution graph, we clean-up the colocation constraints for each group present in an ExecutionJobVertex respectively.  This can lead to invalid reconfiguration upon a restart or any other activity that relies on state cleanup of the execution graph. For example, upon restarting a DataStream job with iterations the following steps are executed:  1) IterationSource colgroup constraints are reset 2) IterationSource execution vertices reset and create new colocation constraints 3) IterationSink colgroup constraints are reset 4) IterationSink execution vertices reset and create different colocation constraints.  This can be trivially fixed by reseting colocation groups independently from ExecutionJobVertices, thus, updating them once per reconfiguration.
FLINK-6968a57a$$ExecutionGraph gets stuck in state FAILING$$It is a bit of a rare case, but the following can currently happen:    1. Jobs runs for a while, some tasks are already finished.   2. Job fails, goes to state failing and restarting. Non-finished tasks fail or are canceled.   3. For the finished tasks, ask-futures from certain messages (for example for releasing intermediate result partitions) can fail (timeout) and cause the execution to go from FINISHED to FAILED   4. This triggers the execution graph to go to FAILING without ever going further into RESTARTING again   5. The job is stuck  It initially looks like this is mainly an issue for batch jobs (jobs where tasks do finish, rather than run infinitely).  The log that shows how this manifests: {code} -------------------------------------------------------------------------------- 17:19:19,782 INFO  akka.event.slf4j.Slf4jLogger                                  - Slf4jLogger started 17:19:19,844 INFO  Remoting                                                      - Starting remoting 17:19:20,065 INFO  Remoting                                                      - Remoting started; listening on addresses :[akka.tcp://flink@127.0.0.1:56722] 17:19:20,090 INFO  org.apache.flink.runtime.blob.BlobServer                      - Created BLOB server storage directory /tmp/blobStore-6766f51a-1c51-4a03-acfb-08c2c29c11f0 17:19:20,096 INFO  org.apache.flink.runtime.blob.BlobServer                      - Started BLOB server at 0.0.0.0:43327 - max concurrent requests: 50 - max backlog: 1000 17:19:20,113 INFO  org.apache.flink.runtime.jobmanager.MemoryArchivist           - Started memory archivist akka://flink/user/archive 17:19:20,115 INFO  org.apache.flink.runtime.checkpoint.SavepointStoreFactory     - No savepoint state backend configured. Using job manager savepoint state backend. 17:19:20,118 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Starting JobManager at akka.tcp://flink@127.0.0.1:56722/user/jobmanager. 17:19:20,123 INFO  org.apache.flink.runtime.jobmanager.JobManager                - JobManager akka.tcp://flink@127.0.0.1:56722/user/jobmanager was granted leadership with leader session ID None. 17:19:25,605 INFO  org.apache.flink.runtime.instance.InstanceManager             - Registered TaskManager at testing-worker-linux-docker-e6d6931f-3200-linux-4 (akka.tcp://flink@172.17.0.253:43702/user/taskmanager) as f213232054587f296a12140d56f63ed1. Current number of registered hosts is 1. Current number of alive task slots is 2. 17:19:26,758 INFO  org.apache.flink.runtime.instance.InstanceManager             - Registered TaskManager at testing-worker-linux-docker-e6d6931f-3200-linux-4 (akka.tcp://flink@172.17.0.253:43956/user/taskmanager) as f9e78baa14fb38c69517fb1bcf4f419c. Current number of registered hosts is 2. Current number of alive task slots is 4. 17:19:27,064 INFO  org.apache.flink.api.java.ExecutionEnvironment                - The job has 0 registered types and 0 default Kryo serializers 17:19:27,071 INFO  org.apache.flink.client.program.Client                        - Starting client actor system 17:19:27,072 INFO  org.apache.flink.runtime.client.JobClient                     - Starting JobClient actor system 17:19:27,110 INFO  akka.event.slf4j.Slf4jLogger                                  - Slf4jLogger started 17:19:27,121 INFO  Remoting                                                      - Starting remoting 17:19:27,143 INFO  org.apache.flink.runtime.client.JobClient                     - Started JobClient actor system at 127.0.0.1:51198 17:19:27,145 INFO  Remoting                                                      - Remoting started; listening on addresses :[akka.tcp://flink@127.0.0.1:51198] 17:19:27,325 INFO  org.apache.flink.runtime.client.JobClientActor                - Disconnect from JobManager null. 17:19:27,362 INFO  org.apache.flink.runtime.client.JobClientActor                - Received job Flink Java Job at Mon Jan 18 17:19:27 UTC 2016 (fa05fd25993a8742da09cc5023c1e38d). 17:19:27,362 INFO  org.apache.flink.runtime.client.JobClientActor                - Could not submit job Flink Java Job at Mon Jan 18 17:19:27 UTC 2016 (fa05fd25993a8742da09cc5023c1e38d), because there is no connection to a JobManager. 17:19:27,379 INFO  org.apache.flink.runtime.client.JobClientActor                - Connect to JobManager Actor[akka.tcp://flink@127.0.0.1:56722/user/jobmanager#-1489998809]. 17:19:27,379 INFO  org.apache.flink.runtime.client.JobClientActor                - Connected to new JobManager akka.tcp://flink@127.0.0.1:56722/user/jobmanager. 17:19:27,379 INFO  org.apache.flink.runtime.client.JobClientActor                - Sending message to JobManager akka.tcp://flink@127.0.0.1:56722/user/jobmanager to submit job Flink Java Job at Mon Jan 18 17:19:27 UTC 2016 (fa05fd25993a8742da09cc5023c1e38d) and wait for progress 17:19:27,380 INFO  org.apache.flink.runtime.client.JobClientActor                - Upload jar files to job manager akka.tcp://flink@127.0.0.1:56722/user/jobmanager. 17:19:27,380 INFO  org.apache.flink.runtime.client.JobClientActor                - Submit job to the job manager akka.tcp://flink@127.0.0.1:56722/user/jobmanager. 17:19:27,453 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Submitting job fa05fd25993a8742da09cc5023c1e38d (Flink Java Job at Mon Jan 18 17:19:27 UTC 2016). 17:19:27,591 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Scheduling job fa05fd25993a8742da09cc5023c1e38d (Flink Java Job at Mon Jan 18 17:19:27 UTC 2016). 17:19:27,592 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (1/4) (c79bf4381462c690f5999f2d1949ab50) switched from CREATED to SCHEDULED 17:19:27,596 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (1/4) (c79bf4381462c690f5999f2d1949ab50) switched from SCHEDULED to DEPLOYING 17:19:27,597 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (1/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4 17:19:27,606 INFO  org.apache.flink.runtime.client.JobClientActor                - Job was successfully submitted to the JobManager akka.tcp://flink@127.0.0.1:56722/user/jobmanager. 17:19:27,630 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Status of job fa05fd25993a8742da09cc5023c1e38d (Flink Java Job at Mon Jan 18 17:19:27 UTC 2016) changed to RUNNING. 17:19:27,637 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (2/4) (e73af91028cb76f7d3cd887cb6d66755) switched from CREATED to SCHEDULED 17:19:27,654 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27	Job execution switched to status RUNNING. 17:19:27,655 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(1/4) switched to SCHEDULED  17:19:27,656 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(1/4) switched to DEPLOYING  17:19:27,666 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (2/4) (e73af91028cb76f7d3cd887cb6d66755) switched from SCHEDULED to DEPLOYING 17:19:27,667 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (2/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4 17:19:27,667 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(2/4) switched to SCHEDULED  17:19:27,669 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(2/4) switched to DEPLOYING  17:19:27,681 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (3/4) (807daf978da9dc347dca930822c78f8f) switched from CREATED to SCHEDULED 17:19:27,682 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (3/4) (807daf978da9dc347dca930822c78f8f) switched from SCHEDULED to DEPLOYING 17:19:27,682 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (3/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4 17:19:27,682 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (4/4) (ba45c37065b67fc8f5005a50d0e88fff) switched from CREATED to SCHEDULED 17:19:27,682 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (4/4) (ba45c37065b67fc8f5005a50d0e88fff) switched from SCHEDULED to DEPLOYING 17:19:27,685 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (4/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4 17:19:27,686 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(3/4) switched to SCHEDULED  17:19:27,687 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(3/4) switched to DEPLOYING  17:19:27,687 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(4/4) switched to SCHEDULED  17:19:27,692 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(4/4) switched to DEPLOYING  17:19:27,833 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (4/4) (ba45c37065b67fc8f5005a50d0e88fff) switched from DEPLOYING to RUNNING 17:19:27,839 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(4/4) switched to RUNNING  17:19:27,840 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (2/4) (e73af91028cb76f7d3cd887cb6d66755) switched from DEPLOYING to RUNNING 17:19:27,852 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(2/4) switched to RUNNING  17:19:27,896 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (1/4) (c79bf4381462c690f5999f2d1949ab50) switched from DEPLOYING to RUNNING 17:19:27,898 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (3/4) (807daf978da9dc347dca930822c78f8f) switched from DEPLOYING to RUNNING 17:19:27,901 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(1/4) switched to RUNNING  17:19:27,905 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(3/4) switched to RUNNING  17:19:28,114 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (3/4) (7997918330ecf2610b3298a8c8ef2852) switched from CREATED to SCHEDULED 17:19:28,126 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/4) (6421c8f88b191ea844619a40a523773b) switched from CREATED to SCHEDULED 17:19:28,134 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/4) (6421c8f88b191ea844619a40a523773b) switched from SCHEDULED to DEPLOYING 17:19:28,134 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4 17:19:28,126 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (2/4) (d0d011dc0a0823bcec5a57a369b334ed) switched from CREATED to SCHEDULED 17:19:28,139 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (2/4) (d0d011dc0a0823bcec5a57a369b334ed) switched from SCHEDULED to DEPLOYING 17:19:28,139 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (2/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4 17:19:28,117 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (4/4) (c928d19f73d700e80cdfad650689febb) switched from CREATED to SCHEDULED 17:19:28,134 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (3/4) (7997918330ecf2610b3298a8c8ef2852) switched from SCHEDULED to DEPLOYING 17:19:28,140 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (3/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4 17:19:28,140 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (4/4) (c928d19f73d700e80cdfad650689febb) switched from SCHEDULED to DEPLOYING 17:19:28,141 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (4/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4 17:19:28,147 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(3/4) switched to SCHEDULED  17:19:28,153 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/4) switched to SCHEDULED  17:19:28,153 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/4) switched to DEPLOYING  17:19:28,153 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(2/4) switched to SCHEDULED  17:19:28,153 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(2/4) switched to DEPLOYING  17:19:28,156 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(3/4) switched to DEPLOYING  17:19:28,158 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(4/4) switched to SCHEDULED  17:19:28,165 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(4/4) switched to DEPLOYING  17:19:28,238 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (2/4) (e73af91028cb76f7d3cd887cb6d66755) switched from RUNNING to FINISHED 17:19:28,242 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(2/4) switched to FINISHED  17:19:28,308 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (3/4) (807daf978da9dc347dca930822c78f8f) switched from RUNNING to FINISHED 17:19:28,315 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (1/4) (c79bf4381462c690f5999f2d1949ab50) switched from RUNNING to FINISHED 17:19:28,317 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(3/4) switched to FINISHED  17:19:28,318 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(1/4) switched to FINISHED  17:19:28,328 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/4) (6421c8f88b191ea844619a40a523773b) switched from DEPLOYING to RUNNING 17:19:28,336 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/4) switched to RUNNING  17:19:28,338 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (3/4) (7997918330ecf2610b3298a8c8ef2852) switched from DEPLOYING to RUNNING 17:19:28,341 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(3/4) switched to RUNNING  17:19:28,459 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (4/4) (ba45c37065b67fc8f5005a50d0e88fff) switched from RUNNING to FINISHED 17:19:28,463 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(4/4) switched to FINISHED  17:19:28,520 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (4/4) (c928d19f73d700e80cdfad650689febb) switched from DEPLOYING to RUNNING 17:19:28,529 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(4/4) switched to RUNNING  17:19:28,540 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (2/4) (d0d011dc0a0823bcec5a57a369b334ed) switched from DEPLOYING to RUNNING 17:19:28,545 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(2/4) switched to RUNNING  17:19:32,384 INFO  org.apache.flink.runtime.instance.InstanceManager             - Registered TaskManager at testing-worker-linux-docker-e6d6931f-3200-linux-4 (akka.tcp://flink@172.17.0.253:60852/user/taskmanager) as 5848d44035a164a0302da6c8701ff748. Current number of registered hosts is 3. Current number of alive task slots is 6. 17:19:32,598 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/1) (d0f8f69f9047c3154b860850955de20f) switched from CREATED to SCHEDULED 17:19:32,598 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/1) (d0f8f69f9047c3154b860850955de20f) switched from SCHEDULED to DEPLOYING 17:19:32,598 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/1) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4 17:19:32,605 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:32	Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/1) switched to SCHEDULED  17:19:32,605 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:32	Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/1) switched to DEPLOYING  17:19:32,611 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (4/4) (c928d19f73d700e80cdfad650689febb) switched from RUNNING to FINISHED 17:19:32,614 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:32	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(4/4) switched to FINISHED  17:19:32,717 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/4) (6421c8f88b191ea844619a40a523773b) switched from RUNNING to FINISHED 17:19:32,719 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:32	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/4) switched to FINISHED  17:19:32,724 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/1) (d0f8f69f9047c3154b860850955de20f) switched from DEPLOYING to RUNNING 17:19:32,726 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:32	Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/1) switched to RUNNING  17:19:32,843 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (2/4) (d0d011dc0a0823bcec5a57a369b334ed) switched from RUNNING to FINISHED 17:19:32,845 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:32	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(2/4) switched to FINISHED  17:19:33,092 WARN  akka.remote.ReliableDeliverySupervisor                        - Association with remote system [akka.tcp://flink@172.17.0.253:43702] has failed, address is now gated for [5000] ms. Reason is: [Disassociated]. 17:19:39,111 WARN  Remoting                                                      - Tried to associate with unreachable remote address [akka.tcp://flink@172.17.0.253:43702]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: Connection refused: /172.17.0.253:43702 17:19:39,113 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Task manager akka.tcp://flink@172.17.0.253:43702/user/taskmanager terminated. 17:19:39,114 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (3/4) (7997918330ecf2610b3298a8c8ef2852) switched from RUNNING to FAILED 17:19:39,120 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:39	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(3/4) switched to FAILED  java.lang.Exception: The slot in which the task was executed has been released. Probably loss of TaskManager f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager 	at org.apache.flink.runtime.instance.SimpleSlot.releaseSlot(SimpleSlot.java:151) 	at org.apache.flink.runtime.instance.SlotSharingGroupAssignment.releaseSharedSlot(SlotSharingGroupAssignment.java:547) 	at org.apache.flink.runtime.instance.SharedSlot.releaseSlot(SharedSlot.java:119) 	at org.apache.flink.runtime.instance.Instance.markDead(Instance.java:156) 	at org.apache.flink.runtime.instance.InstanceManager.unregisterTaskManager(InstanceManager.java:215) 	at org.apache.flink.runtime.jobmanager.JobManager  anonfun handleMessage 1.applyOrElse(JobManager.scala:792) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) 	at org.apache.flink.runtime.LeaderSessionMessageFilter  anonfun receive 1.applyOrElse(LeaderSessionMessageFilter.scala:44) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) 	at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:33) 	at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:28) 	at scala.PartialFunction class.applyOrElse(PartialFunction.scala:118) 	at org.apache.flink.runtime.LogMessages  anon 1.applyOrElse(LogMessages.scala:28) 	at akka.actor.Actor class.aroundReceive(Actor.scala:465) 	at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:100) 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) 	at akka.actor.dungeon.DeathWatch class.receivedTerminated(DeathWatch.scala:46) 	at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:369) 	at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:501) 	at akka.actor.ActorCell.invoke(ActorCell.scala:486) 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) 	at akka.dispatch.Mailbox.run(Mailbox.scala:221) 	at akka.dispatch.Mailbox.exec(Mailbox.scala:231) 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) 	at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)  17:19:39,129 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/1) (d0f8f69f9047c3154b860850955de20f) switched from RUNNING to CANCELING 17:19:39,132 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSink (collect()) (1/1) (895e1ea552281a665ae390c966cdb3b7) switched from CREATED to CANCELED 17:19:39,149 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:39	Job execution switched to status FAILING. java.lang.Exception: The slot in which the task was executed has been released. Probably loss of TaskManager f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager 	at org.apache.flink.runtime.instance.SimpleSlot.releaseSlot(SimpleSlot.java:151) 	at org.apache.flink.runtime.instance.SlotSharingGroupAssignment.releaseSharedSlot(SlotSharingGroupAssignment.java:547) 	at org.apache.flink.runtime.instance.SharedSlot.releaseSlot(SharedSlot.java:119) 	at org.apache.flink.runtime.instance.Instance.markDead(Instance.java:156) 	at org.apache.flink.runtime.instance.InstanceManager.unregisterTaskManager(InstanceManager.java:215) 	at org.apache.flink.runtime.jobmanager.JobManager  anonfun handleMessage 1.applyOrElse(JobManager.scala:792) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) 	at org.apache.flink.runtime.LeaderSessionMessageFilter  anonfun receive 1.applyOrElse(LeaderSessionMessageFilter.scala:44) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) 	at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:33) 	at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:28) 	at scala.PartialFunction class.applyOrElse(PartialFunction.scala:118) 	at org.apache.flink.runtime.LogMessages  anon 1.applyOrElse(LogMessages.scala:28) 	at akka.actor.Actor class.aroundReceive(Actor.scala:465) 	at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:100) 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) 	at akka.actor.dungeon.DeathWatch class.receivedTerminated(DeathWatch.scala:46) 	at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:369) 	at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:501) 	at akka.actor.ActorCell.invoke(ActorCell.scala:486) 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) 	at akka.dispatch.Mailbox.run(Mailbox.scala:221) 	at akka.dispatch.Mailbox.exec(Mailbox.scala:231) 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) 	at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) 17:19:39,173 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:39	Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/1) switched to CANCELING  17:19:39,173 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:39	DataSink (collect())(1/1) switched to CANCELED  17:19:39,174 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/1) (d0f8f69f9047c3154b860850955de20f) switched from CANCELING to FAILED 17:19:39,177 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:39	Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/1) switched to FAILED  java.lang.Exception: The slot in which the task was executed has been released. Probably loss of TaskManager f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager 	at org.apache.flink.runtime.instance.SimpleSlot.releaseSlot(SimpleSlot.java:151) 	at org.apache.flink.runtime.instance.SlotSharingGroupAssignment.releaseSharedSlot(SlotSharingGroupAssignment.java:547) 	at org.apache.flink.runtime.instance.SharedSlot.releaseSlot(SharedSlot.java:119) 	at org.apache.flink.runtime.instance.Instance.markDead(Instance.java:156) 	at org.apache.flink.runtime.instance.InstanceManager.unregisterTaskManager(InstanceManager.java:215) 	at org.apache.flink.runtime.jobmanager.JobManager  anonfun handleMessage 1.applyOrElse(JobManager.scala:792) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) 	at org.apache.flink.runtime.LeaderSessionMessageFilter  anonfun receive 1.applyOrElse(LeaderSessionMessageFilter.scala:44) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) 	at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:33) 	at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:28) 	at scala.PartialFunction class.applyOrElse(PartialFunction.scala:118) 	at org.apache.flink.runtime.LogMessages  anon 1.applyOrElse(LogMessages.scala:28) 	at akka.actor.Actor class.aroundReceive(Actor.scala:465) 	at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:100) 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) 	at akka.actor.dungeon.DeathWatch class.receivedTerminated(DeathWatch.scala:46) 	at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:369) 	at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:501) 	at akka.actor.ActorCell.invoke(ActorCell.scala:486) 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) 	at akka.dispatch.Mailbox.run(Mailbox.scala:221) 	at akka.dispatch.Mailbox.exec(Mailbox.scala:231) 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) 	at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)  17:19:39,179 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:39	Job execution switched to status RESTARTING. 17:19:39,179 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Delaying retry of job execution for 10000 ms ... 17:19:39,179 INFO  org.apache.flink.runtime.instance.InstanceManager             - Unregistered task manager akka.tcp://flink@172.17.0.253:43702/user/taskmanager. Number of registered task managers 2. Number of available slots 4. 17:19:39,179 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Status of job fa05fd25993a8742da09cc5023c1e38d (Flink Java Job at Mon Jan 18 17:19:27 UTC 2016) changed to FAILING. java.lang.Exception: The slot in which the task was executed has been released. Probably loss of TaskManager f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager 	at org.apache.flink.runtime.instance.SimpleSlot.releaseSlot(SimpleSlot.java:151) 	at org.apache.flink.runtime.instance.SlotSharingGroupAssignment.releaseSharedSlot(SlotSharingGroupAssignment.java:547) 	at org.apache.flink.runtime.instance.SharedSlot.releaseSlot(SharedSlot.java:119) 	at org.apache.flink.runtime.instance.Instance.markDead(Instance.java:156) 	at org.apache.flink.runtime.instance.InstanceManager.unregisterTaskManager(InstanceManager.java:215) 	at org.apache.flink.runtime.jobmanager.JobManager  anonfun handleMessage 1.applyOrElse(JobManager.scala:792) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) 	at org.apache.flink.runtime.LeaderSessionMessageFilter  anonfun receive 1.applyOrElse(LeaderSessionMessageFilter.scala:44) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply mcVL sp(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:33) 	at scala.runtime.AbstractPartialFunction mcVL sp.apply(AbstractPartialFunction.scala:25) 	at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:33) 	at org.apache.flink.runtime.LogMessages  anon 1.apply(LogMessages.scala:28) 	at scala.PartialFunction class.applyOrElse(PartialFunction.scala:118) 	at org.apache.flink.runtime.LogMessages  anon 1.applyOrElse(LogMessages.scala:28) 	at akka.actor.Actor class.aroundReceive(Actor.scala:465) 	at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:100) 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) 	at akka.actor.dungeon.DeathWatch class.receivedTerminated(DeathWatch.scala:46) 	at akka.actor.ActorCell.receivedTerminated(ActorCell.scala:369) 	at akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:501) 	at akka.actor.ActorCell.invoke(ActorCell.scala:486) 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254) 	at akka.dispatch.Mailbox.run(Mailbox.scala:221) 	at akka.dispatch.Mailbox.exec(Mailbox.scala:231) 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) 	at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) 17:19:39,180 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Status of job fa05fd25993a8742da09cc5023c1e38d (Flink Java Job at Mon Jan 18 17:19:27 UTC 2016) changed to RESTARTING. 17:19:42,766 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (2/4) (d0d011dc0a0823bcec5a57a369b334ed) switched from FINISHED to FAILED 17:19:42,773 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:42	CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(2/4) switched to FAILED  java.lang.IllegalStateException: Update task on instance f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager failed due to: 	at org.apache.flink.runtime.executiongraph.Execution 5.onFailure(Execution.java:915) 	at akka.dispatch.OnFailure.internal(Future.scala:228) 	at akka.dispatch.OnFailure.internal(Future.scala:227) 	at akka.dispatch.japi CallbackBridge.apply(Future.scala:174) 	at akka.dispatch.japi CallbackBridge.apply(Future.scala:171) 	at scala.PartialFunction class.applyOrElse(PartialFunction.scala:118) 	at scala.runtime.AbstractPartialFunction.applyOrElse(AbstractPartialFunction.scala:25) 	at scala.concurrent.Future  anonfun onFailure 1.apply(Future.scala:136) 	at scala.concurrent.Future  anonfun onFailure 1.apply(Future.scala:134) 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) 	at scala.concurrent.impl.ExecutionContextImpl  anon 3.exec(ExecutionContextImpl.scala:107) 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) 	at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka.tcp://flink@172.17.0.253:43702/user/taskmanager#-1712955384]] after [10000 ms] 	at akka.pattern.PromiseActorRef  anonfun 1.apply mcV sp(AskSupport.scala:333) 	at akka.actor.Scheduler  anon 7.run(Scheduler.scala:117) 	at scala.concurrent.Future InternalCallbackExecutor .scala concurrent Future InternalCallbackExecutor  unbatchedExecute(Future.scala:694) 	at scala.concurrent.Future InternalCallbackExecutor .execute(Future.scala:691) 	at akka.actor.LightArrayRevolverScheduler TaskHolder.executeTask(Scheduler.scala:467) 	at akka.actor.LightArrayRevolverScheduler  anon 8.executeBucket 1(Scheduler.scala:419) 	at akka.actor.LightArrayRevolverScheduler  anon 8.nextTick(Scheduler.scala:423) 	at akka.actor.LightArrayRevolverScheduler  anon 8.run(Scheduler.scala:375) 	at java.lang.Thread.run(Thread.java:745)  17:19:42,774 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Status of job fa05fd25993a8742da09cc5023c1e38d (Flink Java Job at Mon Jan 18 17:19:27 UTC 2016) changed to FAILING. java.lang.IllegalStateException: Update task on instance f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager failed due to: 	at org.apache.flink.runtime.executiongraph.Execution 5.onFailure(Execution.java:915) 	at akka.dispatch.OnFailure.internal(Future.scala:228) 	at akka.dispatch.OnFailure.internal(Future.scala:227) 	at akka.dispatch.japi CallbackBridge.apply(Future.scala:174) 	at akka.dispatch.japi CallbackBridge.apply(Future.scala:171) 	at scala.PartialFunction class.applyOrElse(PartialFunction.scala:118) 	at scala.runtime.AbstractPartialFunction.applyOrElse(AbstractPartialFunction.scala:25) 	at scala.concurrent.Future  anonfun onFailure 1.apply(Future.scala:136) 	at scala.concurrent.Future  anonfun onFailure 1.apply(Future.scala:134) 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) 	at scala.concurrent.impl.ExecutionContextImpl  anon 3.exec(ExecutionContextImpl.scala:107) 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) 	at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka.tcp://flink@172.17.0.253:43702/user/taskmanager#-1712955384]] after [10000 ms] 	at akka.pattern.PromiseActorRef  anonfun 1.apply mcV sp(AskSupport.scala:333) 	at akka.actor.Scheduler  anon 7.run(Scheduler.scala:117) 	at scala.concurrent.Future InternalCallbackExecutor .scala concurrent Future InternalCallbackExecutor  unbatchedExecute(Future.scala:694) 	at scala.concurrent.Future InternalCallbackExecutor .execute(Future.scala:691) 	at akka.actor.LightArrayRevolverScheduler TaskHolder.executeTask(Scheduler.scala:467) 	at akka.actor.LightArrayRevolverScheduler  anon 8.executeBucket 1(Scheduler.scala:419) 	at akka.actor.LightArrayRevolverScheduler  anon 8.nextTick(Scheduler.scala:423) 	at akka.actor.LightArrayRevolverScheduler  anon 8.run(Scheduler.scala:375) 	at java.lang.Thread.run(Thread.java:745) 17:19:42,780 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:42	Job execution switched to status FAILING. java.lang.IllegalStateException: Update task on instance f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager failed due to: 	at org.apache.flink.runtime.executiongraph.Execution 5.onFailure(Execution.java:915) 	at akka.dispatch.OnFailure.internal(Future.scala:228) 	at akka.dispatch.OnFailure.internal(Future.scala:227) 	at akka.dispatch.japi CallbackBridge.apply(Future.scala:174) 	at akka.dispatch.japi CallbackBridge.apply(Future.scala:171) 	at scala.PartialFunction class.applyOrElse(PartialFunction.scala:118) 	at scala.runtime.AbstractPartialFunction.applyOrElse(AbstractPartialFunction.scala:25) 	at scala.concurrent.Future  anonfun onFailure 1.apply(Future.scala:136) 	at scala.concurrent.Future  anonfun onFailure 1.apply(Future.scala:134) 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) 	at scala.concurrent.impl.ExecutionContextImpl  anon 3.exec(ExecutionContextImpl.scala:107) 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) 	at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka.tcp://flink@172.17.0.253:43702/user/taskmanager#-1712955384]] after [10000 ms] 	at akka.pattern.PromiseActorRef  anonfun 1.apply mcV sp(AskSupport.scala:333) 	at akka.actor.Scheduler  anon 7.run(Scheduler.scala:117) 	at scala.concurrent.Future InternalCallbackExecutor .scala concurrent Future InternalCallbackExecutor  unbatchedExecute(Future.scala:694) 	at scala.concurrent.Future InternalCallbackExecutor .execute(Future.scala:691) 	at akka.actor.LightArrayRevolverScheduler TaskHolder.executeTask(Scheduler.scala:467) 	at akka.actor.LightArrayRevolverScheduler  anon 8.executeBucket 1(Scheduler.scala:419) 	at akka.actor.LightArrayRevolverScheduler  anon 8.nextTick(Scheduler.scala:423) 	at akka.actor.LightArrayRevolverScheduler  anon 8.run(Scheduler.scala:375) 	at java.lang.Thread.run(Thread.java:745) 17:19:49,152 WARN  Remoting                                                      - Tried to associate with unreachable remote address [akka.tcp://flink@172.17.0.253:43702]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: Connection refused: /172.17.0.253:43702 17:19:59,172 WARN  Remoting                                                      - Tried to associate with unreachable remote address [akka.tcp://flink@172.17.0.253:43702]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: Connection refused: /172.17.0.253:43702 17:20:09,191 WARN  Remoting                                                      - Tried to associate with unreachable remote address [akka.tcp://flink@172.17.0.253:43702]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: Connection refused: /172.17.0.253:43702 17:24:32,423 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Stopping JobManager akka.tcp://flink@127.0.0.1:56722/user/jobmanager. 17:24:32,440 ERROR org.apache.flink.test.recovery.TaskManagerProcessFailureBatchRecoveryITCase  -  -------------------------------------------------------------------------------- Test testTaskManagerProcessFailure[0](org.apache.flink.test.recovery.TaskManagerProcessFailureBatchRecoveryITCase) failed with: java.lang.AssertionError: The program did not finish in time 	at org.junit.Assert.fail(Assert.java:88) 	at org.junit.Assert.assertTrue(Assert.java:41) 	at org.junit.Assert.assertFalse(Assert.java:64) 	at org.apache.flink.test.recovery.AbstractTaskManagerProcessFailureRecoveryTest.testTaskManagerProcessFailure(AbstractTaskManagerProcessFailureRecoveryTest.java:212) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:606) 	at org.junit.runners.model.FrameworkMethod 1.runReflectiveCall(FrameworkMethod.java:47) 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 	at org.junit.rules.TestWatcher 1.evaluate(TestWatcher.java:55) 	at org.junit.rules.RunRules.evaluate(RunRules.java:20) 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50) 	at org.junit.runners.ParentRunner 3.run(ParentRunner.java:238) 	at org.junit.runners.ParentRunner 1.schedule(ParentRunner.java:63) 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236) 	at org.junit.runners.ParentRunner.access 000(ParentRunner.java:53) 	at org.junit.runners.ParentRunner 2.evaluate(ParentRunner.java:229) 	at org.junit.runners.ParentRunner.run(ParentRunner.java:309) 	at org.junit.runners.Suite.runChild(Suite.java:127) 	at org.junit.runners.Suite.runChild(Suite.java:26) 	at org.junit.runners.ParentRunner 3.run(ParentRunner.java:238) 	at org.junit.runners.ParentRunner 1.schedule(ParentRunner.java:63) 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236) 	at org.junit.runners.ParentRunner.access 000(ParentRunner.java:53) 	at org.junit.runners.ParentRunner 2.evaluate(ParentRunner.java:229) 	at org.junit.runners.ParentRunner.run(ParentRunner.java:309) 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:283) 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:173) 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153) 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:128) {code}
FLINK-ed3810b1$$Disable reference tracking in Kryo fallback serializer$$Kryo runs extra logic to track and resolve repeated references to the same object (similar as JavaSerialization)  We should disable reference tracking   - reference tracking is costly   - it is virtually always unnecessary in the datatypes used in Flink   - most importantly, it is inconsistent with Flink's own serialization (which does not do reference tracking)   - It may have problems if elements are read in a different order than they are written.
FLINK-8fc7e7af$$Early cancel calls can cause Tasks to not cancel properly$$When a task receives the "cancel()" call before the operators are properly instantiated, it can be that the operator never receives a cancel call.  In certain cases, this causes the operator to hang.
FLINK-8e3e2f8f$$Operator checkpoint statistics state size overflow$$State sizes ({{long}}) of checkpoint stats overflow when summing them up per operator, because the sum is stored in an {{int}}.
FLINK-d90672fd$$Fix interplay of automatic Operator UID and Changing name of WindowOperator$$WindowOperator can have a changing name because it has the TypeSerializer .toString() output in it's name. For some type serializers that don't implement toString() this means that the name changes.  This means that savepoint restore does not work for the automatically generated UID.
FLINK-734ba01d$$Cancelling a running job can lead to restart instead of stopping$$I just tried cancelling a regularly running job. Instead of the job stopping, it restarted.   {code} 2016-02-29 10:39:28,415 INFO  org.apache.flink.yarn.YarnJobManager                          - Trying to cancel job with ID 5c0604694c8469cfbb89daaa990068df. 2016-02-29 10:39:28,416 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Source: Out of order data generator -> (Flat Map, Timestamps/Watermarks) (1/1) (e3b05555ab0e373defb925898de9f200) switched from RUNNING to CANCELING .... 2016-02-29 10:39:28,488 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - TriggerWindow(TumblingTimeWindows(60000), FoldingStateDescriptor{name=window-contents, defaultValue=(0,9223372036854775807,0), serializer=null}, EventTimeTrigger(), WindowedStream.apply(WindowedStream.java:397)) (19/24) (c1be31b0be596d2521073b2d78ffa60a) switched from CANCELING to CANCELED 2016-02-29 10:40:08,468 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Source: Out of order data generator -> (Flat Map, Timestamps/Watermarks) (1/1) (e3b05555ab0e373defb925898de9f200) switched from CANCELING to FAILED 2016-02-29 10:40:08,468 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - TriggerWindow(TumblingTimeWindows(60000), FoldingStateDescriptor{name=window-contents, defaultValue=(0,9223372036854775807,0), serializer=null}, EventTimeTrigger(), WindowedStream.apply(WindowedStream.java:397)) (1/24) (5ad172ec9932b24d5a98377a2c82b0b3) switched from CANCELING to FAILED 2016-02-29 10:40:08,472 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - TriggerWindow(TumblingTimeWindows(60000), FoldingStateDescriptor{name=window-contents, defaultValue=(0,9223372036854775807,0), serializer=null}, EventTimeTrigger(), WindowedStream.apply(WindowedStream.java:397)) (2/24) (5404ca28ac7cf23b67dff30ef2309078) switched from CANCELING to FAILED 2016-02-29 10:40:08,473 INFO  org.apache.flink.yarn.YarnJobManager                          - Status of job 5c0604694c8469cfbb89daaa990068df (Event counter: {auto.offset.reset=earliest, rocksdb=hdfs:///user/robert/rocksdb, generateInPlace=soTrue, parallelism=24, bootstrap.servers=cdh544-worker-0:9092, topic=eventsGenerator, eventsPerKeyPerGenerator=2, numKeys=1000000000, zookeeper.connect=cdh544-worker-0:2181, timeSliceSize=60000, eventsKerPey=1, genPar=1}) changed to FAILING. java.lang.Exception: Task could not be canceled. 	at org.apache.flink.runtime.executiongraph.Execution 5.onComplete(Execution.java:902) 	at akka.dispatch.OnComplete.internal(Future.scala:246) 	at akka.dispatch.OnComplete.internal(Future.scala:244) 	at akka.dispatch.japi CallbackBridge.apply(Future.scala:174) 	at akka.dispatch.japi CallbackBridge.apply(Future.scala:171) 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32) 	at scala.concurrent.impl.ExecutionContextImpl  anon 3.exec(ExecutionContextImpl.scala:107) 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) 	at scala.concurrent.forkjoin.ForkJoinPool WorkQueue.runTask(ForkJoinPool.java:1339) 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka.tcp://flink@10.240.242.143:50119/user/taskmanager#640539146]] after [10000 ms] 	at akka.pattern.PromiseActorRef  anonfun 1.apply mcV sp(AskSupport.scala:333) 	at akka.actor.Scheduler  anon 7.run(Scheduler.scala:117) 	at scala.concurrent.Future InternalCallbackExecutor .scala concurrent Future InternalCallbackExecutor  unbatchedExecute(Future.scala:694) 	at scala.concurrent.Future InternalCallbackExecutor .execute(Future.scala:691) 	at akka.actor.LightArrayRevolverScheduler TaskHolder.executeTask(Scheduler.scala:467) 	at akka.actor.LightArrayRevolverScheduler  anon 8.executeBucket 1(Scheduler.scala:419) 	at akka.actor.LightArrayRevolverScheduler  anon 8.nextTick(Scheduler.scala:423) 	at akka.actor.LightArrayRevolverScheduler  anon 8.run(Scheduler.scala:375) 	at java.lang.Thread.run(Thread.java:745) 2016-02-29 10:40:08,477 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - TriggerWindow(TumblingTimeWindows(60000), FoldingStateDescriptor{name=window-contents, defaultValue=(0,9223372036854775807,0), serializer=null}, EventTimeTrigger(), WindowedStream.apply(WindowedStream.java:397)) (3/24) (fc527d65ec8df3ccf68f882d968e776e) switched from CANCELING to FAILED 2016-02-29 10:40:08,487 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - TriggerWindow(TumblingTimeWindows(60000), FoldingStateDescriptor{name=window-contents, defaultValue=(0,9223372036854775807,0), serializer=null}, EventTimeTrigger(), WindowedStream.apply(WindowedStream.java:397)) (4/24) (afb1aa3c2d8acdee0f138cf344238e4e) switched from CANCELING to FAILED 2016-02-29 10:40:08,488 INFO  org.apache.flink.runtime.executiongraph.restart.FixedDelayRestartStrategy  - Delaying retry of job execution for 3000 ms ... 2016-02-29 10:40:08,488 INFO  org.apache.flink.yarn.YarnJobManager                          - Status of job 5c0604694c8469cfbb89daaa990068df (Event counter: {auto.offset.reset=earliest, rocksdb=hdfs:///user/robert/rocksdb, generateInPlace=soTrue, parallelism=24, bootstrap.servers=cdh544-worker-0:9092, topic=eventsGenerator, eventsPerKeyPerGenerator=2, numKeys=1000000000, zookeeper.connect=cdh544-worker-0:2181, timeSliceSize=60000, eventsKerPey=1, genPar=1}) changed to RESTARTING. 2016-02-29 10:40:11,490 INFO  org.apache.flink.yarn.YarnJobManager                          - Status of job 5c0604694c8469cfbb89daaa990068df (Event counter: {auto.offset.reset=earliest, rocksdb=hdfs:///user/robert/rocksdb, generateInPlace=soTrue, parallelism=24, bootstrap.servers=cdh544-worker-0:9092, topic=eventsGenerator, eventsPerKeyPerGenerator=2, numKeys=1000000000, zookeeper.connect=cdh544-worker-0:2181, timeSliceSize=60000, eventsKerPey=1, genPar=1}) changed to CREATED. 2016-02-29 10:40:11,490 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Source: Out of order data generator -> (Flat Map, Timestamps/Watermarks) (1/1) (1319b2f44d78d99948ffde4350c052d9) switched from CREATED to SCHEDULED 2016-02-29 10:40:11,490 INFO  org.apache.flink.yarn.YarnJobManager                          - Status of job 5c0604694c8469cfbb89daaa990068df (Event counter: {auto.offset.reset=earliest, rocksdb=hdfs:///user/robert/rocksdb, generateInPlace=soTrue, parallelism=24, bootstrap.servers=cdh544-worker-0:9092, topic=eventsGenerator, eventsPerKeyPerGenerator=2, numKeys=1000000000, zookeeper.connect=cdh544-worker-0:2181, timeSliceSize=60000, eventsKerPey=1, genPar=1}) changed to RUNNING. {code}
FLINK-434e88fd$$Input type validation often fails on custom TypeInfo implementations$$Input type validation often fails when used with custom type infos. One example of this behaviour can be reproduced by creating a custom type info with our own field type:  StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();  env.generateSequence(1, 10).map(new MapFunction<Long, Tuple1<Optional<Long>>>() { 			@Override 			public Tuple1<Optional<Long>> map(Long value) throws Exception { 				return Tuple1.of(Optional.of(value)); 			} 		}).returns(new TupleTypeInfo<>(new OptionTypeInfo<Long>(BasicTypeInfo.LONG_TYPE_INFO))) 				.keyBy(new KeySelector<Tuple1<Optional<Long>>, Optional<Long>>() {  					@Override 					public Optional<Long> getKey(Tuple1<Optional<Long>> value) throws Exception { 						return value.f0; 					} 				});  This will fail on Input type validation at the KeySelector (or any other function for example a mapper) with the following exception:  Input mismatch: Basic type expected.
FLINK-e3759a5e$$CEP operator does not forward watermarks properly$$The CEP stream operator don't emit a proper watermark when using event time.
FLINK-f2f5bd5b$$Session Window State is Not Checkpointed$$The merging window state in the {{WindowOperator}} is not checkpointed. This means that programs containing session windows will fail upon restore after a failure.  I propose adding a simulated snapshot/restore cycle to the tests in {{WindowOperatorTest}} to catch these problems in the future.
FLINK-494212b3$$Fix StateDescriptor.readObject$$The readObject method of StateDescriptor uses uses {{ObjectInputStream.read()}}. For very large serialized default values this will not necessarily read all data in one go. We need a loop that reads it in several steps.
FLINK-dc78a747$$Kryo StackOverflowError due to disabled Kryo Reference tracking$$As discussed on the dev list,  In {{KryoSerializer.java}}  Kryo Reference tracking is disabled by default:  {code}     kryo.setReferences(false); {code}  This can causes  {{StackOverflowError}} Exceptions when serializing many objects that may contain recursive objects:  {code} java.lang.StackOverflowError 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:48) 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:495) 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:523) 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:61) 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:495) 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:523) 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:61) 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:495) 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:523) 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:61) 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:495) {code}  By enabling reference tracking, we can fix this problem.  [1]https://gist.github.com/andrewpalumbo/40c7422a5187a24cd03d7d81feb2a419
FLINK-32a003d5$$NullPointerException while translating union node$$The NepheleJobGraphGenerator throws a NullPointerException when translating a binary union operator. The BinaryUnionPlanNode is not replaced by a NAryUnionPlanNode and thus is still treated as a DualInputVertex. Accessing the driver code of the BinaryUnionPlanNode causes then the NullPointerException.
WICKET-7e1000dd$$Debug settings / serialize session attributes option not working$$Session attributes are serialized even if this debug setting is turned off. I've noticed that the code that serializes attributes and logs their serialized size in HttpSessionStore#setAttribute is duplicated in Session#setAttribute - but without the debug settings condition. This code was added by the recent patch resolving WICKET-100 and only in the trunk, not in the wicket-1.x branch... why???  Regards, Bendis
WICKET-b154d12f$$PagingNavigator.setEnabled(false) doesn't work$$1. Create paging navigator PagingNavigator  2. call PagingNavigator.setEnabled(false) 3. navigator will be rendered as enabled, if click on any link ("1", "2" etc) - content of the data view will be changed.  In many cases it's necessary disable navigator, for example, when user need to edit only single line of DataView other controls need to be disabled.
WICKET-01a3dd66$$AjaxFormChoiceComponentUpdatingBehavior affects checkboxes even if component uses radios and vice-versa$$I have a form with two radio buttons.  Depending which radio the user selects, I show one form or another form.  I'm using an AjaxFormChoiceComponentUpdatingBehavior attached to the RadioGroup.  One of the forms has a checkbox.  The checkbox triggers an ajax update--even though the AjaxFormChoiceComponentUpdatingBehavior is attached to a RadioGroup.  AjaxFormChoiceComponentUpdatingBehavior should only affect the appropriate controls based on whether it is attached to a choice component that uses radios or checkboxes.  If a developer really wants both, then he can use two AjaxFormChoiceComponentUpdatingBehavior instances.  I've attached a patch.
WICKET-3431e60d$$missing base64/ URL encoding$$yesterday i showed the concept of omponents to a friend and stumled into something i dont understand and think it might be a bug.    I have a small panelcompoment that holds a searchform (textfield + submit) nothing special here, the code behind looks like:     @Override         public void onSubmit()          {             String suchFeld = getSuchfeld();             if(suchFeld.length()>0)             {                 PageParameters params = new PageParameters();                 params.add("finde",suchFeld);                 setResponsePage(Suche.class,params);             }             else             {                 setResponsePage(getPage().getClass());             }         }   the component is put into a "BasePage":    public BasePage() {         ....             add(bar);         add(new SuchPanel("SuchPanel"));         ..... }   wich is then extended by the real page:   public class Foo extends BasePage{          /** Creates a new instance of Zigarren */     public Foo() {         }   wich works all fine, however if the class name contains non ascii letters (e.g: ö ä ü etc.) it gives me a bug if nothing is entered into the search and the part   public class Zubehör extends BasePage{          /** Creates a new instance of Zubehör */     public Zubehör() {     }   "setResponsePage(getPage().getClass());" comes to action, the trouble is that the page might have the URL: ?wicket:bookmarkablePage=:de.pages.Zubeh%C3%B6r but the form tries to go to :  wicket:bookmarkablePage=:de.pages.Zubeh%F6r   wich results in a CODE 404 in the App Server
WICKET-6c5083b4$$missing base64/ URL encoding$$yesterday i showed the concept of omponents to a friend and stumled into something i dont understand and think it might be a bug.    I have a small panelcompoment that holds a searchform (textfield + submit) nothing special here, the code behind looks like:     @Override         public void onSubmit()          {             String suchFeld = getSuchfeld();             if(suchFeld.length()>0)             {                 PageParameters params = new PageParameters();                 params.add("finde",suchFeld);                 setResponsePage(Suche.class,params);             }             else             {                 setResponsePage(getPage().getClass());             }         }   the component is put into a "BasePage":    public BasePage() {         ....             add(bar);         add(new SuchPanel("SuchPanel"));         ..... }   wich is then extended by the real page:   public class Foo extends BasePage{          /** Creates a new instance of Zigarren */     public Foo() {         }   wich works all fine, however if the class name contains non ascii letters (e.g: ö ä ü etc.) it gives me a bug if nothing is entered into the search and the part   public class Zubehör extends BasePage{          /** Creates a new instance of Zubehör */     public Zubehör() {     }   "setResponsePage(getPage().getClass());" comes to action, the trouble is that the page might have the URL: ?wicket:bookmarkablePage=:de.pages.Zubeh%C3%B6r but the form tries to go to :  wicket:bookmarkablePage=:de.pages.Zubeh%F6r   wich results in a CODE 404 in the App Server
WICKET-bb7f9cf5$$WebPage#onAfterRender erroneously reports missing header$$In WebPage#onAfterRender() there's a check wether a header was missing on a page and header contributions would be lost.  In the following case this check erroneously barks: - page A was requested - in A's onBeforeRender() a RestartResponseAtInterceptPageException to page B is thrown - page A's onAfterRender() is invoked in a finally block - processing continues with page B  Page A's onAfterRender() complains about the missing header, althought his page was never completely rendered.  IMHO there's a check missing in WebPage#onAfterRender():      	if (getRequestCycle().getResponsePage() == this) { 		..... 	}  Or is Page A not allowed to throw RestartResponseAtInterceptPageException in onBeforeRender() at all?
WICKET-99e22ce4$$Component reAttach and versioning$$I'm reAttaching a component doing something like:   MyFooPanel p1 = new MyFooPanel(this, "panel";);  MyBarPanel p2 = new MyBarPanel(this, "panel");  p1.reAttach();   When I try to restore to the initial page version I found that the component with id "panel" is not a children component of the page.   I have investigated it and I think it is because when the component is reAttached the order in which the changes are added to the ChangesList is:  - Add p2.  - Remove p1.   When the initial version is restored the undo functionality is done in reverse mode like,  - Add p1.  - Remove p2.   The problem is p1 and p2 have the same id, so when p2 is removed what is removing is p1 that has just added.   Oscar.
WICKET-5226978a$$WicketTester Cookie handling$$While trying to test my SecureForm implementation (https://issues.apache.org/jira/browse/WICKET-1885) with WicketTester I ran into this issue: A cookie set in the response never shows up in the "next" request, because both have their own lists of cookies that aren't shared.  Afaik both should share the same List instance to handle cookies. That way its possible to set a cookie in the response and read it from the request.  A simple testcase is attached.
WICKET-8ee095bf$$StatelessForm submitted to the wrong page$$I made a small application to reproduce the problem. You can download it from http://aditsu.net/wickettest.zip , I'll try to attach it too. Dependencies: jetty 6, wicket 1.4-m3, slf4j, log4j Steps to reproduce: 1. Run the test.Start class 2. Open http://localhost:8080 in a browser 3. Open http://localhost:8080/page2 in a new tab 4. Go to the first tab and click submit  Result:  WicketRuntimeException: unable to find component with path form on stateless page [Page class = test.Page2, id = 0, version = 0]  It looks like the 2 pages are created with the same id in 2 different pagemaps, but when I submit the form, it goes to the second pagemap and finds the second page (with no form on it).
WICKET-986848f7$$FormTester doesn't correctly submit a form when a FileUploadField was not set (which is not required)$$FormTester doesn't correctly submit a form when  a FileUploadField was not set. This file is not required.  So it is impossible to create a real test because I am forced to always set a File to check to whole form.  There was discussion about this problem here: http://www.nabble.com/FormTester-and-FileUploadField-td18566869.html   I will be very grateful if you can fix it :) Artur
WICKET-420ac965$$&amp; instead of & in javascript$$the non httpsessionstore part of: https://issues.apache.org/jira/browse/WICKET-1971  is that   in the  wicket:ignoreIfNotActive actually becomes  amp;wicket:ignoreIfNotActive=true  in:  	protected CharSequence encode(RequestCycle requestCycle, 			IListenerInterfaceRequestTarget requestTarget)  of WebRequestCodingStrategy on the line:  			url.append(url.indexOf("?") > -1 ? "&amp;" : "?").append( 					IGNORE_IF_NOT_ACTIVE_PARAMETER_NAME).append("=true");   so when this happens in  	public final RequestParameters decode(final Request request) {  --- 		if (request.getParameter(IGNORE_IF_NOT_ACTIVE_PARAMETER_NAME) != null) 		{ 			parameters.setOnlyProcessIfPathActive(true); 		} ---  this never actually happens.   then if you have a throttle, ajaxlazyloadpanel etc with onlyprocessifpathactive set to true, and you logout, but go to another wicket page, then the original session is destroyed and a new one is created  if this is worked around in the way the  guys on WICKET-1971 suggest, WebRequestCycleProcessor  method  	public IRequestTarget resolve(final RequestCycle requestCycle, 			final RequestParameters requestParameters)   				if (requestParameters.isOnlyProcessIfPathActive()) last branch falls through: 					else 					{ 						// TODO also this should work.. 					}   and it throws PageExpiredException because the request component/page/behavior does not exist in this new session.   even though onlyprocessifpathactive was set to true, and it's purpose is precisely to avoid pageexpiredexception.
WICKET-e2d88568$$AjaxPreprocessingCallDecorator calls the delegate decorator before itself (same behavior as AjaxPostprocessingCallDecorator)$$AjaxPreprocessingCallDecorator calls the delegate decorator before itself (same behavior as AjaxPostprocessingCallDecorator), when it should call itself before the delegate.
WICKET-0578d6ee$$Invalid javascript when setStripJavascriptCommentsAndWhitespace is enabled$$When setStripJavascriptCommentsAndWhitespace is enabled (for example in deployment mode), some javascript files get corrupted. For example, the following line (notice the 2 spaces after 'return') return  this.__unbind__(type, fn); is compacted to return this.__unbind__(type, fn); which does not execute the unbind function.
WICKET-9da430fb$$Generated urls for mounted pages contain redundant trailing "/"$$Is it OK (i.e. "by design" as opposed to "by mistake") that the urls generated for the mounted pages end up with the "/"?  Provided that there's a page that expects single parameter (here: "content")... public class HelpPage extends WebPage { public HelpPage(PageParameters p) { super(p); add(new DynamicContentPanel("contentPanel", new Model<String>(p.getString("content")))); } }  ...and it is mounted in the Application#init() mount(new BookmarkablePageRequestTargetUrlCodingStrategy("help", HelpPage.class, null));  ...and further referred to somewhere else as: add(new BookmarkablePageLink("helpPage", HelpPage.class, new PageParameters("content=a")));  the url in the generated markup is in the following form: http://localhost:8080/dummy-web/help/content/a/;jsessionid=11624C6125F8DF4867E3218676D79A29  While IMHO it should read: http://localhost:8080/dummy-web/help/content/a;jsessionid=11624C6125F8DF4867E3218676D79A29  It looks even more awkward when there are more parameters and part of them is encoded as a query string: http://localhost:8080/dummy-web/help/content/a/?param2=value2/;jsessionid=11624C6125F8DF4867E3218676D79A29  The page parameter for both cases is resolved correctly by the HelpPage's constructor, so it seems that even though there's an extra "/" at the end of the url it gets omitted. Then why bother generating it?  I stumbled upon an issue https://issues.apache.org/jira/browse/WICKET-765. Apart from the compatibility with wicket 1.2 I see no rationale for trailing "/". Looking at implementations of IRequestTargetUrlCodingStrategy I come to the conclusion the the "append("/")" is being overused and redundant especially when it is preceded by the following code which makes sure that the "/" is in place before adding another parameter.
WICKET-ceac38b1$$Component Use Check always fails for visible components inside an invisible border body$$None
WICKET-b224bad8$$Fixing AjaxTimerBehaviorTest$$This is an attempt to fix failing testcase:  target/surefire-reports/wicket.ajax.AjaxTimerBehaviorTest.txt ------------------------------------------------------------------------------- Test set: wicket.ajax.AjaxTimerBehaviorTest ------------------------------------------------------------------------------- Tests run: 2, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 0.113 sec <<< FAILURE! testAddToAjaxUpdate(wicket.ajax.AjaxTimerBehaviorTest)  Time elapsed: 0.063 sec  <<< FAILURE! junit.framework.AssertionFailedError: There should be 1 and only 1 script in the markup for this behavior,but 0 were found exp ected:<1> but was:<0>         at junit.framework.Assert.fail(Assert.java:47)         at junit.framework.Assert.failNotEquals(Assert.java:282)         at junit.framework.Assert.assertEquals(Assert.java:64)         at junit.framework.Assert.assertEquals(Assert.java:201)         at wicket.ajax.AjaxTimerBehaviorTest.validateTimerScript(AjaxTimerBehaviorTest.java:178)         at wicket.ajax.AjaxTimerBehaviorTest.validate(AjaxTimerBehaviorTest.java:143)         at wicket.ajax.AjaxTimerBehaviorTest.testAddToAjaxUpdate(AjaxTimerBehaviorTest.java:99)  testAddToWebPage(wicket.ajax.AjaxTimerBehaviorTest)  Time elapsed: 0.026 sec  <<< FAILURE! junit.framework.AssertionFailedError: There should be 1 and only 1 script in the markup for this behavior,but 0 were found exp ected:<1> but was:<0>         at junit.framework.Assert.fail(Assert.java:47)         at junit.framework.Assert.failNotEquals(Assert.java:282)         at junit.framework.Assert.assertEquals(Assert.java:64)         at junit.framework.Assert.assertEquals(Assert.java:201)         at wicket.ajax.AjaxTimerBehaviorTest.validateTimerScript(AjaxTimerBehaviorTest.java:178)         at wicket.ajax.AjaxTimerBehaviorTest.validate(AjaxTimerBehaviorTest.java:155)         at wicket.ajax.AjaxTimerBehaviorTest.testAddToWebPage(AjaxTimerBehaviorTest.java:127)  The attached patch properly handles the case when the callback script is added in body onload.  Also, AbstractAjaxTimerBehavior needs to handle AjaxRequestTarget properly, because adding a body onload has no effect in an ajax request.
WICKET-ea4a3f8a$$PageParameters construced with keyValuePairs does not handle array values$$The PageParameters constructor that takes a "keyValuePairs" argument does not convert repeated keys into an array of values.  For example:  {code} // specify three comma delimited values for the "a" parameters PageParameters parameters = new PageParameters("a=1,a=2,a=3"); String[] a = parameters.getStringArray("a"); assertEquals(3, a.length); // fails because a.length == 1 {code}  Issue first described on the user's list: http://www.nabble.com/PageParameters-with-String-array-question-to22540294.html
WICKET-d79d0192$$Bounds error in PageableListView#getCurrentPage()$$In the getCurrentPage() method of class PageableListView, the following code:  while ((currentPage * rowsPerPage) > getList().size()) {            currentPage--; }  checks if "first cell if out of range". However, the index of that first cell is (currentPage * rowsPerPage), and then the comparison with getList().size() should use a ">=" instead a ">".
WICKET-24ac1a35$$Form gets submitted using AjaxSubmitBehavior when sub-form has error's$$from http://www.nabble.com/Should-a-form-submit-when-sub-form-has-error%27s--tt22803314.html  I have a main-form where I add a panel that contains another form. This sub-form contains a formvalidator that gives the error. However the main-form is submitted, but the feedbackpanel does show the error message set in the sub-form's validator.  I'll attach 2 patches with testcases displaying the behavior in wicket 1.3 vs 1.4  (As a side note, I had to rename the org.apache.wicket.markup.html.form.validation.TestHomePage to org.apache.wicket.markup.html.form.validation.HomePageTest to get the test to run when building wicket)
WICKET-089303f4$$wicketTester.executeAjaxEvent(combo, "onchange"); works with 1.4-rc1 but not anymore with 1.4-rc2$$Try the attached Unit Test.
WICKET-6e0b40bc$$MockHttpServletRequest is broken when used with CryptedUrlWebRequestCodingStrategy$$Upgraded to 1.3.6. One of my test cases started to fail with  org.apache.wicket.WicketRuntimeException: Internal error parsing wicket:interface = ?x=GR7uTj8e-D8FE0tmM9vvYcwdiASd9OJ5GgveAhSNaig       I tracked down the issue to MockHttpServletRequest .setRequestToComponent() In line 1253 it check for url starting with 6*. However, in CryptedUrlWebRequestCodingStrategy following encryption is employed:  198:					queryString = shortenUrl(queryString).toString(); 199: 200:					// encrypt the query string 201:					String encryptedQueryString = urlCrypt.encryptUrlSafe(queryString);   shortenUrl will replace 'wicket:interface=' with '6*' but then it gets immediately encrypted, consequently MockHttpServletRequest  will never recognize it correctly.
WICKET-96330447$$DebugBar throws an java.lang.ExceptionInInitializerError when Tomcat is restarted$$I have just added the DebugBar to our base page, and since then when Tomcat is restarted and session would be reloaded by this it throws this exception:  1    ERROR org.apache.catalina.session.ManagerBase  - Exception loading sessions from persistent storage java.lang.ExceptionInInitializerError 	at sun.misc.Unsafe.ensureClassInitialized(Native Method) 	at sun.reflect.UnsafeFieldAccessorFactory.newFieldAccessor(UnsafeFieldAccessorFactory.java:25) 	at sun.reflect.ReflectionFactory.newFieldAccessor(ReflectionFactory.java:122) 	at java.lang.reflect.Field.acquireFieldAccessor(Field.java:918) 	at java.lang.reflect.Field.getFieldAccessor(Field.java:899) 	at java.lang.reflect.Field.getLong(Field.java:528) 	at java.io.ObjectStreamClass.getDeclaredSUID(ObjectStreamClass.java:1614) 	at java.io.ObjectStreamClass.access 700(ObjectStreamClass.java:52) 	at java.io.ObjectStreamClass 2.run(ObjectStreamClass.java:425) 	at java.security.AccessController.doPrivileged(Native Method) 	at java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:413) 	at java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:310) 	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:547) 	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1583) 	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1496) 	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1583) 	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1496) 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1732) 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329) 	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1667) 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1323) 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1947) 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1871) 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753) 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329) 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1947) 	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:480) 	at org.apache.wicket.Component.readObject(Component.java:4469) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:597) 	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:974) 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1849) 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753) 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329) 	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1667) 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1323) 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1947) 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1871) 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753) 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329) 	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:351) 	at java.util.concurrent.CopyOnWriteArrayList.readObject(CopyOnWriteArrayList.java:845) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:597) 	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:974) 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1849) 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753) 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329) 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1947) 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1871) 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753) 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329) 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1947) 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1871) 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753) 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329) 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1947) 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1871) 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753) 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329) 	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1667) 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1323) 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1947) 	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:480) 	at org.apache.wicket.Page.readPageObject(Page.java:1349) 	at org.apache.wicket.Component.readObject(Component.java:4465) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:597) 	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:974) 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1849) 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753) 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329) 	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:351) 	at org.apache.wicket.protocol.http.SecondLevelCacheSessionStore SecondLevelCachePageMap.readObject(SecondLevelCacheSessionStore.java:412) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:597) 	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:974) 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1849) 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753) 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329) 	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:351) 	at org.apache.catalina.session.StandardSession.readObject(StandardSession.java:1407) 	at org.apache.catalina.session.StandardSession.readObjectData(StandardSession.java:931) 	at org.apache.catalina.session.StandardManager.doLoad(StandardManager.java:394) 	at org.apache.catalina.session.StandardManager.load(StandardManager.java:321) 	at org.apache.catalina.session.StandardManager.start(StandardManager.java:637) 	at org.apache.catalina.core.ContainerBase.setManager(ContainerBase.java:432) 	at org.apache.catalina.core.StandardContext.start(StandardContext.java:4160) 	at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1014) 	at org.apache.catalina.core.StandardHost.start(StandardHost.java:736) 	at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1014) 	at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:443) 	at org.apache.catalina.core.StandardService.start(StandardService.java:448) 	at org.apache.catalina.core.StandardServer.start(StandardServer.java:700) 	at org.apache.catalina.startup.Catalina.start(Catalina.java:552) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:597) 	at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:295) 	at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:433) Caused by: org.apache.wicket.WicketRuntimeException: There is no application attached to current thread main 	at org.apache.wicket.Application.get(Application.java:178) 	at org.apache.wicket.devutils.debugbar.DebugBar.getContributors(DebugBar.java:146) 	at org.apache.wicket.devutils.debugbar.DebugBar.registerContributor(DebugBar.java:140) 	at org.apache.wicket.devutils.debugbar.DebugBar.registerStandardContributors(DebugBar.java:152) 	at org.apache.wicket.devutils.debugbar.DebugBar.<clinit>(DebugBar.java:65) 	... 109 more
WICKET-36a41358$$IndexOutOfBoundsException when PropertyResolver is using an invalid list index$$When using  PropertyResolver.getValue("myList[1]", myBean),  the PropertyResolver ListGetSet.getValue() (line 762) unconditionally does: return ((List)object).get(index); which throws an   java.lang.IndexOutOfBoundsException: Index: 1, Size: 1  if the backing list contains only one element (at index 0). Shouldn't the implementation rather return null like with every other property not found? Like when using "bla.bli.blo" as a lookup string and there is no bla field and no getBla() method?  So this method should rather be:  org.apache.wicket.util.lang.PropertyResolver ListGetSet.getValue():  		/** 		 * @see org.apache.wicket.util.lang.PropertyResolver.IGetAndSet#getValue(java.lang.Object) 		 */ 		public Object getValue(Object object) 		{ 			List list = (List) object; 			if (index >= list.size()) { 				return null; 			} 			return list.get(index); 		}
WICKET-cd281092$$Localization messages stops working with validators since 1.4-rc2$$With the previous 1.3.6 and 1.4-rc1 releases I was capable to restrict a localization message for a validation to only one wicket id e.g. :  in foobar.java RequiredTextField nameTF = new RequiredTextField("name"); nameTF.add(StringValidator.lengthBetween(2, 255)); nameTF.add(new PatternValidator("[^|:]*"));  and in foobar.properties name.Required=some text name.StringValidator.range=some other text name.PatternValidator=some other text again  So, like this I could have to create an another RequiredTextField named "password", and attach to it a different localization message (for example "password.Required=blabla").  But somehow with the 1.4-rc2-5 it looks like that this function is broken, it only recognizes the localization text, when I remove the "name." prefix from my property.
WICKET-fae1601b$$Page.checkRendering fails after setting BorderBodyContainer visiblity to false$$After toggling visibility of the BorderBodyContainer to false the Page.checkRendering method fails in line 1157, claiming an iterator IllegalStateException. This happens because iterator.remove() is called twice for a child component in the border component, if the body is not visible.  My Code:  public class TogglePanel extends Border { 	private boolean expanded = true;  	public TogglePanel(String id, IModel<String> titleModel) { 		super(id, titleModel);  		Link link = new Link("title") {  			@Override 			public void onClick() { 				expanded = !expanded; 				getBodyContainer().setVisible(expanded); 			} 		}; 		link.add(new Label("titleLabel", titleModel));  		add(link); 	}  }  Markup:  <wicket:border> 	<h3 class="collapse" wicket:id="title"> 		<span class="label" wicket:id="titleLabel">Panel Title</span> 		<a class="foldicon">&nbsp;</a> 	</h3> 	<wicket:body /> </wicket:border>
WICKET-0f8a2990$$Regression: "Could not find child with id: <ID> in the wicket:enclosure" for non-component tag$$Attached testcase passes with wicket-1.4.1 but fails with 1.4.2 saying:  org.apache.wicket.WicketRuntimeException: Could not find child with id: radio in the wicket:enclosure 	at org.apache.wicket.markup.html.internal.Enclosure.checkChildComponent(Enclosure.java:210) 	at org.apache.wicket.markup.html.internal.Enclosure.ensureAllChildrenPresent(Enclosure.java:249) 	at org.apache.wicket.markup.html.internal.Enclosure.onComponentTagBody(Enclosure.java:169) 	at org.apache.wicket.Component.renderComponent(Component.java:2626) 	at org.apache.wicket.MarkupContainer.onRender(MarkupContainer.java:1512) 	at org.apache.wicket.Component.render(Component.java:2457) 	at org.apache.wicket.MarkupContainer.autoAdd(MarkupContainer.java:229) 	at org.apache.wicket.markup.resolver.EnclosureResolver.resolve(EnclosureResolver.java:61) 	at org.apache.wicket.markup.resolver.ComponentResolvers.resolve(ComponentResolvers.java:81) 	at org.apache.wicket.MarkupContainer.renderNext(MarkupContainer.java:1418) 	at org.apache.wicket.MarkupContainer.renderComponentTagBody(MarkupContainer.java:1577) 	at org.apache.wicket.MarkupContainer.onComponentTagBody(MarkupContainer.java:1501) 	at org.apache.wicket.Component.renderComponent(Component.java:2626) 	at org.apache.wicket.MarkupContainer.onRender(MarkupContainer.java:1512) 	at org.apache.wicket.Component.render(Component.java:2457) 	at org.apache.wicket.MarkupContainer.renderNext(MarkupContainer.java:1414) 	at org.apache.wicket.MarkupContainer.renderAll(MarkupContainer.java:1528) 	at org.apache.wicket.Page.onRender(Page.java:1545) 	at org.apache.wicket.Component.render(Component.java:2457) 	at org.apache.wicket.Page.renderPage(Page.java:914) 	at org.apache.wicket.request.target.component.BookmarkablePageRequestTarget.respond(BookmarkablePageRequestTarget.java:262) 	at org.apache.wicket.request.AbstractRequestCycleProcessor.respond(AbstractRequestCycleProcessor.java:105) 	at org.apache.wicket.RequestCycle.processEventsAndRespond(RequestCycle.java:1258) 	at org.apache.wicket.RequestCycle.step(RequestCycle.java:1329) 	at org.apache.wicket.RequestCycle.steps(RequestCycle.java:1428) 	at org.apache.wicket.RequestCycle.request(RequestCycle.java:594) 	at org.apache.wicket.protocol.http.MockWebApplication.processRequestCycle(MockWebApplication.java:478) 	at org.apache.wicket.protocol.http.MockWebApplication.processRequestCycle(MockWebApplication.java:390) 	at org.apache.wicket.util.tester.BaseWicketTester.startPage(BaseWicketTester.java:300) 	at org.apache.wicket.EnclosurePageTest.testRender(EnclosurePageTest.java:23) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:597) 	at junit.framework.TestCase.runTest(TestCase.java:154) 	at junit.framework.TestCase.runBare(TestCase.java:127) 	at junit.framework.TestResult 1.protect(TestResult.java:106) 	at junit.framework.TestResult.runProtected(TestResult.java:124) 	at junit.framework.TestResult.run(TestResult.java:109) 	at junit.framework.TestCase.run(TestCase.java:118) 	at junit.framework.TestSuite.runTest(TestSuite.java:208) 	at junit.framework.TestSuite.run(TestSuite.java:203) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:597) 	at org.apache.maven.surefire.junit.JUnitTestSet.execute(JUnitTestSet.java:213) 	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:140) 	at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:127) 	at org.apache.maven.surefire.Surefire.run(Surefire.java:177) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:597) 	at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:345) 	at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1009)
WICKET-12e1f39b$$CreditCardValidator accepts invalid inputs$$(1) The onValidate() method of the CreditCardValidator class returns true for invalid inputs with null or unicode character such as 4\0\0\0\0\0\0\0\0\0\0\0\0\0\0.  (2) Also there is no length check on the input, therefore even invalid length inputs such as 9845 are accepted.  (3) There is no check for invalid issuer identifier, i.e.,  840898920205250 is accepted, where 84XXXX is not a valid issuer identifier
WICKET-9ced53a5$$Inheritance layout excludes XML header from output$$When using inheritance layout, if the superclass (Layout class) has an ?xml header at the top, it's excluded from the rendering of subclasses, if they have an associated html file. If the subclass has no .html file associated with it, the ?xml header is preserved in the rendering output.  To reproduce: Create a SuperPage class extending WebPage. At the top of SuperPage.html, put "<?xml version="1.0" encoding="utf-8"?>" . Create two subclasses of SuperPage, one with an HTML file and one without. View the sub pages. Notice when the one with an HTML file is rendered, the xml header is excluded.  Expected: The ?xml header should always be preserved in the rendered output as it's vital to the layout.
WICKET-7da4ad17$$EnumChoiceRenderer misbehaves with anonymous enum classes$$Please find attached testcase reproducing the problem.  Proper fix is to do return object.getDeclaringClass().getSimpleName() + "." + object.name()  instead of return object.getClass().getSimpleName() + "." + object.name()  in EnumChoiceRenderer.resourceKey
WICKET-c849f986$$Ajax buttons inside ModalWindows don't submit properly$$I have a ModalWindow that contains an IndicatingAjaxButton. When I click the button, I get a big Java error complaining that the form submit wasn't multipart.  Digging into the javascript in wicket-ajax.js, I found this from line 1102 in the method handleMultipart  {code} multipart=multipart||form.enctype=="multipart/form-data";  if (multipart==false) {      // nothing to handle     return false;  } {code}  When this executed, multipart was false, and enctype was "" and therefore the submit aborted. This may be the cause.  Here's the Java stacktrace  {noformat} java.lang.IllegalStateException: ServletRequest does not contain multipart content 	at org.apache.wicket.protocol.http.servlet.MultipartServletWebRequest.<init>(MultipartServletWebRequest.java:113) 	at org.apache.wicket.protocol.http.servlet.MultipartServletWebRequest.<init>(MultipartServletWebRequest.java:83) 	at org.apache.wicket.extensions.ajax.markup.html.form.upload.MultipartRequest.<init>(MultipartRequest.java:41) 	at org.apache.wicket.extensions.ajax.markup.html.form.upload.UploadWebRequest.newMultipartWebRequest(UploadWebRequest.java:66) 	at org.apache.wicket.markup.html.form.Form.handleMultiPart(Form.java:1651) 	at org.apache.wicket.markup.html.form.Form.onFormSubmitted(Form.java:850) 	at org.apache.wicket.ajax.form.AjaxFormSubmitBehavior.onEvent(AjaxFormSubmitBehavior.java:135) 	at org.apache.wicket.ajax.AjaxEventBehavior.respond(AjaxEventBehavior.java:177) 	at org.apache.wicket.ajax.AbstractDefaultAjaxBehavior.onRequest(AbstractDefaultAjaxBehavior.java:299) 	at org.apache.wicket.request.target.component.listener.BehaviorRequestTarget.processEvents(BehaviorRequestTarget.java:119) 	at org.apache.wicket.request.AbstractRequestCycleProcessor.processEvents(AbstractRequestCycleProcessor.java:92) 	at org.apache.wicket.RequestCycle.processEventsAndRespond(RequestCycle.java:1250) 	at org.apache.wicket.RequestCycle.step(RequestCycle.java:1329) 	at org.apache.wicket.RequestCycle.steps(RequestCycle.java:1428) 	at org.apache.wicket.RequestCycle.request(RequestCycle.java:545) 	at org.apache.wicket.protocol.http.WicketFilter.doGet(WicketFilter.java:479) 	at org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:312) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235) {noformat}
WICKET-ef880545$$MethodGetAndSet.setValue uses wrong source to determine which type to convert to when there's no setter$$MethodGetAndSet.setValue uses wrong source to determine which type to convert to when there's no setter, resulting in exceptions like this: org.apache.wicket.WicketRuntimeException: Error setting field: private int PropertyResolverTest DirectFieldSetWithDifferentTypeThanGetter.value on object: PropertyResolverTest DirectFieldSetWithDifferentTypeThanGetter@396477d9 	at org.apache.wicket.util.lang.PropertyResolver MethodGetAndSet.setValue(PropertyResolver.java:1150) 	at org.apache.wicket.util.lang.PropertyResolver ObjectAndGetSetter.setValue(PropertyResolver.java:588) 	at org.apache.wicket.util.lang.PropertyResolver.setValue(PropertyResolver.java:136) 	at PropertyResolverTest.testDirectFieldSetWithDifferentTypeThanGetter(PropertyResolverTest.java:12)  Bug is located in: converted = converter.convert(value, getMethod.getReturnType());  Instead, it should read: converted = converter.convert(value, type);  Testcase attached.  Additional thoughts: if (setMethod != null) {   type = getMethod.getReturnType(); } This is really confusing (we check setMethod presence but get type from getMethod). Luckily, this works as expected because in MethodGetAndSet.findSetter only methods with same (or superclass) type as getter are returned.
WICKET-15477252$$ajax not working due to bugs in resource handling$$A couple of bugs were found that were preventing .js resources to be returned to the client correctly. One bug was returning the jar file size as the content length of the resource if it is in a jar file. The other was copying past a source buffer into the response.  After fixing these bugs, the ajax functions in the trunk seems to be working.  A patch is provided. Test cases included.
WICKET-ebe56869$$Dynamically adding component via an IComponentResolver fails within an enclosure for versions after 1.4.1$$We have been using an IComponentResolver implementation for a long time to allow the inclusion of certain panels to be determined by the markup. Some panels are included inside enclosures and some are not. Both cases worked fine in wicket 1.4.1 but in versions 1.4.2 and later a 'Tag expected' error occurs if the component is wrapped inside a wicket enclosure.  A quickstart example has been included to demonstrate the problem.
WICKET-0e70ce39$$"isPrimary" check is not applied to beans in parent contexts$$see this comment in WICKET-2771: https://issues.apache.org/jira/browse/WICKET-2771?focusedCommentId=12872246&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12872246
WICKET-5c592d85$$WebRequestCodingStrategy: path mounting and matching$$Assuming a mount path to "/p", it will match /pxyz  Assuming this is the desired behavior of matching (warning), then to avoid this match it should be declared "/p/" but it will create urls such as '/app/p//SomePage'. which is wrong.  In the servlet specs,  the mapping syntax '/p' is an exact match, this is not what you want in your case since you're doing path mapping, so the syntax if you want to stick close to the servlet specs should be '/p/*' or if you wan to get close to mod_proxy syntax it would be '/p/'  Note that the examples are also using this wrong mapping declaration. In the example below: both should throw a 404: http://www.wicket-library.com/wicket-examples/niceurl/my/mounted/packageXXX http://www.wicket-library.com/wicket-examples/niceurl/my/mounted/Xpackage
WICKET-3d8c9d75$$Adding a component in Component#onInitialize() leads to StackOverflowError$$Adding a component in Page#onInitialize() leads to StackOverflowError:   at org.apache.wicket.MarkupContainer.addedComponent(MarkupContainer.java:978)      at org.apache.wicket.MarkupContainer.add(MarkupContainer.java:168)      at org.apache.wicket.examples.WicketExamplePage.onInitialize(WicketExamplePage.java:67)      at org.apache.wicket.Component.initialize(Component.java:970)      at org.apache.wicket.MarkupContainer.initialize(MarkupContainer.java:992)      at org.apache.wicket.Page.componentAdded(Page.java:1130)      at org.apache.wicket.MarkupContainer.addedComponent(MarkupContainer.java:978)      at org.apache.wicket.MarkupContainer.add(MarkupContainer.java:168)      at org.apache.wicket.examples.WicketExamplePage.onInitialize(WicketExamplePage.java:67)      at org.apache.wicket.Component.initialize(Component.java:970)      at org.apache.wicket.MarkupContainer.initialize(MarkupContainer.java:992)      at org.apache.wicket.Page.componentAdded(Page.java:1130)      at org.apache.wicket.MarkupContainer.addedComponent(MarkupContainer.java:978)      at org.apache.wicket.MarkupContainer.add(MarkupContainer.java:168)      at org.apache.wicket.examples.WicketExamplePage.onInitialize(WicketExamplePage.java:67)      at org.apache.wicket.Component.initialize(Component.java:970)      at org.apache.wicket.MarkupContainer.initialize(MarkupContainer.java:992)      at org.apache.wicket.Page.componentAdded(Page.java:1130)      at org.apache.wicket.MarkupContainer.addedComponent(MarkupContainer.java:978)      at org.apache.wicket.MarkupContainer.add(MarkupContainer.java:168)      at org.apache.wicket.examples.WicketExamplePage.onInitialize(WicketExamplePage.java:67)      at org.apache.wicket.Component.initialize(Component.java:970)      at org.apache.wicket.MarkupContainer.initialize(MarkupContainer.java:992)      at org.apache.wicket.Page.componentAdded(Page.java:1130)      at org.apache.wicket.MarkupContainer.addedComponent(MarkupContainer.java:978)      at org.apache.wicket.MarkupContainer.add(MarkupContainer.java:168)      at org.apache.wicket.examples.WicketExamplePage.onInitialize(WicketExamplePage.java:67)      at org.apache.wicket.Component.initialize(Component.java:970) ...
WICKET-0b4f78cc$$ClassCastException when requesting for non-page class$$org.apache.wicket.request.mapper.BookmarkableMapper tries to instantiate Page even for classes which are not Page. Requesting http://localhost:8080/wicket/bookmarkable/com.mycompany.Pojo fails with:  ERROR - DefaultExceptionMapper     - Unexpected error occurred java.lang.ClassCastException: com.mycompany.Pojo 	at org.apache.wicket.session.DefaultPageFactory.newPage(DefaultPageFactory.java:155) 	at org.apache.wicket.session.DefaultPageFactory.newPage(DefaultPageFactory.java:59) 	at org.apache.wicket.session.DefaultPageFactory.newPage(DefaultPageFactory.java:43) 	at org.apache.wicket.Application 2.newPageInstance(Application.java:1425) 	at org.apache.wicket.request.handler.PageProvider.getPageInstance(PageProvider.java:259) 	at org.apache.wicket.request.handler.PageProvider.getPageInstance(PageProvider.java:160) 	at org.apache.wicket.request.handler.render.WebPageRenderer.getPage(WebPageRenderer.java:59) 	at org.apache.wicket.request.handler.render.WebPageRenderer.renderPage(WebPageRenderer.java:131) 	at org.apache.wicket.request.handler.render.WebPageRenderer.respond(WebPageRenderer.java:232) 	at org.apache.wicket.request.handler.RenderPageRequestHandler.respond(RenderPageRequestHandler.java:147) 	at org.apache.wicket.request.RequestHandlerStack.executeRequestHandler(RequestHandlerStack.java:84) 	at org.apache.wicket.request.cycle.RequestCycle.processRequest(RequestCycle.java:217) 	at org.apache.wicket.request.cycle.RequestCycle.processRequestAndDetach(RequestCycle.java:253) 	at org.apache.wicket.protocol.http.WicketFilter.processRequest(WicketFilter.java:135) 	at org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:188) 	at org.mortbay.jetty.servlet.ServletHandler CachedChain.doFilter(ServletHandler.java:1157)          .....
WICKET-b293b75c$$HomePageMapper ignores request to '/' with query string parameters$$Issue a request to http://host:port/contextpath/?something Wicket will log an error message like: ERROR - RequestCycle               - Unable to execute request. No suitable RequestHandler found. URL=?something  I think the reason is in HomePageMapper which maps to the configured home page only if there are no query parameters.  HomePageMapper.java: {code} public IRequestHandler mapRequest(Request request) 	{ 		if (request.getUrl().getSegments().size() == 0 && 			request.getUrl().getQueryParameters().size() == 0) 		{ 			return new RenderPageRequestHandler(new PageProvider(getContext().getHomePageClass())); 		} 		else 		{ 			return null; 		} 	} {code}
WICKET-d3dc9a50$$UrlUtils.isRelative returns false if URL parameter contains an absolute URL$$I have a page that gets a return path for a back link as a parameter. A link to this page looks like this:  ./mypage?return=http://example.com  In WebRequestCodingStrategy.encode, this URL is returned by pathForTarget. Then it is checked whether this URL is relative using UrlUtils.isRelative. The URL is apparently relative, but UrlUtils.isRelative returns false, since the check contains:  (url.indexOf("://") < 0  this is false for the above example. Thus, an incorrect path is returned by WebRequestCodingStrategy.encode (relative path resolution does not take place).  A fix for the problem would be to check for   !(url.startsWith("http://") || url.startsWith("https://"))  Or, if other protocols should also be supported, a regular expression like "^[^/?]*://" should work.
WICKET-1b7afefc$$AjaxEventBehavior#onEvent is invoked on disabled behavior$$Security bug  AjaxEventBehavior#onEvent is invoked on disabled behavior. It should not be - it is really dangerous, can you fix it.  I think it is security bug.
WICKET-4d7f7359$$isVisibleInHierarchy() possibly unnecessarily checks children whose parents are invisible?$$Hi!  See attached quickstart with junit test reproducing the bug. See also patch proposal.  I have a page with two panels:  page.form.add(panel1); page.form.add(panel2);  in some situations panel1 is not visible.  However, a form submit event will visit all formcomponents of panel1 via:         at org.apache.wicket.markup.html.form.FormComponent.visitFormComponentsPostOrder(FormComponent.java:400)        at org.apache.wicket.markup.html.form.Form.visitFormComponentsPostOrder(Form.java:1209)        at org.apache.wicket.markup.html.form.Form.inputChanged(Form.java:1403)        at org.apache.wicket.markup.html.form.Form.onFormSubmitted(Form.java:865)  This results in a crash because panel1 components are not prepared to be invoked via isvisible when the panel itself is not visible.  I wonder if the component.isVisibleInHierarchy could be changed as follows, to first check parent visibility:   public final boolean isVisibleInHierarchy()  {    Component component = this;    while (component != null)    {      Component componentParent = component.getParent();       if (((componentParent == null) || componentParent.isVisibleInHierarchy()) && component.determineVisibility())      {        component = componentParent;      }      else      {        return false;      }    }    return true;  }  Similar change could/should maybe be possible also for isEnabledInHierarchy ?
WICKET-0cf14725$$SmartLinkLabel failing to process email with +$$Using SmartLinkLabel with an email address that includes a "+" generates a link only on the right-most part of the address.  Example: - my+test@example.com Will generate a link like: - my+<a href="mailto:test@example.com">test@example.com@pappin.ca</a>  THe addition of the "+" char is a valid email address format.
WICKET-f1c0f263$$UrlValidator failes to validate urls that containt multiple dots in path$$refer to UrlValidator.java:466 (isValidPath). if we have an url, that contains more than two consequent dots, for example "http://www.somedomain.com/this_one_is_tricky...but...still.....valid", validator will fail. btw, the other side effect is that countTokens actually counts '...' a two 2dots. One possible workaround is not just count '..' tokens, but count them along with slash, like '../'.
WICKET-be70e608$$getMarkupId() can be used only if the component's markup is attached$$With change r1037139 Component#getMarkupImpl() first tries to get the markup id from the component's markup. If the markup is not available/attached yet for this component the call ends with : org.apache.wicket.markup.MarkupException: Can not determine Markup. Component is not yet connected to a parent. [Component id = label]
WICKET-5729ed90$$AbstractMarkupParser doesn't remove Comments correctly$$AbstractMarkupParser removeComment(...) doesn't remove Comments correctly  if two html comments stand to close together <!-- foo --> <!-- bar --> foo will be removed but not bar.  see:  https://github.com/mafulafunk/wicketComments  git@github.com:mafulafunk/wicketComments.git
WICKET-71b6e905$$NPE with nested property models$$After updated from 1.4.8 to 1.4.14 I got this bug.  The problem is with nested property models where the "top" model has a null model object that is bound to a TextField. You get a NPE when the page is rendered. There is a quick workaround by overriding getOjbectClass() on the property model.  I provide a running example of the problem.
WICKET-c86b972a$$Set an request parameter on Wicket tester do not add it in the request URL$$When submitting an form, the parameters set in request do not get appended to the URL query string. Initial impression is that UrlRender should append query parameters in the base URL on relatives URL.
WICKET-60d07288$$DropDownChoice no selection value$$This problem came from this topic: http://apache-wicket.1842946.n4.nabble.com/DropDownChoice-no-selection-value-td3160661.html  I've noticed that the method AbstractSingleSelectChoice.getNoSelectionValue() returns the value for no selection. In AbstractSingleSelectChoice.getDefaultChoice(final Object selected) on line 314:        return "\n<option selected=\"selected\" value=\"\">" + option + "</option>";   and on line 296:        buffer.append(" value=\"\">").append(option).append("</option>");   In those cases the null value option has empty value attribute. Wouldn't it be more consistent for this option to have the "value" attribute with the result provided from getNoSelectionValue()?
WICKET-295e73bd$$IResponseFilter doesn't work in 1.5$$In current 1.5-SNAPSHOT there are no callers of org.apache.wicket.settings.IRequestCycleSettings.getResponseFilters() and thus filters are never executed.
WICKET-71499e17$$UrlAttributes are encoded incorrectly when style is null but variation is not$$AbstractResourceReferenceMapper.encodeResourceReferenceAttributes() method generates the same "-foo" output for these two different inputs: {locale = null, style = "foo", variation = null} and {locale = null, style = null, variation = "foo"}. For the second input it should generate "--foo" (double dash prefix).
WICKET-7e7ab76c$$TextField ingnores convertEmptyInputStringToNull = true property when the String type is set$$I posted this patch on WICKET-3269, but the discussion on this ticket is about an improvement request, not a bug. I opened this one for the bug.
WICKET-debca73b$$unable to add nodes to an empty rootless Tree (e.g. LinkTree)$$2 scenarios which adding new nodes (via ajax) to a rootless Tree is not working as expected. the node is getting added to the treemodel but non is displayed.  1) adding a node to the rootnode. the newly added node is not displayed. 2) the rootless tree already has a node. if you add additional nodes to the root node, they will be displayed (compare to 1), if you add an additional node to one of the added nodes, the complete tree will disappear.  see attached quickstart
WICKET-ddf7e8a2$$Links with multiple parameters are wrongly generated$$If you have a PageParameters, with multiple params, then the resulting link will be something like this /url?id=123&amp;sid=456, so for some reason the & sign is encoded to &amp; which will result in the following parameters on the receiving page: id=[123], amp;sid=[456] See the attached quickstart for example.
WICKET-499a9c6b$$FLAG_INHERITABLE_MODEL and default model change$$The issue is about correctness of Component#setDefaultModel (Component#setModelImpl) method behavior. I expect that the flag FLAG_INHERITABLE_MODEL should be checked there and turned off in case if new model is not a IComponentInheritedModel.   Let check the next code: public MyPanel(String id) {  super(id);   ...   form.setModel(new CompoundPropertyModel(this));   DropDownChoice ddc = new DropDownChoice("variant", Arrays.ofList(...)) {    // p1     @Override     protected void onInitialize() {        super.onInitialize();        setModel(new DefaultingWrapModel(getModel(), Model.of("default value"));            // p2     }   };   ddc.setNullValid(false);   ddc.setRequired(true);   form.add(ddc);   ... }  In the (p1) the DDC will initialize with CompoundPropertyModel and the FLAG_INHERITABLE_MODEL will be turned on soon by the first invocation of FormComponent#getModel().   In the (p2) we wrap the DDC model with the model which provide the default value (DefaultingWrapModel implements IWrapModel). So we change the model, but the FLAG_INHERITABLE_MODEL is still turned on. On the Component#detach() event, the method Component#setModelImpl(null) will be invoked for the ddc and the DefaultingWrapModel instance will be lost:  		// reset the model to null when the current model is a IWrapModel and 		// the model that created it/wrapped in it is a IComponentInheritedModel 		// The model will be created next time. 		if (getFlag(FLAG_INHERITABLE_MODEL)) 		{ 			setModelImpl(null); 			setFlag(FLAG_INHERITABLE_MODEL, false); 		}  I think that such behavior is unexpected.  http://apache-wicket.1842946.n4.nabble.com/1-4-15-FLAG-INHERITABLE-MODEL-and-default-model-change-td3252093.html
WICKET-be97d017$$javascript with a less than character ("<") fails to execute when added through a header contribution in ajax response$$This is adapted from a wicket users post I made (links are to the same thread in two archive systems):  http://markmail.org/search/?q=wicket%20users%20wicket-ajax.js#query:wicket%20users%20wicket-ajax.js+page:1+mid:rfts3ar3upffhbbt+state:results  http://mail-archives.apache.org/mod_mbox/wicket-users/201102.mbox/%3CAANLkTi=EkmTA0RnA+GyJE-CQWmkCxRLsjp+z8jwv-Aw9@mail.gmail.com%3E  The problem:  I have a panel with this:      <wicket:head> 	<script> 		if (someVariable < 0) { 			someVariable = 0;		 		} 	</script>     </wicket:head>  This script fails to execute when the panel is loaded by ajax.  If I replace the less than character "<" with equals "==", then it executes (but of course, this is not what I need).  I tested this in Firefox 4.0b10 and Chrome 8.  After some debugging, it seems to me that this needs to be corrected in wicket-ajax.js. The header contribution is sent to the browser inside of a CDATA section so the "<" character arrives to javascript intact. However, in parsing the script tag, the "<" seems to signal the beginning of an HTML tag that then is considered malformed.   Possible workarounds for apps:   - Invert the logic so a greater-than is used. In my example, this would be: "if (0 > someVariable) {"  - Put the code into a separate JS file (the downside is it requires another network hop from the browser)  - Embed the script in <wicket:panel> rather than <wicket:head> (the disadvantage is the script will be re-sent with the panel content when the panel is re-used on the same page)
WICKET-ffc0cae9$$IRequestCycleListener: RequestCycle.get() is null inside onBeginRequest$$I expect the request cycle that is supplied as an argument to onBeginRequest to be the same as RequestCycle.get.  == == == CODE == == ==      @Override     public void onBeginRequest(RequestCycle cycle) {         Session session = Session.get(); // throws IllegalArgumentException         if (session.getMetaData(REDIRECTED_JSESSIONID) == null) {             logger.debug("first application request - redirecting to loading page");             session.setMetaData(REDIRECTED_JSESSIONID, Boolean.TRUE);             String url = getServletRequestContextPath() + "/" + cycle.getRequest().getUrl();             throw new RestartResponseException(newLoadingPage(url));         }     } == == == == == == == ==   == == == STACK TRACE == == ==  java.lang.IllegalArgumentException: Argument 'requestCycle' may not be null.     at org.apache.wicket.util.lang.Args.notNull(Args.java:37)     at org.apache.wicket.Application.fetchCreateAndSetSession(Application.java:1436)     at org.apache.wicket.Session.get(Session.java:154)     at com.joynit.tuv.common.view.request.SessionIdRemoveListener.onBeginRequest(SessionIdRemoveListener.java:30)  ... [snipped -other part is not relevant]  == == == == == == == == == == ==
WICKET-f1e854b3$$Value exchange in a wicket:message throws an exception$$i tried to exchange values in a <wicket:message> like described in wiki <https://cwiki.apache.org/WICKET/wickets-xhtml-tags.html#Wicket%27sXHTMLtags-Elementwicket:message>. But i get an exception: ERROR - RequestCycle               - No get method defined for class: class org.apache.wicket.markup.resolver.MarkupInheritanceResolver TransparentWebMarkupContainer expression: vat1value org.apache.wicket.WicketRuntimeException: No get method defined for class: class org.apache.wicket.markup.resolver.MarkupInheritanceResolver TransparentWebMarkupContainer expression: vat1value at org.apache.wicket.util.lang.PropertyResolver.getGetAndSetter(PropertyResolver.java:488) at org.apache.wicket.util.lang.PropertyResolver.getObjectAndGetSetter(PropertyResolver.java:330) at org.apache.wicket.util.lang.PropertyResolver.getObjectAndGetSetter(PropertyResolver.java:237) ...  Maybe it's caused by usage of border. I've debugged a bit, but could get a real glue.  I added a quick start with test case.
WICKET-f30bd1cb$$onremove() in RefreshingView.onPopulate$$file a bug with a quickstart. onremove() should be called on all removed components.  -igor  On Fri, Feb 18, 2011 at 5:38 AM, Benedikt Rothe <benedikt.rothe@qleo.de> wrote: > > Hi > > > > Are the existing children of a RepeatingView/RefreshingView being informed, > > when > > the View is newly populated (RefreshingView.onPopulate). > > > > I'd like to clean some internal references in this case. > > I tried: > > - aChild.onRemove is not called in this situation > > - aChild.setParent(null) is called. I treid to override setParent it. But > > setParent is private. > > > > Any suggestions? > > Benedikt
WICKET-292a2582$$DateTimeField improperly converts time causing wrong dates when the server's current date is different from the client's date.$$The bug is in DateTimeField#convertInput(). <code> // Get year, month and day ignoring any timezone of the Date object Calendar cal = Calendar.getInstance(); cal.setTime(dateFieldInput); int year = cal.get(Calendar.YEAR); int month = cal.get(Calendar.MONTH) + 1; int day = cal.get(Calendar.DAY_OF_MONTH); int hours = (hoursInput == null ? 0 : hoursInput % 24); int minutes = (minutesInput == null ? 0 : minutesInput);  // Use the input to create a date object with proper timezone MutableDateTime date = new MutableDateTime(year, month, day, hours, minutes, 0, 0, 	DateTimeZone.forTimeZone(getClientTimeZone())); </code> If the server's current date is different from the client's, this produces wrong output. I attached a patch with a test case that simulates this condition.  I don't know why this "casting" of day, month, year is done.
WICKET-4a875f46$$Mapping ResourceReferences to Urls is slow$$PackageResourceReference is often used for stylesheets and JavaScript resources, many of which can appear on a typical page (WicketAjaxReference is one common example). Every time the page is rendered, these resources are mapped to urls in order to build the appropriate <link href="..."> or <script src="..."> tags.  The trouble is that this mapping process is extremely inefficient. To map a ResourceReference to a url, ResourceReference#getLastModified() must be consulted for FilenameWithTimestampResourceCachingStrategy, and ResourceReference#getUrlAttributes() is called to append appropriate query parameters.  In PackageResourceReference, both of these methods delegate to the very expensive PackageResourceReference#lookupStream(), which makes several attempts to locate the underlying file or classpath item using various permutations of locale, style, and variation. Each of these attempts involves I/O. The default ResourceStreamLocator, which does the actual file and classpath queries, does no caching whatsoever.  On a trivial Wicket page containing 7 total PackageResourceReferences for images, stylesheets and JavaScript files, the average response time in my tests was 211 ms. The vast majority of that time was spent in ResourceStreamLocator, due to the expensive steps described above.  It seems that putting caching at the ResourceStreamLocator would be extremely beneficial. I am attaching a simple implementation. With caching enabled in ResourceStreamLocator, the response time of my test page dropped from 211 ms to 49 ms.
WICKET-2b6da516$$SimpleTree example not working with CryptoMapper$$Adding the following lines to WicketExampleApplication.java causes the SimpleTree example to break. There are no expand icons anymore and there is no way to expand the tree. Even the expand link will not work.  Add to WicketExampleApplication.java   IRequestMapper cryptoMapper = new CryptoMapper(getRootRequestMapper(), this); setRootRequestMapper(cryptoMapper);   Comment out in WicketExampleApplication.java   //getSecuritySettings().setCryptFactory(new ClassCryptFactory(NoCrypt.class, ISecuritySettings.DEFAULT_ENCRYPTION_KEY));  Without the CryptoMapper everythings works fine.
WICKET-d1b62639$$SHOW_NO_EXCEPTION_PAGE responding with HTTP status 500 is overwritten by redirect$$If the application is configured with SHOW_NO_EXCEPTION_PAGE as unexpectedExceptionDisplay, an exception thrown while submitting a form should result in an HTTP 500 status. Since the request is already marked as a redirect in AbstractListenerInterfaceRequestTarget#onProcessEvents(), the 500 status is overwritten with status 200 when the redirect is handled afterwards.
WICKET-a4459ef4$$Event broadcast type "Depth" does not work when the sink is a Component but not a MarkupContainer$$Event broadcast type "Depth" does not work when the sink is a Component but not a MarkupContainer. In this case, no sinks receive the event.
WICKET-c62b66c1$$Interaction betwen IAjaxRegionMarkupIdProvider, renderPlaceholderTag and visibility$$I've just discovered what I think is a bug with IAjaxRegionMarkupIdProvider. We are using it on a Behavior that provides a border to form components (label, mandatory marker, etc), which for the most part works great.  We have encountered a problem when toggling the visibility of a form component with this behavior via ajax.   The component is first sent out visible and the markup is all correct.  A change elsewhere on the page causes the component to be set to not visible and redrawn via ajax. The ajax response contains a tag with a markupid generated via renderPlaceholderTag. This does not take into account the  IAjaxRegionMarkupIdProvider behaviour.  Another change happens on the page causing the component to become visible, and the ajax replace can't happen because the component with the correct markupId is not present.
WICKET-5e2c6702$$Wicket 1.5 RC-3 Bug with conditional comments$$IE Conditional Comments with script block causes malformed HTML on Chrome and Firefox.
WICKET-7c364566$$DatePicker issues with locale medium date format$$DateTextField as follows: DateTextField d = new DateTextField(id, model, new StyleDateConverter("M-",false));  Case 1: - en-US locale - for example DatePicker insert 04 rather than Apr in DateTextField, even though pattern clearly says MMM  Case 2: - pl-PL locale - 2010-10-25 is in the DateTextField - DatePicker opens on April 2031 rather than October 2010  I believe the problem lies in wicket-date.js, in functions substituteDate and parseDate.  I know this might be duplicate of WICKET-2427 and WICKET-2375, but apparently this hasn't been properly fixed yet.
WICKET-aa1d177a$$DataTable row groups are present in markup even when they contain no rows.$$As per the HTML spec : "When present, each THEAD, TFOOT, and TBODY contains a row group. Each row group must contain at least one row, defined by the TR element."  There is no check in place to remove the row group tags from the output if they don't contain any row.
WICKET-7ae109a6$$Using render strategy ONE_PASS_RENDER fails for Ajax requests$$I have an application which has two pages. Page A has an AjaxLink which makes some checks and either sets some error feedback and stays on the same page (e.g. login page with "Invalid user" error) or if everything is OK then redirects to page B (via setResponsePage(B.class)). The problem comes when the current render strategy is ONE_PASS_RENDER. In this case no matter that fromUrl and toUrl are different and the request is Ajax the current code directly writes the page markup to the response. I think it should trigger a redirect instead. I am not sure whether it should be redirect to render or to buffer ...
WICKET-fbfd17e6$$IllegalStateException: Header was already written to response!$$Getting this error for no apparent reason, the code works fine on wicket 1.4.17. Code example with error is attached.  Click on the 'click here' link to see the error occur in the console, below is part of the stack trace.   ERROR - DefaultExceptionMapper     - Unexpected error occurred java.lang.IllegalStateException: Header was already written to response!         at org.apache.wicket.protocol.http.HeaderBufferingWebResponse.checkHeader(HeaderBufferingWebResponse.java:62)         at org.apache.wicket.protocol.http.HeaderBufferingWebResponse.setDateHeader(HeaderBufferingWebResponse.java:131)          at org.apache.wicket.protocol.http.BufferedWebResponse SetDateHeaderAction.invoke(BufferedWebResponse.java:241)         at org.apache.wicket.protocol.http.BufferedWebResponse.writeTo(BufferedWebResponse.java:487)         at org.apache.wicket.request.handler.render.WebPageRenderer.respond(WebPageRenderer.java:225)         at org.apache.wicket.request.handler.RenderPageRequestHandler.respond(RenderPageRequestHandler.java:139)         at org.apache.wicket.request.cycle.RequestCycle HandlerExecutor.respond(RequestCycle.java:715)         at org.apache.wicket.request.RequestHandlerStack.execute(RequestHandlerStack.java:63)         at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:274)         at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:283)         at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:283)         at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:283)         at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:283)         at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:283)         at org.apache.wicket.request.cycle.RequestCycle.processRequest(RequestCycle.java:227)         at org.apache.wicket.request.cycle.RequestCycle.processRequestAndDetach(RequestCycle.java:253)         at org.apache.wicket.protocol.http.WicketFilter.processRequest(WicketFilter.java:138)         at org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:194)         at org.eclipse.jetty.servlet.ServletHandler CachedChain.doFilter(ServletHandler.java:1323)         at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:474)         at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:119)         at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:517)         at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)         at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:935)         at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:404)         at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:184)         at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:870)         at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)         at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:247)         at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:151)         at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)         at org.eclipse.jetty.server.Server.handle(Server.java:346)         at org.eclipse.jetty.server.HttpConnection.handleRequest(HttpConnection.java:596)         at org.eclipse.jetty.server.HttpConnection RequestHandler.headerComplete(HttpConnection.java:1051)         at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:592)         at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:214)         at org.eclipse.jetty.server.HttpConnection.handle(HttpConnection.java:426)         at org.eclipse.jetty.server.bio.SocketConnector ConnectorEndPoint.run(SocketConnector.java:241)         at org.eclipse.jetty.server.ssl.SslSocketConnector SslConnectorEndPoint.run(SslSocketConnector.java:646)         at org.eclipse.jetty.util.thread.QueuedThreadPool 3.run(QueuedThreadPool.java:528)         at java.lang.Thread.run(Thread.java:619)
WICKET-1a2bc1bc$$IResponseFilter cannot change buffer contents$$Changes to the responseBuffer, passed to an IResponseFilter, are not picked up, nor are newly created AppendingStringBuffer (return value of the method). Both callers of the method invoke it with a copy of the buffer and ignore return values (BufferedWebResponse line 145 and AjaxRequestTarget line 687).
WICKET-ab1856db$$RequestCycleListenerCollection.onException should not throw an exception when multiple listeners handle the exception$$When multiple listeners handle the exception, RequestCycleListenerCollection should simple take the first handler. The current approach makes it impossible to add a listener that handles all exceptions and later add listeners for specific exceptions. Simply removing the 'if (handlers.size() > 1)' should suffice.
WICKET-12124902$$RequestHandler listeners are called with null handler$$When running RequestCycleListeners with WicketTester, they get called with null for the requesthandler in onScheduleHandler(). This is an artifact of the wicket tester requestcycle, which I think should normally not occur in Wicket processing. The parameters are not marked as optional, so it is IMO safe to assume that handler should never be null.  I propose to modify scheduleRequestHandlerAfterCurrent(IRequestHandler handler) to not invoke the listeners when the handler is null, but only to clear the current scheduled handler.
WICKET-1b57b51c$$Component#getMarkupId() throws exceptions when not added to page yet$$When retrieving the markup ID for a component that has not yet been added to a page, Wicket currently throws an exception telling that the markup could not be found, or that the markup type (in case the component was added to a Panel) could not be determined. In 1.4, Wicket would generate a markup ID in these cases.  Proposed solution: to first see if a markup ID has been either generated or set (using setOutputMarkupId), and then returning that, or if no ID was yet available *and* the component has been added to a Page: use the ID from the markup, or if the component has not been added to a Page nor a markup ID: generate the ID.
WICKET-a08562a7$$wicket:border: inconsistency between add() and remove()$$Assuming c1 is a Border and c2 is some component, the following sequence crashes with duplicate addition:  c1.add(c2); c1.remove(c2); c1.add(c2);  The reason for this is that remove() doesn't remove the object from the bodycontainer. The sequence can be made to work by changing the middle line to:  c1.getBodyContainer().remove(c2);  That remove() doesn't look the component from the same container as add() adds it to, seems to violate the principle of least astonishment. Unfortunately the Component structure manipulation API has more methods, such as swap(), size(), get(), etc. which are final, and can't be overridden by Border as they are. It could be best to force all users to use c1.getBodyContainer().add() instead of c1.add(), because consistent operation is probably easier to deal with in the long run than behavior that conforms to initial assumptions but has flaws elsewhere.  This ticket suggests removing the overload of add() and documenting the difference in migration guide.
WICKET-e1168a57$$g/apache/wicket/protocol/http/request/UserAgent matches method is not correct$$In the UserAgent Enum matches method, the loop over detectionStrings is at most executed once:      for (List<String> detectionGroup : detectionStrings)     {       for (String detectionString : detectionGroup)       {         if (!userAgent.contains(detectionString))         {           return false;         }       }        return true;     }  It returns true after only processing the first element in the detectionStrings list. It never looks at any of the other elements of the list.
WICKET-557de7bc$$FileUpload writeToTempFile() method throws NPE for sessionless requests$$I have created stateless page with stateless form containing FileUploadField, however when I tried to post file to it, NPE was thrown.  The issue is caused by method FileUpload#writeToTempFile() method trying to use session id as temp file prefix.   Workaround: create temp file manually and use method FileUpload#writeToFile( myTempFile)
WICKET-5ad32df9$$Component's markup cannot be found in Ajax requests if the parent is transparent$$When TransparentWebMarkupContainer is used an inner markup container cannot find its markup on Ajax updates. The problem seems to be caused by the fact that ComponentResolvers#resolve() is not executed and since there is transparent container involved Markup.find(String) cannot find the markup for non-transparent markup containers. I'll commit a disabled test case that shows the problem.
WICKET-1858bc18$$The Url's query parameters are not properly URL encoded$$If page parameter has a value with special characters like ' then it is rendered as it is in the produced markup and is only XML encoded but never URL encoded. This causes broken html for example in the case when a Link is attached to a non- a|area|link tag:   <html><body><span wicket:id="link" onclick="var win = this.ownerDocument.defaultView || this.ownerDocument.parentWindow; if (win == window) { window.location.href=&#039;bookmarkable/org.apache.wicket.MockPageWithLink?urlEscapeNeeded=someone&#039;s+ba+parameter&#039;; } ;return false"></span></body></html>  Notice that &#039; after 'someone' closes the location.href string too early and breaks the app.
WICKET-48454f4d$$Ajax behaviors are failing in stateless pages$$Stateless ajax behaviors are not working in stateless pages in 1.5-RC4.2. I verified it with the stateless demo project of Martin Grigorov (https://github.com/martin-g/wicket-stateless), when changing the dropdown on the start page an exception is thrown (clicking the increment link causes a similar exception):   org.apache.wicket.behavior.InvalidBehaviorIdException: Cannot find behavior with id: 0 on component: [DropDownChoice [Component id = c]]  At first glance the reason may be located in org.apache.wicket.Behaviors.getBehaviorById() which does not create the ID list if missing (getBehaviorsIdList(false) in line 286 instead of getBehaviorsIdList(true)), because this error does not occur when getBehaviorId() was manually called in the page constructor to force creation of the list.
WICKET-84c3baac$$INullAcceptingValidator behavior seems broken in 1.5-RC4.2$$As discussed in this forum thread: http://apache-wicket.1842946.n4.nabble.com/INullAcceptingValidator-behavior-tp3570352p3570352.html  It appears that Wicket no longer calls INullAcceptingValidator intances when the validatable value is null.  Wicket wraps validators  as behaviors, using the adapter pattern. The adapter class (org.apache.wicket.validation.ValidatorAdapter) implements  the interface IValidator<T>. This "hides" the case where the actual validator is an INullAcceptingValidator. Therefore, when going through a component's attached validators, the code of org.apache.wicket.markup.html.form.FormComponent will never call INullAcceptingValidators when the value is null.
WICKET-b4e9d426$$WicketSessionFilter and HttpSessionStore use different attribute name for Wicket Session$$from this topic  http://apache-wicket.1842946.n4.nabble.com/WicketSessionFilter-and-ignorePaths-in-WicketFilter-td3570291.html Please, look at the second post (the ignorePaths param is not linked with this issue as the title suggests).  How to reproduce with the quickstart: 1. open localhost:8080 - a wicket test page is displayed. 2. open localhost:8080/external - this is the external servlet that tries to access the wicket session. An exception is thrown.
WICKET-30255f11$$WicketTester does not follow absolute redirects$$Wicket tester does not follow absolute redirects:  This is a problem when using HttpsMapper. For example when requesting a page over http:// with an forced redirect to https:// for secure access will make wicket tester return 'null' for the last renderer page instead of the rendered page instance. In general all kinds of absolute redirects to another page will not be tracked by wicket tester. So this potentially a problem for all kinds of tests that rely on absolute redirects.
WICKET-747bccb5$$WicketTester does not follow absolute redirects$$Wicket tester does not follow absolute redirects:  This is a problem when using HttpsMapper. For example when requesting a page over http:// with an forced redirect to https:// for secure access will make wicket tester return 'null' for the last renderer page instead of the rendered page instance. In general all kinds of absolute redirects to another page will not be tracked by wicket tester. So this potentially a problem for all kinds of tests that rely on absolute redirects.
WICKET-843b76b1$$regression on strategy to integrate cas authentication$$yes, It happens in org.apache.wicket.request.handler.PageProvider.getPageInstance() , but not for the WelcomePage, but the redirection page (RedirectPage). the CASPageAuthorizationStrategy as we are not authentified a org.apache.wicket.RestartResponseAtInterceptPageException with in parameter an instance of RedirectPage. On the second call of PageProvider.getPageInstance, the pageId is of 0, an all other parameters are nulls.  The run seems quite different on 1.5-RC4.2 version - There is only one call of the method PageProvider.getPageInstance() and it come after the CASPageAuthorizationStrategy.isPageAuthorized  Le 27/06/2011 11:24, Martin Grigorov a écrit : > Put a breakpoint in > org.apache.wicket.request.handler.PageProvider.getPageInstance() and > see what happens. > It seems the test tries to retrieve a page from the page store by id > but there is no such. > > On Mon, Jun 27, 2011 at 12:20 PM, Thomas Franconville > <tfranconville@tetraedge.com>  wrote: >> Hi, >> >> Upgrading wicket from 1.5-RC4.2 to 1.5-RC5.1  make my Junit Test down with >> the error 'Page expired' >> >> /** >>   * Simple test using the WicketTester >>   */ >> public class TestHomePage >> { >>     private WicketTester tester; >> >>     @Before >>     public void setUp() >>     { >>         tester = new WicketTester(new MyApplication()); >>     } >> >>     @Test >>     public void homepageRendersSuccessfully() >>     { >>         //start and render the test page >>         tester.startPage(WelcomePage.class); >> >>         //assert rendered page class >>         tester.assertRenderedPage(RedirectPage.class); >>     } >> } >> >> My application use a CASPageAuthorizationStrategy inspired of >> http://www.lunikon.net/2009/11/24/integrating-cas-and-wicket/ >> >> >> Kind Regards >> >> Thomas
WICKET-97514205$$In wicket 1.5 urlFor returns incorrect string for package mounted pages$$Attached two quickstart projects for 1.4 and 1.5.  Then access http://localhost:8080/app/Page1 and see 1.5 returns wrong address.
WICKET-afc7034d$$support custom response headers in AbstractResource.ResourceResponse$$I'm converting an application to Wicket 1.5 and I see some problems with resources.  There is a case I need to add headers (not present in ResourceResponse properties) and it looks ugly.  This is what I need to do:      @Override     protected void configureCache(ResourceResponse data, Attributes attributes)     {         super.configureCache(data, attributes);         ((WebResponse) attributes.getResponse()).setHeader("Accept-Ranges", "bytes");     }  It's a hack to use configureCache here, but this can't be added to setResponseHeaders, which seams a better apparent method name for it.
WICKET-d1e0e411$$AbstractTransformerBehavior sets wrong namespace$$AbstractTransformerBehaviour adds a wicket namespace (http://wicket.apache.org) to its tag which is different from that of the whole page (http://wicket.apache.org/dtds.data/wicket-xhtml1.4-strict.dtd).  This causes (at least) XPath queries for Wicket nodes to fail when matching the contents of components with an AbstractTransformerBehavior.
WICKET-3feb0e3a$$MarkupContainer.removeAll() does not detach models recursively$$ML thread at: http://markmail.org/message/ybdfd2ts4i3j2b72
WICKET-b772ff87$$Null model for AttributeAppender should not render empty attribute$$I can't think of a reason this would be valid, but passing in null model renders <span class="">Test</span>.  If previous and new attribute are both null, the component should render cleanly like <span>Test</span>.
WICKET-beb9086d$$setResponsePage in AjaxLink goes always to localhost:8080 instead to the right host and port$$setResponsePage in an AjaxLink in Wicket 1.4 redirects with a relative path to the response page. Wicket 1.5 takes the absolute path "localhost:8080/path to the response page" even when the host and port are different. (e.g. with Apache2 a virtual host is created with server name www.mycompany.com, setResponce wil go to "localhost:8080/path to page" instead of  "www.mycompany.com/path to page")
WICKET-aadaa4e9$$PageParameters#set not follow INamedParameters#set behavior$$Couple of problems to work with page parameters: Major - The PageParameters#set(final String name, final Object value, final int index) used remove/add pattern instead of set parameter value by specified index. Minor - Inposible to get the index of key in elegant way to use obtained index in #set operation
WICKET-8fbdc68f$$Component markup caching inconsistencies$$In WICKET-3891 we found that Component#markup field is not being reset between requests. The problem is that this field is transient and it is null-ified only when the page is read from the second level page cache (see https://cwiki.apache.org/confluence/x/qIaoAQ). If the page instance is read from first level cache (http session) then its non-serialized version is used and the markup field value is still non-null.  In WICKET-3891 this looked like a minor issue with the markup caching in development mode but actually this problem is valid even in production mode. See the attached application. When the panel's variation is changed every MarkupContainer inside still uses its old markup.
WICKET-6051019b$$A (stateless) page immediately disappears after the first render$$Using setResponsePage(new SomeStatelessNonBookmarkablePage(aParameter)) renders the page but trying to reload the page in the browser fails with PageExpiredException.  The reason is that the page is stateless and thus it is not saved in the page stores. Since it was scheduled for render with setResponsePage(Page) method its Url is created by PageInstanceMapper (i.e. something like: wicket/page?1). An attempt to refresh such page fails with "Page with id '1' is not found => PageExpiredException".  Igor suggested to call 'page.setStatelessHint(false)' for all pages passed to PageProvider(IRequestablePage) constructor, i.e. such pages must be stored. This solved the problem but exposed few more problems: - MockPageManager (used in WicketTester) until now always touched/stored pages, no matter their statelessness - org.apache.wicket.markup.html.internal.EnclosureTest.testRender10() was wrong for some unknown reason. All expectations against EnclosurePageExpectedResult_10-2.html should not have the enclosure rendered because "input" component is invisible
WICKET-6a8fc1cc$$MarkupNotFoundException when refreshing a component with AJAX inside a TransparentWebMarkupContainer$$A component placed inside a TransparentWebMarkupContainer, added to its parent cannot be refreshed with AJAX. See quickstart.
WICKET-b76f9c44$$CreditCardValidator returns incorrect cardId for VISA$$When the validation for a VISA is correct, it returns a SWITCH cardId instead of a VISA. This error occurs in both 1.4.x and 1.5.X
WICKET-38e928c1$$Header contributions order is not stable$$In the last RCs, I started to experience problems with the contributions order. For example, I add jQuery, and until 1.5RC5, it worked well, but now the call to the jQuery script has been moved to the bottom of the page head, and this disables all my other scripts that are expecting jQuery's   to be defined.  I attach a quickstart to demonstrate the problem. Maybe the order in the quickstart is not the expected one, but what it shows is that the order does not make real sense (at least to me) : In the quickstart, the wicket:head tag contributions are in the order 3 - 8 - 9 - 5, and the renderHead methods contributions are in the order 4 - 1 - 2 - 6 - 7.
WICKET-d35d2d85$$Component's onAfterRender() is called so many times as it is depth in the component tree + 1$$org.apache.wicket.Component.afterRender() calls org.apache.wicket.Component.onAfterRenderChildren() which for MarkupContainers calls afterRender() for its children.  So for code like:   WebMarkupContainer comp1 = new WebMarkupContainer("c1");         add(comp1);                  WebMarkupContainer comp2 = new WebMarkupContainer("c2");         comp1.add(comp2);                  WebMarkupContainer comp3 = new WebMarkupContainer("c3") {              @Override             protected void onAfterRender() {                 super.onAfterRender();                 System.err.println("called");             }                      };         comp2.add(comp3);  you'll see "called" printed 4 times in a single request.  Additionally I think onAfterRenderChildren() should be called before onAfterRender() in Component.afterRender(). The flow should be first-in last-out: onBeforeRender > onBeforeRenderChildren > onAfterRenderChildren > onAfterRender,
WICKET-e60bac5f$$Wicket 1.5 Form Post Action and Link Get discard Page Class Information$$Page expiry is a very annoying and perplexing event especially if users stay logged in via remember-me cookie.  It is therefore not a fancy enhancement but an essential business requirement to not drop the user out of context after session expiry. Only stateless pages can fully achieve this, but it is not always desirable to go fully stateless, especially while a recovery solution already exists. In 1.4, this appears to be automatic with BookmarkablePageRequestTargetUrlCodingStrategy - without any additional coding.  The solution is well known - keep as much state in the client as required to recover the page class, and possibly even its page parameters, and to not destroy this information.  The two attached testcases show two possible methods of page fallback recovery (one with AJAX, one without) that already work behind the scenes. Of course it is easy with AJAX, to just force a page reload, but this is not discussed here. AJAX just serves to demonstrate how easy the principle actually is. In most cases the user could successfully reload the page but Wicket 1.5 can't create a response because it has forgotten the class of the expired page.  In 1.4, it is possible to recover the class of an expired page via its mount path. This feature is lost in 1.5.  To get this functionality back in a more streamlined fashion, I am additionaly proposing in a separate jira issue 4013 to store page class and page parameters in PageExpiredException.  Meanwhile, the focus of this issue is to request whatever means to not overwrite the path of a page in a form post action or get request, and to get the page class back as in 1.4 by whatever means.  The two attached testcases may be helpful for expermintation. The 1.4 tescase demonstrates how the scheme works, unfortunately I could not fill the blanks in the 1.5 testcase.  In 1.4,  a form tag is rendered as: <form wicket:id="form" action="?wicket:interface=:0:form::IFormSubmitListener::" This is requested as: /testForm.0?wicket:interface=:0:form::IFormSubmitListener:: and the page class can be recovered from the mount path "testForm" as in     mount(new HybridUrlCodingStrategy("testForm", TestPageForm.class));  an anchor tag is rendered as: <a href="?wicket:interface=:0:linkSwitch::ILinkListener::" This is requested as: /testLink.0?wicket:interface=:0:linkSwitch::ILinkListener:: and the page class can be recovered from the mount path "test" as in     mount(new HybridUrlCodingStrategy("testLink", TestPageLink.class));    In 1.5,  a form tag is rendered as: <form wicket:id="form" action="wicket/page?0-2.IFormSubmitListener-form" This is requested requested as: /wicket/page?0-1.IFormSubmitListener-form  This overwrites the mount path "testForm" as in     mountPage("testForm", TestPageForm.class); Consequently the server cannot discover the page class   an anchor tag is rendered as: <a href="wicket/page?0-1.ILinkListener-linkSwitch" This is requested requested as: /wicket/page?0-1.ILinkListener-linkSwitch  This overwrites the mount path "testLink" as in     mountPage("testLink", TestPageLink.class); Consequently the server cannot discover the page class
WICKET-f1c9cef2$$MarkupContainer.toString(true) fails with MarkupNotFoundException if the call is made in the component constructor$$org.apache.wicket.MarkupContainer.toString(boolean) uses "if (getMarkup() != null)" to decide whether to write something for the markup but since recently Component#getMarkup() throws MarkupNotFoundException when there is no markup and doesn't return null.
WICKET-081cdeb2$$ResourceMapper throws IllegalStateException when attempting to map a request to a URL ending in a empty segment (directory)$$ResourceMapper.mapRequest() calls ResourceMapper.removeCachingDecoration() which, throws IllegalStateException if the URL's last segment is an empty string.  URLs like: path/to/my/non/wicket/directory/ end in a empty segment.   We must change the behaviour to not attempt to undecorate a URL ending in an empty segment.
WICKET-5f69685d$$HeaderResponse.renderCSSReference does not render context path relative url, but wicket filter url-pattern relative url$$In an application with a wicket filter url-pattern different than /*, if you use HeaderResponse.renderCSSReference(String url), where url is a context-path-relative url (css/main.css, for example), the generated css link is not context relative, but wicket url-pattern relative.
WICKET-f3d7565c$$MountedMapper.mapHandler ruins Links inside mounted pages appending parameters wicket-ajax and wicket-ajax-baseurl$$With the last commit n° 1166194 method mapHandler has been added to MountedMapper class in order to solve WICKET-4014. Unfortunately this method seems to ruin Link url inside mounted page (for example home page) if this page uses AJAX. mapHandler modifies Link url appending parameters 'wicket-ajax' and 'wicket-ajax-baseur'l to it. In this way when we click Link we get an error from browser like this:         " This XML file does not appear to have any style information associated with it. The document tree is shown below.       <ajax-response><redirect>wicket/page?41</redirect></ajax-response> "  The error message is the same for Firefox and Chromium. See attached quickstart.  Warning: as I'm writing this issue, Wicket snapshot is not affected yet by this bug, so you have to run quickstart with the last source from repository.
WICKET-4d3d1f85$$RestartResponseAtInterceptPageException.InterceptData is never cleared$$RestartResponseAtInterceptPageException.InterceptData is supposed to be cleared after continueToOriginalDestination() is called. This is accomplished via RestartResponseAtInterceptPageException.MAPPER, which is registered in the SystemMapper.  However there seems to be a serious bug here. The MAPPER always returns a compatibilityScore of 0, and thus is never actually invoked. The InterceptData is thus never cleared. Furthermore, even if the MAPPER did return a Integer.MAX_VALUE score, it would still not be invoked in many scenarios, since other mappers in the SystemMapper are registered later and therefore have higher priority.  In practice, this can lead to very odd login behavior in Wicket applications (which is where RestartResponseAtInterceptPageException is typically used). For example, if the user clicks a "login" link they may end up on a totally unexpected page, due to stale InterceptData that is hanging around in the session.  I am attaching a quick start that demonstrates the problem, as well as a patch the fixes the compatibilityScore and moves the MAPPER to a higher priority in the SystemMapper.
WICKET-d450acb0$$Errors reported from Form#onValidateModelObjects() are ignored$$None
WICKET-7d5b8645$$Form Input example fails when changing the language$$Trying to change the language of http://localhost:8080/forminput example fails with:   Caused by: java.lang.ArrayIndexOutOfBoundsException: -1 	at java.util.ArrayList.remove(ArrayList.java:390) 	at org.apache.wicket.request.Url.resolveRelative(Url.java:884) 	at org.apache.wicket.markup.html.form.Form.dispatchEvent(Form.java:1028) 	at org.apache.wicket.markup.html.form.Form.onFormSubmitted(Form.java:699) 	at org.apache.wicket.markup.html.form.Form.onFormSubmitted(Form.java:670) 	... 37 more
WICKET-1dcaec98$$SmartLinkLabel doesn't recognize already tagged links$$The SmartLinkLabel works as expected for the texts without <a>..</a> tag.  for text like extensions @ http://www.wicketframework.org/wicket-extensions/index.html are cool!! SmartLinkLabel generates the html -  extensions @ <a href="http://www.wicketframework.org/wicket-extensions/index.html">http://www.wicketframework.org/wicket-extensions/index.html</a> are cool!!  but for the text like extensions @ <a href='http://www.wicketframework.org/wicket-extensions/index.html'>http://www.wicketframework.org/wicket-extensions/index.html</a> are cool!! SmartLinkLabel generates the html -  extensions @ <a href='<a href="http://www.wicketframework.org/wicket-extensions/index.html">http://www.wicketframework.org/wicket-extensions/index.html</a>'><a href="http://www.wicketframework.org/wicket-extensions/index.html">http://www.wicketframework.org/wicket-extensions/index.html</a></a> are cool!!  I think this is a bug & needs a fix.
WICKET-e743fd7e$$AutoLabelTextResolver fails to pick up locale changes in the session$$When using <wicket:label key="..."> AutoLabelTextResolver correctly picks up the localized message identified by the key. However, if the Session locale is changed, neither the printed label nor the FormComponent's label model get updated - both will still contain the initial message. This is inconsistent with the behavior of <wicket:message> and StringResourceModel. The principle of least surprise (and in my opinion, also that of highest usefulness ;-) ) suggests that AutoLabelTextResolver should dynamically get the localized string whenever it deals with something that can be localized. That includes the <wicket:label key="..."> case mentioned above, as well as when using FormComponent#getDefaultLabel.  I have only tested this in trunk 1.5 (since it recently came up during a training I gave on Wicket 1.5). I suspect it also affects 1.4.x.
WICKET-64656c98$$Using ajax to update a component that has an AbstractTransformerBehavior attached throws a ClassCastException$$Using ajax to update a component that has an AbstractTransformerBehavior attached throws a ClassCastException:  java.lang.ClassCastException: org.apache.wicket.ajax.AjaxRequestTarget AjaxResponse cannot be cast to org.apache.wicket.request.http.WebResponse       at org.apache.wicket.markup.transformer.AbstractTransformerBehavior.beforeRender(AbstractTransformerBehavior.java:68)        at org.apache.wicket.Component.notifyBehaviorsComponentBeforeRender(Component.java:3421)       at org.apache.wicket.Component.internalRender(Component.java:2344)       at org.apache.wicket.Component.render(Component.java:2273)        at org.apache.wicket.MarkupContainer.renderNext(MarkupContainer.java:1474)       at org.apache.wicket.MarkupContainer.renderAll(MarkupContainer.java:1638)       at org.apache.wicket.MarkupContainer.renderComponentTagBody(MarkupContainer.java:1613)        at org.apache.wicket.MarkupContainer.onComponentTagBody(MarkupContainer.java:1567)       at org.apache.wicket.markup.html.form.Form.onComponentTagBody(Form.java:1570)       at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.onComponentTagBody(DefaultMarkupSourcingStrategy.java:72)        at org.apache.wicket.Component.internalRenderComponent(Component.java:2515)       at org.apache.wicket.MarkupContainer.onRender(MarkupContainer.java:1576)       at org.apache.wicket.Component.internalRender(Component.java:2345)        at org.apache.wicket.Component.render(Component.java:2273)       at org.apache.wicket.MarkupContainer.renderNext(MarkupContainer.java:1474)       at org.apache.wicket.MarkupContainer.renderAll(MarkupContainer.java:1638)        at org.apache.wicket.MarkupContainer.renderComponentTagBody(MarkupContainer.java:1613)       at org.apache.wicket.MarkupContainer.renderAssociatedMarkup(MarkupContainer.java:735)       at org.apache.wicket.markup.html.panel.AssociatedMarkupSourcingStrategy.renderAssociatedMarkup(AssociatedMarkupSourcingStrategy.java:76)        at org.apache.wicket.markup.html.panel.PanelMarkupSourcingStrategy.onComponentTagBody(PanelMarkupSourcingStrategy.java:112)       at org.apache.wicket.Component.internalRenderComponent(Component.java:2515)       at org.apache.wicket.MarkupContainer.onRender(MarkupContainer.java:1576)        at org.apache.wicket.Component.internalRender(Component.java:2345)       at org.apache.wicket.Component.render(Component.java:2273)       at org.apache.wicket.ajax.AjaxRequestTarget.respondComponent(AjaxRequestTarget.java:982)        at org.apache.wicket.ajax.AjaxRequestTarget.respondComponents(AjaxRequestTarget.java:796)       at org.apache.wicket.ajax.AjaxRequestTarget.constructResponseBody(AjaxRequestTarget.java:676)       at org.apache.wicket.ajax.AjaxRequestTarget.respond(AjaxRequestTarget.java:637)        at org.apache.wicket.request.cycle.RequestCycle HandlerExecutor.respond(RequestCycle.java:712)       at org.apache.wicket.request.RequestHandlerStack.execute(RequestHandlerStack.java:63)       at org.apache.wicket.request.RequestHandlerStack.execute(RequestHandlerStack.java:96)        at org.apache.wicket.request.cycle.RequestCycle.processRequest(RequestCycle.java:208)       at org.apache.wicket.request.cycle.RequestCycle.processRequestAndDetach(RequestCycle.java:251)       at org.apache.wicket.protocol.http.WicketFilter.processRequest(WicketFilter.java:162)        at org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:218)       at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)       at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)       at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:175)       at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:128)        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)       at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)       at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:286)        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:844)       at org.apache.coyote.http11.Http11Protocol Http11ConnectionHandler.process(Http11Protocol.java:583)       at org.apache.tomcat.util.net.JIoEndpoint Worker.run(JIoEndpoint.java:447)        at java.lang.Thread.run(Thread.java:619)
WICKET-8f7805f8$$AutocompleteTextField after Submit does not work$$I use an AutocompleteTextfield together with a submit-Button. After once submitting the content oft the AutocompleteTextField the parameter q is added to the URL. After that the autocompletion will only complete the parameter q in the url and not the parameter given by ajax.  I tracked the problem down to the callbackURL.  It contains a pattern looking as follows: ....&q=<paramproducedbysubmit>&q=<paramproducedbyajaxautocomplete>  The callbackurl is build of the parameter q and the extraction of parameters only accepts the first parameter
WICKET-4624ab3d$$Ajax link reports weird error when session is expired$$Reproducing steps:  1. Put below simple page into a Wicket application and get it mounted:  TestPage.java:  import org.apache.wicket.ajax.AjaxRequestTarget; import org.apache.wicket.ajax.markup.html.AjaxLink; import org.apache.wicket.markup.html.WebPage;  @SuppressWarnings("serial") public class TestPage extends WebPage { 	 	public TestPage() { 		 		add(new AjaxLink<Void>("test") {  			@Override 			public void onClick(AjaxRequestTarget target) { 			} 			 		}); 		 	} 	 }  TestPage.html:  <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"> <?xml version="1.0" encoding="UTF-8"?> <html xmlns="http://www.w3.org/1999/xhtml"> 	<head> 		<title>Test Page</title> 	</head> 	<body> 		<a wicket:id="test">test</a> 	</body> </html>  2. Access the page in browser via mounted url, the page will display a link.   3. Wait until current session is expired (do not refresh the page or click the link while waiting).   4. Hit the link and below exception will be thrown: Message: Cannot find behavior with id: 0 on component: [ [Component id = test]]. Perhaps the behavior did not properly implement getStatelessHint() and returned 'true' to indicate that it is stateless instead of returning 'false' to indicate that it is stateful.  5. In wicket 1.5.0, this results in a PageExpiredException which is more comprehensive.
WICKET-bb7a6995$$FileResourceStream returns unknown content type$$See http://apache-wicket.1842946.n4.nabble.com/PackageResourceReference-and-Doctype-in-Markup-file-tp3889467p3889587.html  The response for FileResourceStreams returns an unknown content type for css- and image-files. Correct content types should be "text/css" and "image/png" (see also attached quickstart).
WICKET-8967eb2b$$WizardStep FormValidatorWrapper.isActiveStep(WizardStep.java) causes NullPointerException$$Using the Wizard with a nested WizardModel (see RecursiveWizardModel implementation at the attached quickstart application) causes a NullPointerException. I was able to run the same test application with wicket 1.4.18 without any problems.  Caused by: java.lang.NullPointerException         at org.apache.wicket.extensions.wizard.WizardStep FormValidatorWrapper.isActiveStep(WizardStep.java:145)         at org.apache.wicket.extensions.wizard.WizardStep FormValidatorWrapper.getDependentFormComponents(WizardStep.java:109)         at org.apache.wicket.markup.html.form.validation.FormValidatorAdapter.getDependentFormComponents(FormValidatorAdapter.java:47)         at org.apache.wicket.markup.html.form.Form.validateFormValidator(Form.java:1782)         at org.apache.wicket.markup.html.form.Form.validateFormValidators(Form.java:1828)         at org.apache.wicket.markup.html.form.Form.validate(Form.java:1706)         at org.apache.wicket.markup.html.form.Form.process(Form.java:773)         at org.apache.wicket.markup.html.form.Form.onFormSubmitted(Form.java:728)         at org.apache.wicket.markup.html.form.Form.onFormSubmitted(Form.java:670)
WICKET-7c89598a$$BookmarkablePageLinks not working on a forwarded page$$While migrating our app from 1.4 to 1.5 we have discovered a problem with how BookmarkablePageLinks are rendered.  The attached quickstart demonstrates the problem:  Two pages: HomePage and OtherPage mounted at: /content/home and /content/other respectively.  These are mounted using the encoder UrlPathPageParametersEncoder for backwards compatibility with existing 1.4 style URLs.  A filter has been established in web.xml to forward requests to root (eg., localhost) to localhost/content/home [Note: I have left out the port :8080 part from all URL references so please insert when testing]  Point browser to http://localhost and the page is forwarded to http://localhost/content/home and displays correctly (browser URL still shows http://localhost as desired) but the links do not work because they remove the 'content' segment of the URL:  eg., Home link -> http://localhost/home - fails - should have been http://localhost/content/home  If you type in the full URL: http://localhost/content/home  then the home page displays and the links work correctly.  A similar page set  up works fine in 1.4.
WICKET-2737d7c7$$The tbody section of a DataTable is empty when no records are returned by the provider.$$When a DataTable is rendered without records, the tbody section is empty. This violates the html spec.  From the spec: "When present, each THEAD, TFOOT, and TBODY contains a row group. Each row group must contain at least one row, defined by the TR element." and "The THEAD, TFOOT, and TBODY sections must contain the same number of columns."
WICKET-84bbbf68$$Incorrect URL for setResponsePage() within a Form#onSubmit( )$$If the WebApplication uses IRequestCycleSettings.RenderStrategy.ONE_PASS_RENDER, the issue described and exemplified in the attached quickstart at  https://issues.apache.org/jira/browse/WICKET-3442  prevails.   Clicking the link on /pageone results in this URL: /pageone?0-1.IFormSubmitListener-form
WICKET-a0150366$$AppendingStringBuffer.insert  infinite loop$$When trying to insert a StringBuffer into an AppendingStringBuffer, the method   public AppendingStringBuffer insert(final int offset, final Object obj)  will call itself repeatedly generating an infinite loop.  The fix would be to call toString() method if the object is a StringBuffer   public AppendingStringBuffer insert(final int offset, final Object obj) 	{ 		if (obj instanceof AppendingStringBuffer) 		{ 			AppendingStringBuffer asb = (AppendingStringBuffer)obj; 			return insert(offset, asb.value, 0, asb.count); 		} 		else if (obj instanceof StringBuffer) 		{ 			//return insert(offset, obj);                        return insert(offset, obj.toString());  		} 		return insert(offset, String.valueOf(obj)); 	}
WICKET-5fd03973$$ListenerInterfaceRequestHandler should not assume existence of a page$$ListenerInterfaceRequestHandler should not assume a page instance is always available in isPageInstanceCreated. This handler can also be used for links on bookmarkable pages. The attached patch fixes this.
WICKET-53bcb78d$$Multipart Form and AjaxSubmitLink will result in invalid redirect after user session expires$$Hi,  I have hit an issue similar to this one:  https://issues.apache.org/jira/browse/WICKET-3141  I do not receive any errors from Wicket itself to help clarify, so I will try to explain using an example.  The example below with which I could recreate the issue uses the default SignInPanel (in my LoginPage.clas) and AuthenticatedWebSession to authenticate the user and store the session:  	protected Class<? extends WebPage> getSignInPageClass() 	{ 		return LoginPage.class; 	}  If the authentiation is succesfull then the user is redirect back to the test page:  			protected void onSignInSucceeded() { 				setResponsePage(Test.class); 			}  So far so good. However if I use a form with setMultiPart(true) in combination with an AjaxSubmitLink as shown in the following piece of code:  import org.apache.wicket.ajax.AjaxRequestTarget; import org.apache.wicket.ajax.markup.html.form.AjaxSubmitLink; import org.apache.wicket.authroles.authorization.strategies.role.annotations.AuthorizeInstantiation; import org.apache.wicket.markup.html.WebPage; import org.apache.wicket.markup.html.form.Form;  @AuthorizeInstantiation("USER") public class Test extends WebPage {  	public Test() 	{ 		super(); 		 		final Form testForm =  				new Form("testForm"); 	 		testForm.setMultiPart(true); 		 		testForm.add(new AjaxSubmitLink("testButton", testForm) { 			 			@Override 			protected void onSubmit(AjaxRequestTarget target, Form form) { 				super.onSubmit(); 			}; 			 			@Override 			protected void onError(AjaxRequestTarget target, Form form) { 				 			}; 		}); 				 		add(testForm); 	} }  And have selected the option "Remember credentials" in the SignInPanel, clicking on the testButton AFTER the session has expired will result in:  http://localhost:8080/PaladinWicket/?3-1.IBehaviorListener.0-testForm-testButton&wicket-ajax=true&wicket-ajax-baseurl=.  which displays this in the browser:  This XML file does not appear to have any style information associated with it. The document tree is shown below. <ajax-response> <redirect> <![CDATA[ .?1 ]]> </redirect> </ajax-response>
WICKET-c250db9c$$bug in org.apache.wicket.validation.validator.UrlValidator$$Looks like there is a bug in UrlValidator. It validates URLs like "http://testhost.local/pages/index.php" as invalid. But URL is valid! Try to execute "new java.net.URL("http://testhost.local/pages/index.php");" for example. It does not throws "MalformedURLException" because URL is valid.  In method: UrlValidator.isValidAuthority() there is code: "if (topLevel.length() < 2 || topLevel.length() > 4){return false;}" Looks like this "> 4" is a wrong constraint.
WICKET-09166ea8$$onBeforeRender() is called on components that are not allowed to render$$pasted from the list:  Hi,  I ran into an odd problem this week. A model fed to a ListView was calling service methods the current user wasn't allowed to use, and I was wondering how that could happen. A panel far above this ListView in the hierarchy had been secured (using Shiro annotations, but that turns out to not matter at all) and was not supposed to be rendered for this user. From this I had expected the ListView not to be rendered either, but here it was trying to assemble itself in onBeforeRender and thus calling the "forbidden" service methods.  I investigated Component and friends for a bit, and have found a potential problem.  internalBeforeRender() checks determineVisibility() before doing anything. So far so good. determineVisibility() then checks isRenderAllowed() so the application's IAuthorizationStrategy can block certain components. This is where it goes wrong though: isRenderAllowed() only looks at FLAG_IS_RENDER_ALLOWED for performance reasons, and that flag hasn't been set yet! internalPrepareForRender() only calls setRenderAllowed() *after* beforeRender().  Due to this, the supposedly secure panel was going through its own beforeRender and thus calling that ListView's beforeRender.  I think this can be a serious problem, such as in my case described above. I'd expect that if isActionAuthorized(RENDER) is false, the secured component should basically never get to the beforeRender phase. My questions are now:  - Is this intentional? If yes, please explain the reasoning behind it,  because it isn't obvious to me.  - If not, can we fix it? My intuitive suggestion would be to simply  move the call to setRenderAllowed() from the end of  internalBeforeRender() (prepareForRender in 1.4) to the beginning of  that method, so beforeRender() can reliably look at that flag.  - If we can fix it, when and where do we fix it? This hit me in 1.4,  and looking at the code it's still there in 1.5. I'd *really* like it  fixed in the last 1.4 release, and certainly in 1.5, given that this  has the potential to impact security.   It's not an API break, but I'm not sure whether the implications for  application behavior are tolerable for all existing applications. On  the other hand, it seems to be a real security problem, so maybe the  change is justified. I'd like some core dev opinions please :-)  If this is in fact a bug, I'm of course willing to provide a ticket and a patch :-)  Thanks!  Carl-Eric www.wicketbuch.de
WICKET-1f128536$$Using an IValidator on an AjaxEditableLabel causes ClassCastException$$AjaxEditableLabel<Integer> label = new AjaxEditableLabel<Integer>("label", new PropertyModel<Integer>(this, "value")); form.add(label); label.setRequired(true); label.add(new RangeValidator<Integer>(1, 10));  Using a RangeValidator<Integer> on an AjaxEditableLabel<Integer>  causes an ClassCastException after editing the label.   java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.String  This can be avoided by setting the type explicit on the AjaxEditableLabel.  label.setType(Integer.class);  But this wasn't necessary in Wicket 1.4.19. In this version all works fine without setting the type explicit.  I found out, that AbstractTextComponent.resolveType() is not able to get the type of the DefaultModel of the AjaxEditableLabel in Wicket 1.5.3.  I will attach two QuickStarts to demonstrate the bug. One with wicket 1.4.19 and the other with Wicket 1.5.3
WICKET-925cae5c$$UrlRenderer renders invalid relative URLs if first segment contains colon$$Seen on Wicket 1.5.3.  If a relative url of a link starts with a path segment containing a colon then the whole uri will be regarded as absolute uri, so typically browsers will complain that there is no handle for the protocol foo in foo:bar/dee/per.  See also the attached quickstart. The start page contains three links, one relative with colon, one absolute and one to a mounted page without colon for comparison. The application also has a static switch to add an extended urlrenderer, prepending "./" if needed. This fix is merely a quick shot and there might be better alternatives.
WICKET-32c76c4a$$Select component loses it's value$$Select component loses selected option and shows the first option in some situations (one example is when you try to submit a form, but there are validation errors).  It was working fine in 1.4.18, but it's broken in 1.4.19.This must be caused by the solution from this issue https://issues.apache.org/jira/browse/WICKET-3962 I think the problem is likely in Select.isSelected method, where String[] paths = getInputAsArray() is actually an array of uuid-s, so uuid-s are compared to paths.  I haven't tested wicket 1.5, but this problem may also affect 1.5 versions.
WICKET-4a6a573b$$MiniMap.iterator().next() should throw NoSuchElementException$$The wicket.util.collections.MiniMap.iterator().next() should throw NoSuchElementException when there are no more elements to return (line 235), please add: if(i >= size)     throw new NoSuchElementException();
WICKET-d906576c$$MiniMap.iterator().next() should throw NoSuchElementException$$The wicket.util.collections.MiniMap.iterator().next() should throw NoSuchElementException when there are no more elements to return (line 235), please add: if(i >= size)     throw new NoSuchElementException();
WICKET-e1953357$$Confusion between a form component's wicket:id and a PageParameter in Wicket 1.5.x$$A Form has a strange behavior when a component has the same wicket:id than a page parameter.  To create a Bookmarkable link after a form is submited, setResponsePage is called, and a PageParameter object is given as a parameter :  			PageParameters params = new PageParameters(); 			params.add("searchString", searchField.getValue()); 			setResponsePage(SomePage.class, params);  In Wicket 1.5, if "searchString" is also a form-component's wicket:id, the form will only be submitted once :  searchField.getValue() will always return the first value entered by the user.   Here's an example :   public class SearchPanel extends Panel {  	public SearchPanel(String id) { 		super(id); 		add(new SearchForm("searchForm")); 	}  	private class SearchForm extends Form<String> {  		private static final long serialVersionUID = 1L; 		private TextField<String> searchField;  		public SearchForm(String id) { 			super(id); 			searchField = new TextField<String>("searchString", new Model<String>("")); 			add(searchField); 		}  		@Override 		public void onSubmit() { 			PageParameters params = new PageParameters(); 			params.add("searchString", searchField.getValue()); 			setResponsePage(ListContactsPage.class, params); 		} 	} }   I tested the same application with Wicket 1.4.17 and it was fine. I only had this problem in Wicket 1.5.2 and 1.5.3.
WICKET-9cb617ae$$MockHttpServletResponse.addCookie(Cookie) adds duplicate cookies$$org.apache.wicket.protocol.http.mock.MockHttpServletResponse.addCookie(Cookie) makes a bad check whether the cookie to be added is already in the list of cookies. Since javax.servlet.http.Cookie doesn't implement #equals() "cookies.remove(cookie)" wont remove the previous cookie because the identity is different.  According to http://www.ietf.org/rfc/rfc2109.txt, p.4.3.3 :   If a user agent receives a Set-Cookie response header whose NAME is    the same as a pre-existing cookie, and whose Domain and Path    attribute values exactly (string) match those of a pre-existing    cookie, the new cookie supersedes the old.  However, if the Set-    Cookie has a value for Max-Age of zero, the (old and new) cookie is    discarded.  Otherwise cookies accumulate until they expire (resources    permitting), at which time they are discarded.  I.e. the equality is on the name, path and domain.
WICKET-50b52742$$ByteArrayResource throws error if data is null$$When ByteArrayResource#getData(org.apache.wicket.request.resource.IResource.Attributes) returns null, the class throws a WicketRuntimeException.  This behavior differs from DynamicImageResource and ResourceStreamResource which instead issue the following call: response.setError(HttpServletResponse.SC_NOT_FOUND);  ByteArrayResource should follow the same behavior. This would allow for instance to use it for resources which depend on the contents of attributes.getParameters(). When the parameters are invalid, a 404 should be issued instead of an exception.
WICKET-b4274415$$StringValueConversionException for correct situation$$StringValue.toOptionalLong() produces org.apache.wicket.util.string.StringValueConversionException if empty string was passed. Let me suggest, that this behavior should be changes for all toOptionalXXX methods except getOptionalString method.  The problem in inner code:  The problem in following code:  public final Long toOptionalLong() throws StringValueConversionException     {         return (text == null) ? null : toLongObject();     }  Should be something like this:  The problem in following code:  public final Long toOptionalLong() throws StringValueConversionException     {         return Strings.isEmpty() ? null : toLongObject();     }  But there is another problem: what to do if incorrect param was passed - for example "abc" for parameter of Long type?
WICKET-e24874da$$StringResourceModels doesn't seem to detach properly$$If a StringResourceModel contains a model for property substitutions, and there has not been assigned a component it is relative to on construction time, it will not detach the property substitution model.  See this thread for a full explanation http://apache-wicket.1842946.n4.nabble.com/StringResourceModels-doesn-t-seem-to-detach-properly-td4257267.html
WICKET-9decad35$$POST params ignored by IPageParametersEncoder#decodePageParameters()$$As per this conversation: http://apache-wicket.1842946.n4.nabble.com/how-to-get-https-port-number-in-Wicket-1-5-td4295139.html  it seems that POST params are not properly processed and made available as PageParameters. Can anyone say whether this is intended behavior or not? I will attach a Quickstart to demonstrate.  Martin's proposed fix is straightforward, but I am not comfortable enough with Wicket internals to say whether or not this would break something.  Thanks
WICKET-4f08e6f2$$CryptoMapper does not work for applications having a home page that needs query parameters$$CryptoMapper.decryptUrl() should not return null for requests like http://myhost/MyApplication/app/?param=xx   As a possible fix one can replace  if (encryptedUrl.getSegments().isEmpty() && encryptedUrl.getQueryParameters().isEmpty()) {            return encryptedUrl; }  with   if (encryptedUrl.getSegments().isEmpty()) {            return encryptedUrl; }  but I suspect that the original test is intended to answer to another use case...
WICKET-02ebc8ae$$BufferedWebResponse fails to add/clear cookie in redirect$$bufferedWebResponse.addCookie( cookie );  That fails under certain conditions: (1) when called on the last of three 302 redirects during OpenID login; and (2) on single redirect immediately after container startup, though it later recovers.  Failure confirmed in Firebug; no cookies sent in any of the response headers.  My workaround is to bypass the buffered response.  This works:  ((HttpServletResponse)bufferedWebResponse.getContainerResponse()).addCookie( cookie );
WICKET-1485a856$$Form components' name/value are encoded in stateless form's action url$$Stateless forms aren't working well as you can see on wicket examples: http://www.wicket-library.com/wicket-examples/stateless/foo  The first time you submit, (for example, the value 10), everything works as supposed to. If you now change the value (to 11 for example) and submit the form, the value wicket shows is 10.  I think the problem is stateless forms are generating an action URL with submitted values on query string, and when you resubmit the form, this values on query string replace the POST(or GET) values.
WICKET-7ca927c1$$HttpSession getSession() in MockHttpServletRequest is not compliant with the j2ee servlet spec$$The implementation of httpRequest.getSession(); for MockHttpServletRequest seems not correct since it can return null when the servler api specs (http://docs.oracle.com/javaee/1.4/api/) says:  public HttpSession getSession() Returns the current session associated with this request, or if the request does not have a session, creates one.  So as far as I understand httpRequest.getSession(); and httpRequest.getSession(true); are equivalent  The MockHttpServletRequest implementation is     public HttpSession getSession()    {        if (session instanceof MockHttpSession && ((MockHttpSession)session).isTemporary())        {            return null;        }        return session;    }  I think it should be     public HttpSession getSession()    {        return getSession(true);    }
WICKET-7a162f77$$org.apache.wicket.validation.ValidatorAdapter class causes problem with validator properties to be loaded$$PROBLEM: <e1nPL> hi I am having such problem:  <e1nPL> I have implemented validator by implementing IValidator<T> interface <e1nPL> and I have impelemnted the same validator by extending AbstractValidator<T> class  CODE:     ===================== VALIDATOR EXTENDED FROM AbstractValidator =====================     package com.mycompany;           import java.util.regex.Pattern;     import org.apache.wicket.IClusterable;     import org.apache.wicket.util.lang.Classes;     import org.apache.wicket.validation.IValidatable;     import org.apache.wicket.validation.IValidator;     import org.apache.wicket.validation.ValidationError;     import org.apache.wicket.validation.validator.AbstractValidator;           /**      *      * @author e1n      */     public class PasswordPolicyValidator<T> extends AbstractValidator<T> {               private static final Pattern UPPER = Pattern.compile("[A-Z]");         private static final Pattern LOWER = Pattern.compile("[a-z]");         private static final Pattern NUMBER = Pattern.compile("[0-9]");                 @Override         public void onValidate(IValidatable<T> validatable) {             final String password = (String)validatable.getValue();                         if (!NUMBER.matcher(password).find()) {                 error(validatable, "no-digit");             }             if (!LOWER.matcher(password).find()) {                 error(validatable, "no-lower");             }             if (!UPPER.matcher(password).find()) {                 error(validatable, "no-upper");             }               }                 @Override         public void error(IValidatable<T> validatable, String errorKey) {             ValidationError err = new ValidationError();             err.addMessageKey(Classes.simpleName(getClass()) + "." + errorKey);             validatable.error(err);         }             }                 =============== VALIDATOR directly implementing IValidator interfce ====================     package com.mycompany;           import java.util.regex.Pattern;     import org.apache.wicket.IClusterable;     import org.apache.wicket.util.lang.Classes;     import org.apache.wicket.validation.IValidatable;     import org.apache.wicket.validation.IValidator;     import org.apache.wicket.validation.ValidationError;     import org.apache.wicket.validation.validator.AbstractValidator;           /**      *      * @author e1n      */     public class PasswordPolicyValidator<T> implements IValidator<T> {               private static final Pattern UPPER = Pattern.compile("[A-Z]");         private static final Pattern LOWER = Pattern.compile("[a-z]");         private static final Pattern NUMBER = Pattern.compile("[0-9]");               public void validate(IValidatable<T> validatable) {             final String password = (String)validatable.getValue();                         if (!NUMBER.matcher(password).find()) {                 error(validatable, "no-digit");             }             if (!LOWER.matcher(password).find()) {                 error(validatable, "no-lower");             }             if (!UPPER.matcher(password).find()) {                 error(validatable, "no-upper");             }               }                 public void error(IValidatable<T> validatable, String errorKey) {             ValidationError err = new ValidationError();             err.addMessageKey(Classes.simpleName(getClass()) + "." + errorKey);             validatable.error(err);         }             }    <e1nPL> I also have properties file which is named after validator class <e1nPL> and placed in the same package <e1nPL> my problem is that when i use to validate my form field validator which implements IValidator interface it is not capable of loading error messages from properties file <e1nPL> but when i am using validator which is extending AbstractValidator class <e1nPL> properties file with error msgs gets loaded POSSIBLE FIX: <e1nPL> ok i have found class which is responsible for my problem and it is probably a bug <e1nPL> org.apache.wicket.validation.ValidatorAdapter <e1nPL> which wraps classes that directly implements IValidator interface <e1nPL> then when resources are loaded, and properties file are searched in class path etc., loaders search in wrong path that is build against org.apache.wicket.validation.ValidatorAdapter  PLACE WHER FIX SHOULD OCCOUR org.apache.wicket.resource.loader.ValidatorStringResourceLoader::loadStringResource(java.lang.Class,java.lang.String,java.util.Locale,java.lang.String,java.lang.String)
WICKET-614e3b50$$improve wicket's handling of empty / null page parameters$$- DefaultPageFactory#newPage() should be sure to not pass 'null' to a page constructor with page parameters
WICKET-5d64196a$$XsltOutputTransformerContainer incorrectly claims markup type "xsl"$$XsltOutputTransformerContainer return "xsl" from getMarkupType(), forcing is on all contained components.  If the components in org.apache.wicket.markup.outputTransformer.Page_1 are reordered (XsltOutputTransformerContainer coming first) the test fails because no markup for SimpleBorder can be found.
WICKET-f88721fd$$Any empty url-parameter will make wicket 1.5 crash$$Adding an empty parameter to the query string will make wicket crash.  http://www.example.com/?oneParam&   How to reproduce in test:  PageParameters params = new PageParameters(); params.set("",""); params.getAllNamed();   Cause: Wicket accepts empty parameters, but when encoding the url for a rendered page it will call params.getAllNamed().  params.getAllNamed() instantiates new NamedPairs, which calls Args.notEmpty() on the key during instantiation, causing the application to crash.   The NamedPair constructor should probably allow empty string as a key, and call Args.notNull() on the key in stead.
WICKET-246d53c5$$adding (and querying) feedback messages at construction time fails.$$See http://www.nabble.com/error%28...%29-No-page-found-for-component-tf3497125.html  Currently, adding (and querying) feedback messages fails whenever it is done on components that are not yet added to a page (or were removed from them due to component replacement).  There are two ways to fix this. The first fix is attached as a patch, and basically uses a thread local to temporarily store the messages and distribute them to the relevant page instances just in time or when rendering starts. The advantage of this method is that it is completely back wards compatible.  The other way to fix this is to store all messages, whether component specific or not, in the session, and pull them from there. We need to be careful about how/ when to clean these error messages up though. We can use this issue to think about it a little bit more.
WICKET-54c86ebb$$PageProvider should create a new Page instance if PageParameters are changed, even if a stored page exists.$$The 'getStoredPage(int)' method returns a stored page instance even if user changes parameter values encoded into URL, and the PageParameters object of the stored page instance is never changed. So same page is displayed always though user changes url on browser manually.  ** HOW TO REPRODUCT **  1. unpack the attached sample project 'pagebug.tar.gz'. 2. mvn jetty:run 3. access to http://localhost:8080/user/user1  You will see a form filled with information about user 1. The user's name is 'user 1', age is 30 and country is 'Japan'. The mount path of this page is '/user/ {userId}'. so 'user1' in the accessed url is a parameter value.  after accessing to the url, the url will be changed to http://localhost:8080/user/user1?0 .  it contains the page id of the currently displayed page.  4. change some values and submit the form. page id will be changed on every submit.  5. change only parameter value in url to 'user2'. Never change page-id.  for example, if you now access to http://localhost:8080/user/user1?5, change the url to http://localhost:8080/user/user2?5 .  6. This program must display information about user2, because the parameter value of url is changed. But you will see the information of user 1. Wicket always display the page of page-id = 5 (even though user changed url manually).  In this sample program, I use LoadableDetachableModel for retrieving current parameter-value. But I don't get the new parameter-value because pageParameters object in a page instance is never changed after the construction. pageParameters is fixed in the constructor of Page class.  I think that there are no easy way to retrieve parameter-values encoded into mount-path. Request.getRequestParameters() does not contain parameters encoded into mount-path. So there are no work-around for this issue.   ** HOW TO FIX THIS ISSUE **  We must return null from getStoredPage(int) method of PageProvider class, if current PageParameters is not same with the PageParameters of a stored page. In current code, getStoredPage(int) checks only if the class of both pages are same. We must check the PageParameters of both pages.   ** PATCH **  I attached a pache for PageProvider class. try it.
WICKET-2624d2db$$SmartLinkLabel failing to process email with -$$In a similar vein to WICKET-3174 - using SmartLinkLabel with an email address that includes a "-" generates a link only on the right-most part of the address.   Example:  - my-test@example.com  Will generate a link like:  - my-<a href="mailto:test@example.com">test@example.com</a>   The addition of the "-" char is a valid email address format.
WICKET-53442bb4$$Component#setDefaultModel() should call #modelChanging()$$Component#setDefaultModel() should call #modelChanging() as #setDefaultModelObject() does. It worked by chance so far because addStateChange() is called.  http://markmail.org/thread/uxl6uufusggqbb6s
WICKET-e6582c52$$URL with a previous page version ignores requested page based on mount path$$See discussion on http://mail-archives.apache.org/mod_mbox/wicket-users/201203.mbox/browser  With 2 mounts /page1 and /page2 to stateful pages and the following sequence: 1-With a new session, user visits "/page1". Displayed URL is "/page1?0" 2-Whatever, without expiring session 3-User requests URL "/page2?0" because it was bookmarked, received via email, etc. 4-Rendered page is "/page1?0" which was stored in the page map. The actual URL displayed is "/wicket/bookmarkable/com.mycompany.Page1?0"  If a requested page id exists but does not match the page class mounted on the actual requested url, Wicket should not use the old page version. This is very counter-intuitive for users having bookmarks to stateful pages or exchanging links.
WICKET-35843c19$$HtmlHandler wrongly handles tags not requiring closed tags if the markup does not have "top" level tag$$Hi,   I have custom component (extends MarkupContainer implements IMarkupCacheKeyProvider, IMarkupResourceStreamProvider) which fetches its HTML markup from database.  Following HTML markup:   <img alt="" src="logo.png">  <br>Some text  <br>Some more text   causes following error:   2012-04-12 10:52:53,012 [http-8080-6] ERROR: Unexpected error occurred  Unable to find close tag for: '<img alt="logo" src="logo.png">' in org.apache.wicket.util.resource.StringResourceStream@3d7e16fc   MarkupStream: [unknown]          at org.apache.wicket.markup.MarkupFragment.<init>(MarkupFragment.java:127)          at org.apache.wicket.markup.MarkupStream.getMarkupFragment(MarkupStream.java:485)          at org.apache.wicket.MarkupContainer.autoAdd(MarkupContainer.java:244)          at org.apache.wicket.MarkupContainer.renderNext(MarkupContainer.java:1421)          at org.apache.wicket.MarkupContainer.renderAll(MarkupContainer.java:1596)          at org.apache.wicket.MarkupContainer.renderComponentTagBody(MarkupContainer.java:1571)          at org.apache.wicket.MarkupContainer.onComponentTagBody(MarkupContainer.java:1525)   I think the problem is that org.apache.wicket.markup.parser.filter.HtmlHandler does not handle such markup correctly. It does not call ComponentTag.setHasNoCloseTag(true) for the img tag. Such call is missing in postProcess() method. I think that this problem can be fixed by inserting:   top.setHasNoCloseTag(true);   after line 80 in HtmlHandler.java file.    Michal
WICKET-a4caaa57$$AbstractTextComponent not escaping html data by default therefore user text is not redisplayed correctly$$User input is not escaped in all text fields by default (and the default is not configurable).  This leads to user entered text not being redisplayed correctly.  * You can replicate using the project from WICKET-3330. * Just enter the text my&frac12;companyname and press enter * The field will not redisplay the text entered properly
WICKET-b672cb2d$$Spaces in path cause ModifcationWatcher to fail$$The ModificationWatcher isn't able to reload resource files if there's a space in the path.  The problem is that Files#getLocalFileFromUrl(String) receives an URL encoded String in which spaces are encoded to %20. They are never decoded and passed to File(). The fix is not to use the external representation of an URL but the file representation.
WICKET-4ee5ad1f$$Stack overflow when render malformed html.$$Stack overflow when render malformed html.  Please, note that </HEAD> element is inserted after </body>.  HTML: <html> <head> <body> Malformed HTML </body> </head> </html>  Java: package com.mycompany;  import org.apache.wicket.markup.html.WebPage; public class Test1 extends WebPage { 	private static final long serialVersionUID = -4267477971499123852L;  }   Thanks.
WICKET-a88882f7$$Wicket example 'forminput' is broken due to bad url for IOnChangeListener$$http://localhost:8080/forminput (wicket-examples) doesn't change the locale of the labels when the locale select is changed. The reason seems to be the produced url: './.?5-1.IOnChangeListener-inputForm-localeSelect'  This is parsed to a Url with one empty segment and thus HomePageMapper doesn't match it and doesn't handle it.
WICKET-e62ded51$$discrepancy between JavaDoc and code in MarkupContainer#visitChildren()$$The JavaDoc for  MarkupContainer#visitChildren() states that  "@param clazz The class of child to visit, or null to visit all children"  The parameter clazz is used to create a new ClassVisitFilter which in its visitObject() does not check for clazz == null, leading to a NPE.
WICKET-b91154ea$$Inline enclosure doesn't work if wicket:message attribute is used on the same tag$$Markup like:          <div wicket:enclosure="child" wicket:message="title:something"> 	        <div>Inner div 		        <span wicket:id="child">Blah</span> 	        </div>         </div>  doesn't work (Inner div is visible, no matter whether 'child' is visible or not) because the auto component created for wicket:message breaks somehow wicket:enclosure.
WICKET-ccb8fc9e$$Inline enclosure doesn't work if wicket:message attribute is used on the same tag$$Markup like:          <div wicket:enclosure="child" wicket:message="title:something"> 	        <div>Inner div 		        <span wicket:id="child">Blah</span> 	        </div>         </div>  doesn't work (Inner div is visible, no matter whether 'child' is visible or not) because the auto component created for wicket:message breaks somehow wicket:enclosure.
WICKET-9a6a06be$$NullPointerException in org.apache.wicket.markup.html.form.ValidationErrorFeedback$$org.apache.wicket.markup.html.form.ValidationErrorFeedback throws a NPE in the following situation:  - Form with a TextField<Integer> that has a RangeValidator - value outside range is entered - form is submitted  See attached quickstart.
WICKET-dfc56674$$DiskDataStore returns the wrong page when the page disk space is full$$If the configured file size for the session data is overflowed (see org.apache.wicket.settings.IStoreSettings#setMaxSizePerSession(Bytes)) then Wicket may return wrong page data (bytes) for a expired page.  The problem is in org.apache.wicket.pageStore.PageWindowManager#idToWindowIndex which may have several page ids (the keys) pointing to the same window index (values).
WICKET-c66cf607$$Link always causes Page to become stateful, regardless of visibility$$Despite the changes made in WICKET-4468 , an invisible Link still causes a Page to become stateful.   The problem seems to be that Component#isStateless does this before even checking the visibility:   		if (!getStatelessHint()) 		{ 			return false; 		}   ... and Link#getStatelessHint() just contains just "return false" .
WICKET-556a2236$$Do not use the parsed PageParameters when re-creating an expired page$$WICKET-4014 and WICKET-4290 provided functionality to re-create an expired page if there is a mount path in the current request's url. There is a minor problem with that because the page parameters are passed to the freshly created page. I.e. parameters for a callback behavior are now set as page construction parameters. Since the execution of the behavior is ignored for the recreated page these parameters should be ignored too.
WICKET-5e1bf8d8$$Do not use the parsed PageParameters when re-creating an expired page$$WICKET-4014 and WICKET-4290 provided functionality to re-create an expired page if there is a mount path in the current request's url. There is a minor problem with that because the page parameters are passed to the freshly created page. I.e. parameters for a callback behavior are now set as page construction parameters. Since the execution of the behavior is ignored for the recreated page these parameters should be ignored too.
WICKET-9dab1bb5$$bug in Duration.toString(Locale locale)$$Duration.toString(Locale locale) misses milliseconds in line 529
WICKET-b19a3d69$$WicketTester.assertRedirectUrl always fails because it always thinks the redirect was null$$I have a page which always redirects.  When I write a test for this page, tester.assertRedirectUrl(...) always fails with the assertion failure showing that the redirect URL was null.  The page does redirect when running the application for real and I have stepped through in the debugger when running the test and it goes all the way to the HttpServletResponse.sendRedirect call.  However, in the same debugging session, tester.getLastResponse().getRedirectLocation() == null  Cut-down example follows.   public class AlwaysRedirectPage extends WebPage {     public AlwaysRedirectPage()     {         // redirects to another web server on the same computer         throw new RedirectToUrlException("http://localhost:4333/");     } }  public class TestAlwaysRedirectPage {     @Test     public void test()     {         WicketTester tester = new WicketTester();         tester.startPage(AlwaysRedirectPage.class);         tester.assertRedirectUrl("http://localhost:4333/");     } }
WICKET-dd1df04b$$onError call order doesn't match onSubmit$$onError in Forms and Buttons should be called in the same order as onSubmit (i.e. post-order).
WICKET-ef3adb12$$TabbedPanel CSS "last" is wrong if last step is not visible$$TabbedPanel renders a "last" CSS class for the last tab, this fails however if the last tab is not visible.
WICKET-ccd74641$$The default exception mapper is replying cacheable exceptional responses$$The problem is that some common URLs in the application like to a page instance are responding the cached exception page rather than hitting the server for the page instance being requested. It happens because at some moment in the past a exception page were replied and cached for a request in this URL.
WICKET-2fcb3417$$Url#getQueryString(charset) method returns quesrystring with "?" prefixed to it$$i have just pointed out 6.0.0-beta3/6.x but it must be same in 1.5.x too ,afaik "?" is not considered part of querystring ,"?" is considered separator see http://tools.ietf.org/html/rfc3986#section-3 this method is used in Url#toString() too which can be easily fixed but it may be used at other places too so i don't know if removing "?" will break things now.  so how things break currently RequestUtils.decodeParameters(url.getQueryString(),parameters); decodeparameters will considered first key to be "?key"  so may be requestutils#decodeparameters method should strip away "?" if it's present in the query string before populating pageparameters  thanks!
WICKET-f3ec1503$$XmlPullParser doesn't parse correctly attributes with complex namespace$$Having a markup like: <a class="addthis_button_google_plusone_badge" g:plusone:size="smallbadge"  g:plusone:href="https://plus.google.com/25252/"></a> causes XmlPullParser to throw the following exception:  java.text.ParseException: Same attribute found twice: g:plusone (line 19, column 100)      at org.apache.wicket.markup.parser.XmlPullParser.parseTagText(XmlPullParser.java:673)      at org.apache.wicket.markup.parser.XmlPullParser.next(XmlPullParser.java:294)      at org.apache.wicket.markup.parser.filter.RootMarkupFilter.nextElement(RootMarkupFilter.java:58) .....
WICKET-89184b79$$MountMapper does not support correctly parameter placeholders$$Package mounting doesn't support parameter placeholders. The problem seems to be inside MountMapper which should wrap PackageMapper and take care of substituting placeholders with their actual value.  More precisely this class doesn't read parameter values from PageParameters and it's not very clear to me how it tries to read these values. Does anybody have some hints about this class?
WICKET-f5f802c5$$NumberTextField doesn't accept values <=0 for Double and Float$$The org.apache.wicket.util.lang.Numbers class defines the method : public static Number getMinValue(Class<? extends Number> numberType)  This method return the MatchingNumberTypeClass.MIN_VALUE. But for Double.MIN_VALUE and Float.MIN_VALUE return the smallest positive number, not the smallest negative number like for the other number classes.  One side effect is that by default you can't enter a negative value, or a 0 in a NumberTextField<Double> or NumberTextField<Float>.
WICKET-4fc82e35$$WebApplication doesn't recognize if an incoming request is multipart.$$Thanks to the mail at http://apache-wicket.1842946.n4.nabble.com/Read-POST-based-request-from-external-site-td4651269.html we have spotted a problem with method  newWebRequest of class WebApplication.  It seems that this method doesn't test if the original request is multipart and doing so post parameters go lost.  We should create a  MultipartServletWebRequestImpl when such a type of request is being served. I attach a possible patch but I'm not 100% about two things: - which is the best way to determinate if a HttpServletRequest is multipart? - in order to build a MultipartServletWebRequestImpl we need to provide a string identifier for the upload.   How can we generate it (in my patch it's a constant value)?
WICKET-6a1b2f61$$StringValidator.exactLength has wrong variable in ErrorMessage$$In error message for StringValidator.exactLength is variable  {exact} , but in StringValidator.decorate is added variable length to map and not exact.   Exception when is error message interpolate for show in feedback.  Caused by: java.lang.IllegalArgumentException: Value of variable [[exact]] could not be resolved while interpolating [[' {label}' is not exactly  {exact} characters long.]]  property from application. StringValidator.exact=' {label}' is not exactly  {exact} characters long.  When I added same property in my own properties and change exact to length, it works.
WICKET-a7ce7f91$$DownloadLink doesn't wrap the String model used for file name nor does it detach$$Component DownloadLink doesn't call method wrap of class Component on parameter fileNameModel. This causes models like StringResourceModel to not resolve resource bundles correctly. See the discussion here: http://stackoverflow.com/questions/12196533/how-to-use-wicket-stringresourcemodel-in-downloadlink  The patch seems quite trivial.   Detachment is also missing.
WICKET-21a47387$$Resource bundles are not resolved on PriorityHeaderItems$$If a bundle X provides resource A, and resource A is rendered as priority header item, the resource A is rendered, not bundle X.
WICKET-87ae870f$$"'NEW VALUE' is not a valid Serializable" error during ajax form submission$$I attached a quickstart with a test in TestHomePage#formSubmitsSuccessfully.  The test throws "'NEW VALUE' is not a valid Serializable" error when "NEW VALUE" string in "value" textField is submitted as a part of myForm ajax submission.  The problem is that a call to Objects#convertValue(nonNullNonArrayValue, Object.class) will always return null if nonNullNonArrayValue is a value that is not null and not an array! Shouldn't it always return the first parameter when the second parameter is Object.class?  Sven on Wicket forum suggested to fix this as by adding another if-statement in Objects#convertValue() if (toType.isInstance(value)) {   result = toType.cast(value); }  See the following forum thread for more information http://apache-wicket.1842946.n4.nabble.com/Issues-with-default-type-conversion-in-1-5-td4651857.html
WICKET-fd910746$$FormComponents remain invalid forever if there is no feedback panel$$if there is no feedback panel the error messages are not removed in ondetach and form component re-validation is skipped so the form component, once marked as invalid, will remain invalid forever or at least until its error messages are rendered.  the error messages should be dropped and the form component should be re-validated on every form submit.
WICKET-2f1ece4b$$JavaScriptStripper fails with single line comments$$The valid input x++ // x++  gets transformed to x++ x++  which is syntactically invalid. This breaks the unminified version of bootstrap 2.1.1.  The problem doesn't occur with multiline comments because the linebreaks are preserved there.
WICKET-cda34428$$multiple <style> tags in header are rendered incorrectly$$I created a small quickstart.  The BasePage has some multiple <style> tags. Only he first one is rendered correctly, all following render the tag body only, the surrounding <style></style> is missing.
WICKET-1ac05533$$PageParameters#mergeWith may loose values of the 'other' PP$$The code at org.apache.wicket.request.mapper.parameter.PageParameters#mergeWith() looks like:  for (NamedPair curNamed : other.getAllNamed()) 		set(curNamed.getKey(), curNamed.getValue());  may loose some values if 'other' has a named parameter with several values.With the current code only the last name/value pair is preserved.
WICKET-eccb3b11$$JavaScriptReference escapes given URL$$while trying to integrate gmaps3 in our webapp i had issues with the wicketstuff-gmap3 stuff ( - we need a client-id for our request) ...  so i have: {noformat} public static final String GMAP_API_URL = "%s://maps.google.com/maps/api/js?v=3&sensor=%s&client-id=%s";  response.render(JavaScriptHeaderItem.forUrl(String.format(GMAP_API_URL, schema, sensor, clientid))); {noformat}  the rendered result of this is: {noformat} <script type="text/javascript" src="http://maps.google.com/maps/api/js?v=3&amp;sensor=false&amp;client-id=...."></script> {noformat}  so the requestparameters are encoded  which is happening in the JavaScriptUtils Helper: {noformat} public static void writeJavaScriptUrl(final Response response, final CharSequence url, final String id, boolean defer, String charset) {         response.write("<script type=\"text/javascript\" ");         if (id != null)         {             response.write("id=\"" + Strings.escapeMarkup(id) + "\" ");         }         if (defer)         {             response.write("defer=\"defer\" ");         }         if (charset != null)         {             response.write("charset=\"" + Strings.escapeMarkup(charset) + "\" ");         }         response.write("src=\"");         response.write(Strings.escapeMarkup(url));         response.write("\"></script>");         response.write("\n"); } {noformat} but ... is this right to escape the url?  when i open the above mentioned script, google tells me i have no parameter "sensor" ... which i can understand as ther is only a parameter amp ...
WICKET-6f0863f4$$URL rendering regression$$The way URLs are encoded was changed (WICKET-4645) and now the first request (with ;jsessionid in path) generates invalid internal links: My page is mounted to "/Home/" and I get redirected to "/Home/;jsessionid=1234?0" (fine). There's a Link  on the page and the generated URL for it is "../Home;jsessionid=1234?0-1.ILinkListener-link". Note the missing "/". This results in a 404 and breaks basically all of my system tests.  I'll attach a simple quickstart which demonstrates the problem. It's important to delete the jsessionid cookie before accessing the page.
WICKET-66bfc885$$Handling of semicolons in form action URLs$$What I expect to happen when there is no semicolon support in Wicket is that a URL in a form like below stays intact and will not be cut off at the position of the first semicolon:  <form action="http://localhost:8080/dor/abc_1234:56;023:456_def_78;90.html" method="post"><input type="submit" value="Submit" /></form>  In my application the part abc_1234:56;023:456_def_78;90.html is "named1" in the mapping below:  mount(new MountedMapper("dor/#{named1}", TestPage.class, new MyPageParametersEncoder()));  and parsed in MyPageParametersEncoder.  The officially intended use of semicolons in URLs seems to be specified in "RFC 1808 - Relative Uniform Resource Locators, 2.4.5." (http://www.faqs.org/rfcs/rfc1808.html). But that´s not what I´m looking for.  If I had not some pages running on this syntax, I could easily swap the semicolon with another symbol. Nevertheless and if I´m correctly informed, I think those URLs should not be cut off.  (Quotation from the mailing list)  The quickstart can be tested with the following URLs:  http://localhost:8080/dor/abc_1234:56;023:456_def_78;90.html http://localhost:8080/dor/abc_1234:56%3B023:456_def_78%3B90.html http://localhost:8080/dor/?abc=1234:56%3B023:456&def=78%3B90  The crucial part is the action attribute in the form in the page´s source code, which contains i.e. "./abc_1234:56?-1.IFormSubmitListener-form".
WICKET-ad849602$$Redirect to HTTPS is using wrong port 80 if HttpsConfig with default ports 80/443 is used$$HttpsMapper#mapHandler() doesn't set the Url's port, if the desired protocol uses the standard port.  This leads to UrlRenderer choosing to the request's port as fallback (which is 80 before switching to https).
WICKET-8b294488$$Date converters should use a new instance of DateFormat to be thread safe$$Please consider the linked issue WICKET-4833.  I had to open a new issue because I cannot attach the quickstart project I've prepared to a closed issue.
WICKET-ce172da8$$Return error code 400 when an Ajax request has no base url set in header/request parameters.$$Hello,  currently we've got a problem with faked ajax requests. these ajax  requests misses some parameters, but the wicket-ajax header flag is set.  So ServletWebRequest throws an exception:  java.lang.IllegalStateException: Current ajax request is missing the base url header or parameter          at org.apache.wicket.util.lang.Checks.notNull(Checks.java:38)          at org.apache.wicket.protocol.http.servlet.ServletWebRequest.getClientUrl(ServletWebRequest.java:171)          at org.apache.wicket.request.UrlRenderer.<init>(UrlRenderer.java:59)   These faked requests are so massive, that our application is no longer  monitorable. Our workaround rejects these requests via apache config.   Instead of logging an exception, in deployment mode wicket should log a warning and reject the request
WICKET-6470c3f7$$encodeUrl fails parsing jsessionid when using root context$$We are using Selenium 2.26.0 to test our Wicket application, using Jetty 6.1.25 (also tried 7.0.0.pre5) and Firefox 12 as client browser.  With Wicket 1.5.8 everything worked fine but updating to 1.5.9 the following error occurs on first request:  java.lang.NumberFormatException: For input string: "56704;jsessionid=t3j8z4tsuazh1jfbcnjr8ryg" 	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48) 	at java.lang.Integer.parseInt(Integer.java:458) 	at java.lang.Integer.parseInt(Integer.java:499) 	at org.apache.wicket.request.Url.parse(Url.java:195) 	at org.apache.wicket.request.Url.parse(Url.java:121) 	at org.apache.wicket.protocol.http.servlet.ServletWebResponse.encodeURL(ServletWebResponse.java:194) 	at org.apache.wicket.protocol.http.HeaderBufferingWebResponse.encodeURL(HeaderBufferingWebResponse.java:161) 	at org.apache.wicket.request.cycle.RequestCycle.renderUrl(RequestCycle.java:524) 	at org.apache.wicket.request.cycle.RequestCycle.urlFor(RequestCycle.java:492) 	at org.apache.wicket.request.cycle.RequestCycle.urlFor(RequestCycle.java:477) 	at org.apache.wicket.Component.urlFor(Component.java:3319) 	at org.apache.wicket.markup.html.link.BookmarkablePageLink.getURL(BookmarkablePageLink.java:209) 	at org.apache.wicket.markup.html.link.Link.onComponentTag(Link.java:361) 	at org.apache.wicket.Component.internalRenderComponent(Component.java:2530) 	at org.apache.wicket.MarkupContainer.onRender(MarkupContainer.java:1530) 	at org.apache.wicket.Component.internalRender(Component.java:2389) 	at org.apache.wicket.Component.render(Component.java:2317) 	at org.apache.wicket.MarkupContainer.renderNext(MarkupContainer.java:1428) 	at org.apache.wicket.MarkupContainer.renderAll(MarkupContainer.java:1592) 	at org.apache.wicket.Page.onRender(Page.java:907) 	at org.apache.wicket.markup.html.WebPage.onRender(WebPage.java:140) 	at org.apache.wicket.Component.internalRender(Component.java:2389) 	at org.apache.wicket.Component.render(Component.java:2317) 	at org.apache.wicket.Page.renderPage(Page.java:1035) 	at org.apache.wicket.request.handler.render.WebPageRenderer.renderPage(WebPageRenderer.java:118) 	at org.apache.wicket.request.handler.render.WebPageRenderer.respond(WebPageRenderer.java:246) 	at org.apache.wicket.request.handler.RenderPageRequestHandler.respond(RenderPageRequestHandler.java:167) 	at org.apache.wicket.request.cycle.RequestCycle HandlerExecutor.respond(RequestCycle.java:784) 	at org.apache.wicket.request.RequestHandlerStack.execute(RequestHandlerStack.java:64) 	at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:304) 	at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313) 	at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313) 	at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313) 	at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313) 	at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313) 	at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313) 	at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313) 	at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313) 	at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313) 	at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313) 	at org.apache.wicket.request.cycle.RequestCycle.processRequest(RequestCycle.java:227) 	at org.apache.wicket.request.cycle.RequestCycle.processRequestAndDetach(RequestCycle.java:283) 	at org.apache.wicket.protocol.http.WicketFilter.processRequest(WicketFilter.java:188) 	at org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:244)  Using debugger, the encodeUrl method has variables   fullUrl = http://localhost:56704 encodedFullUrl = http://localhost:56704;jsessionid=8kxeo3reannw1qjtxgkju8yiu  before the exception occurs. I believe this is related to https://issues.apache.org/jira/browse/WICKET-4645.
WICKET-d78132be$$CryptoMapper ignores original queryString parameters$$When an AjaxRequest with parameters (e.g.: Autocomplete.getChoices()) arrives and CryptoMapper decrypts it, original queryString parameters dissapears.  Debugging CryptoMapper, I've checked that this method: private Url decryptUrl(final Request request, final Url encryptedUrl) {         ... }  receives querystrings parameters (on field url.parameter from "request" parameter) and the new Url returned by the method never adds them to its own list.
WICKET-8c827e33$$Header can not be set from IRequestCycleListener#onEndRequest()$$Due to HeaderBufferingWebResponse a header can no longer be set from IRequestCycleListener#onEndRequest().  In 1.4.x this was possible because BufferedWebResponse just passed through all headers to HttpServletResponse.
WICKET-f20b2d70$$Mounted page is not throwing ExpireException with setting setRecreateMountedPagesAfterExpiry(false)$$We have a page that is both bookmarkable (and accessible with certain page parameters) and has a second constructor taking an object.  When ever the session time-out we want to show a session expired page. But we get a exception because Wicket is trying to rebuild the page with no page parameters.  We have set the setting getPageSettings().setRecreateMountedPagesAfterExpiry(false); This works when clicking on (ajax)links, but it's not working when using the back/forward button in the browser (or javascript:history.go(-1)).  I'll attache a quickstart.
WICKET-a4a3a9a6$$AbstractNumberConverter issue when used with NumberFormat#getCurrencyInstance$$Summary of the discussion on users@:  There is an issue when using AbstractNumberConverter when #getNumberFormat returns NumberFormat#getCurrencyInstance() I think the problem is due to AbstractNumberConverter#parse(Object, double, double, Locale):  if (value instanceof String) {         // Convert spaces to no-break space (U+00A0) to fix problems with         // browser conversions.         // Space is not valid thousands-separator, but no-br space is.         value = ((String)value).replace(' ', '\u00A0'); }  Which replace spaces, so a string like "1,5 €" is invalid while being parsed.  public class CurrencyConverter extends AbstractNumberConverter<Double> {     private static final long serialVersionUID = 1L;      public CurrencyConverter()     {     }      @Override     protected Class<Double> getTargetType()     {         return Double.class;     }      @Override     public NumberFormat getNumberFormat(Locale locale)     {         return NumberFormat.getCurrencyInstance(locale);     }      @Override     public Double convertToObject(String value, Locale locale)     {         locale = Locale.FRANCE;          return this.parse(value, Double.MIN_VALUE, Double.MAX_VALUE, locale);  //        This does work: //        final NumberFormat format = this.getNumberFormat(locale); //        return this.parse(format, value, locale);     } }  As Sven indicates, there is (yet another) issue in Java currency formating (space as thousand separator) http://matthiaswessendorf.wordpress.com/2007/12/03/javas-numberformat-bug/ http://bugs.sun.com/view_bug.do?bug_id=4510618  So will I let you decide whether or not you wish to fix it (the space before the currency symbol).  Thanks & best regards, Sebastien.
WICKET-ee02c883$$Mounted bookmarkable Page not recreated on Session Expiry$$With the default true of org.apache.wicket.settings.IPageSettings#getRecreateMountedPagesAfterExpiry() PageExpiryException is thrown when a Link on a page is clicked.  I find it very useful and in fact indispensible to rely on RecreateMountedPagesAfterExpiry especially on logout links.  But it doesn't seem to work for me in 6.4.0. I think this is because Link#getUrl() calls Component#utlFor() which requires a stateless page for this to work:  		if (page.isPageStateless()) 		{ 			handler = new BookmarkableListenerInterfaceRequestHandler(provider, listener); 		} 		else 		{ 			handler = new ListenerInterfaceRequestHandler(provider, listener); 		}  With a stateless page a url is:  http://localhost:8080/wicket/HomePage?-1.ILinkListener-toolBar-signout  With a non stateless but bookmarkable page a url is:  http://localhost:8080/wicket/page?1-1.ILinkListener-toolBar-signout  So I guess that a stateful page cannot be recovered because after session expiry Wicket cannot find out what the page is. It is not coded in the URL.  Looking at the semantics of UrlFor(), I thought this might be a bug and I copied and changed the code in my Link subclass from  //		if (page.isPageStateless()) { to:         if (page.isBookmarkable()) { 		 It works as expected but I don't know whether it would break other things if implemented in Wicket.  I guess I am not the only one who needs recovery for bookmarkable pages in this way. Would it be possible to change Wicket to fix this so it becomes possible?
WICKET-917dd2b5$$Handling of NO_MINIFIED_NAME in PackageResourceReference#internalGetMinifiedName()$$The Value NO_MINIFIED_NAME is not handled correctly as entry in the MINIFIED_NAMES_CACHE in PackageResourceReference#internalGetMinifiedName()     	private String internalGetMinifiedName() 	{ 		String minifiedName = MINIFIED_NAMES_CACHE.get(this); 		if (minifiedName != null && minifiedName != NO_MINIFIED_NAME) 		{                                                        ^^^^^^^ 			return minifiedName;                 }                 ...  You should remove the condition "minifiedName != NO_MINIFIED_NAME" here to leverage the  MINIFIED_NAMES_CACHE for NO_MINIFIED_NAME cache entries. Otherwise you always run into the resource resolving code if there is no minified resource.
WICKET-2b1ce91d$$Page not mounted with WebApplication#mountPackage$$A bookmarkable page FormPage is mounted via WebApplication#mountPackage().  If this page is opened via IModel model; setResponsePage(new FormPage(IModel model)); then the URL is /wicket/page?0 which is not mounted.  If the page is mounted via WebApplication#mountPage() then the URL is mounted as expected.  If the page is not mounted then the users get PageExpiredException which in this case is unrecoverable.
WICKET-56169634$$Page mount with an optional named parameter overtakes a mount with more specific path$$See the discussion in http://markmail.org/thread/sgpiku27ah2tmcim  Having:   mountPage("/all/sindex",Page1.class);   mountPage("/all/#{exp}", Page2.class);  Request to /all/sindex will be handled by Page2.  Compatibility score for optional parameters should be lower than mandatory parameters which should be lower than exact value.
WICKET-8e6a6ec5$$Fragment and Component with same id fail with misleading exception$$A page having a component from inherited markup *and* fragment with the *same* id fails with misleading exception message.  Exception message: The component(s) below failed to render. Possible reasons could be that: 1) you have added a component in code but forgot to reference it in the markup (thus the component will never be rendered), 2) if your components were added in a parent container then make sure the markup for the child container includes them in <wicket:extend> ... and list of component id's from fragment multiplied by amount of rows in DataTable  Cause: The markup of the component is used by the fragment.
WICKET-6e794ad0$$404 Error on Nested ModalWindows in IE7 and IE8$$When opening a ModalWindow inside a ModalWindow, the inner ModalWindow generates a 404 error.  Both windows use a PageCreator for content.  To replicate, you must use an actual IE 7 or IE 8 browser, as this does not replicate using developer tools and setting the document and brower to IE 7.  The problem can be seen at http://www.wicket-library.com/wicket-examples/ajax/modal-window.  I will attach a Quickstart as well.
WICKET-a2f848f2$$404 Error on Nested ModalWindows in IE7 and IE8$$When opening a ModalWindow inside a ModalWindow, the inner ModalWindow generates a 404 error.  Both windows use a PageCreator for content.  To replicate, you must use an actual IE 7 or IE 8 browser, as this does not replicate using developer tools and setting the document and brower to IE 7.  The problem can be seen at http://www.wicket-library.com/wicket-examples/ajax/modal-window.  I will attach a Quickstart as well.
WICKET-d3d42d42$$404 Error on Nested ModalWindows in IE7 and IE8$$When opening a ModalWindow inside a ModalWindow, the inner ModalWindow generates a 404 error.  Both windows use a PageCreator for content.  To replicate, you must use an actual IE 7 or IE 8 browser, as this does not replicate using developer tools and setting the document and brower to IE 7.  The problem can be seen at http://www.wicket-library.com/wicket-examples/ajax/modal-window.  I will attach a Quickstart as well.
WICKET-faaae8d3$$404 Error on Nested ModalWindows in IE7 and IE8$$When opening a ModalWindow inside a ModalWindow, the inner ModalWindow generates a 404 error.  Both windows use a PageCreator for content.  To replicate, you must use an actual IE 7 or IE 8 browser, as this does not replicate using developer tools and setting the document and brower to IE 7.  The problem can be seen at http://www.wicket-library.com/wicket-examples/ajax/modal-window.  I will attach a Quickstart as well.
WICKET-381b90fd$$Cookies#isEqual(Cookie, Cookie) may fail with NullPointerException$$If c1.getPath == null but c2.getPath != null then a NPE will occur. Same is valid for the 'domain' property.
WICKET-217fbb3b$$Ajax update renders parent/child JS in different order than initial Page render$$See attached quickstart.  On initial page load, the child Javascripts are rendered and executed first, followed by the parent's JS - in this case a Datatables.net JS. Everything works fine.  However, if you click on a link in the DefaultDataTable, we trigger a DDT refresh via Ajax, and then you can see that the parent's JS is executed first, before the child JS - this causes a problem since the parent JS modifies the visible rows in the table and Wicket can no longer find some of the child rows.  I expected the order of JS contributions to be the same for initial page render and any Ajax updates.
WICKET-581c7306$$InlineEnclosure are piling up on each render$$InlineEnclosureHandler#resolve() uses an auto-incremented id for its resolved InlineEnclosure,   On the next render, a new instance will be resolved, since the id of the already resolved InlineEnclosure does not match the id in the markup.  But InlineEnclosures are not removed after render as other auto-components, thus all instances pile up in the owning container of the markup.
WICKET-ba516f02$$FormTester throws an exception when a Palette component is added to a Form associated with a compound property model$$FormTester throws an exception when a Palette component is added to a Form associated with a compound property model: org.apache.wicket.WicketRuntimeException: No get method defined for class ... expression: choices  It worked fine in Wicket 6.5.0, and works fine if the form is not associated with a compound property model.
WICKET-74e77676$$ISecuritySettings#getEnforceMounts(true) prevents access to *all* non-mounted bookmarkable pages$$ISecuritySettings#setEnforceMounts(true) is meant to be used to prevent access to mounted-pages via BookmarkableMapper, e.g. when Page1.class is mounted:     http://localhost:8080/niceurl/a/nice/path/to/the/first/page  ... then the following url will not be accepted:     http://localhost:8080/niceurl/wicket/bookmarkable/org.apache.wicket.examples.niceurl.Page1  But starting with Wicket 1.5.x access to *all* non-mounted pages via BookmarkableMapper is prevented, i.e. no url "http://localhost:8080/niceurl/wicket/bookmarkable/*" is matched.
WICKET-d110e307$$wicket-bean-validation: Bean validation PropertyValidator only works with direct field access$$There's a substring indexing bug in the wicket-bean-validation module in org.apache.wicket.bean.validation.DefaultPropertyResolver that causes it to only work with direct field access and fail when field is missing and getter method should be used.  The problem is on this line:      String name = getter.getName().substring(3, 1).toLowerCase() + getter.getName().substring(4);  Which should be:      String name = getter.getName().substring(3, 4).toLowerCase() + getter.getName().substring(4);  (or simply a single character access)
WICKET-ed780cc7$$Parantheses problem with UrlValidator$$One of our users got an error message when trying to add a new URL:  'http://en.wikipedia.org/wiki/Genus_(mathematics)' is not a valid URL  I just created very quickly a junit test and it fails:  String[] schemes = {"http"}; UrlValidator urlValidator = new UrlValidator(schemes); assertTrue(urlValidator.isValid("http://en.wikipedia.org/wiki/Genus_(mathematics)"));
WICKET-518c933b$$Url#toString(StringMode.FULL) throws exception if a segment contains two dots$$When invoking toString(StringMode.FULL) for a URL like /mountPoint/whatever.../ an IllegalStateException is thrown with message: Cannot render this url in FULL mode because it has a `..` segment: /mountPoint/whatever.../  The method does not actually check for `..` segments but rather checks whether path.contains("..")
WICKET-4b7367ef$$Problems with cookies disabled when using 301/302 and also 303 (even with cookies)$$As mentioned in the mailing list by Martin, i open this as a bug...  Its not possible to use 303 as redirect (SC_SEE_OTHER) because thats not supported, only 302 and 301 are supported but this is defined in RFC HTTP 1.1 from 1997.   301 will add the Location header - which works as expected when disabling cookies. But a 302 (which is what i prefer) will redirect to the same page because the Location header is missing. When i enable cookies, its working.  Example can be found here: https://github.com/olze/WicketRedirect
WICKET-e8dab4a0$$Wicket does not correctly handle http OPTIONS requests$$currently these requests cause regular processing (page rendering), when in fact they should have a special response.  rendering the page in OPTIONS causes renderCount to be incremented and this messes with the subsequent request to the same url via a GET or POST
WICKET-184e51e9$$WicketTester MockHttpRequest.getCookies very slow / OutOfMemory$$We have an extensive set of WicketTester tests. Recently, the wicket RELEASE in the maven repository changed to 6.7.0. After the new version, our tests got very slow.  When profiling, I discovered that the MockHttpRequest.getCookies() was taking up a lot of time. Also, tests failed because of OutOfMemory exceptions. My guess is that somehow a lot of objects are created at such speeds that the GC cannot clean them  I will investigate further, but switching back to 6.6.0 solved the issue.   [Edit] The tests are run with TestNG and using 'mvn test'
WICKET-961f2477$$URL query parameter values containing equals sign get cut off$$When calling a page with a query parameter like 'param1=val1=val2' the value of 'param1' obtained from PageParameters will be 'val1'. Everything after the equals sign inside the parameter value gets cut off.
WICKET-0d4d1df7$$Session should be bound when adding messages to it$$When using the Sessions info(), error() and success() methods, and the session is temporary, the messages can be dropped silently. This happens when on stateless pages and a redirect happens in the same request during which a session message is added.  The fix for this could be to make sure the session is bound and call Session#bind() automatically when a session message is added.  Email thread: http://wicket-users.markmail.org/thread/zd72s4gwnlp5d7ch
WICKET-34634266$$StringResourceModel doesn't detach model in some cases$$We have come across an issue with StringResourceModel not detaching the model it holds under a certain condition.  The problem is the case where the StringResourceModel is created but it is not used - for example when it is on a tab that is not displayed.  StringResourceModel is a subclass of LoadableDetachableModel and it simply implements onDetach(), letting the superclass decide whether it is attached or not. The problem is that when StringResourceModel is created, LoadableDetachableModel.attached will be false.  If the StringResourceModel is never read (i.e. getObject() is not called) the LoadableDetachableModel will not be marked as attached and when detach() is called, onDetach() will not be called.  Therefore StringResourceModel will not call detach() on the model that it holds.
WICKET-2293764f$$Base url is incorrect for error dispatched pages$$The fix for https://issues.apache.org/jira/browse/WICKET-4387 includes the following code in org.apache.wicket.protocol.http.servlet.ServletWebRequest#ServletWebRequest(HttpServletRequest httpServletRequest, String filterPrefix, Url url):  if (forwardAttributes != null || errorAttributes != null) 		{ 			if (LOG.isDebugEnabled()) 			{ 				LOG.debug("Setting filterPrefix('{}') to '' because there is either an error or a forward. {}, {}", 						new Object[] {filterPrefix, forwardAttributes, errorAttributes}); 			} 			// the filter prefix is not needed when the current request is internal 			// see WICKET-4387 			this.filterPrefix = "";  The filterPrefix is actually needed later when a request is made due to an error (e.g. 404):  public Url getClientUrl() 	{ 		if (errorAttributes != null && !Strings.isEmpty(errorAttributes.getRequestUri())) 		{ 			String problematicURI = Url.parse(errorAttributes.getRequestUri(), getCharset()) 				.toString(); 			return getContextRelativeUrl(problematicURI, filterPrefix);  With filterPrefix=="" the urls for any resources in the error page are wrong.
WICKET-9e6efa61$$The DateTimeField.onBeforeRender() method does not format the fields correctly.$$The current implementation relies on the org.joda.time.MutableDateTime instance to format the date, hours, amOrPm, and minutes fields. Unfortunately, the MutableDateTime constructor is not provided with the client's TimeZone value (assuming it is set). As a result, the joda library uses the JVM's default timezone. If the defaul timezone differs from the client's timezone, the formatted fields may turn out to be incorrect.
WICKET-55eb5212$$NPE when using ComponentRenderer.renderComponent on a panel with <wicket:enclosure>$$Hi,  Consider this example: <wicket:panel> 	<wicket:enclosure child="externalLink"> 		<a wicket:id="externalLink">Link</a> 	</wicket:enclosure> </wicket:panel>  When trying to render such a panel with ComponentRenderer.renderComponent, a NPE is thrown because Wicket try to render Enclosure without initializing it.  Root cause: java.lang.NullPointerException 	at org.apache.wicket.markup.html.internal.Enclosure.isVisible(Enclosure.java:143) 	at org.apache.wicket.Component.determineVisibility(Component.java:4363) 	at org.apache.wicket.Component.internalBeforeRender(Component.java:916) 	at org.apache.wicket.Component.beforeRender(Component.java:991) 	at org.apache.wicket.Component.internalPrepareForRender(Component.java:2214) 	at org.apache.wicket.Component.render(Component.java:2303) 	at org.apache.wicket.MarkupContainer.renderNext(MarkupContainer.java:1390) 	at org.apache.wicket.MarkupContainer.renderAll(MarkupContainer.java:1554) 	at org.apache.wicket.MarkupContainer.renderComponentTagBody(MarkupContainer.java:1529) 	at org.apache.wicket.MarkupContainer.renderAssociatedMarkup(MarkupContainer.java:689) 	at org.apache.wicket.markup.html.panel.AssociatedMarkupSourcingStrategy.renderAssociatedMarkup(AssociatedMarkupSourcingStrategy.java:76) 	at org.apache.wicket.markup.html.panel.PanelMarkupSourcingStrategy.onComponentTagBody(PanelMarkupSourcingStrategy.java:112) 	at org.apache.wicket.Component.internalRenderComponent(Component.java:2549) 	... 29 more  See the attached quickstart.  I've looked a little into it, and it seems that RenderPage (used by ComponentRenderer to render components) is never initialized. Therefore the panel's children are never initialized too (see MarkupContainer l.930), and this causes Enclosure to have a null childComponent.  Thanks.
WICKET-8e518d88$$CDI integration fails in Glassfish 4.0 with WELD-000070$$When CDI is configured in the Application and a page has a non-static inner class the page throws exception, regardless of whether there are any injected fields.  Caused by: org.jboss.weld.exceptions.DefinitionException: WELD-000070 Simple bean [EnhancedAnnotatedTypeImpl] private  class com.inversebit.HomePage AForm cannot be a non-static inner class 	at org.jboss.weld.injection.producer.BasicInjectionTarget.checkType(BasicInjectionTarget.java:81) 	at org.jboss.weld.injection.producer.BasicInjectionTarget.<init>(BasicInjectionTarget.java:69) 	at org.jboss.weld.injection.producer.BeanInjectionTarget.<init>(BeanInjectionTarget.java:52) 	at org.jboss.weld.manager.InjectionTargetFactoryImpl.createInjectionTarget(InjectionTargetFactoryImpl.java:95) 	at org.jboss.weld.manager.InjectionTargetFactoryImpl.createInjectionTarget(InjectionTargetFactoryImpl.java:78) 	... 65 more
WICKET-9c8f658a$$AjaxFormChoiceComponentUpdatingBehavior fails for choices containing other invalid FormComponents$$If a TextField inside a RadioGroup has a ValidationError, processing of AjaxFormChoiceComponentUpdatingBehavior will erroneously update the group's model:  - RadioGroup#validate() does not convert the input, because #isValid() returns false (since the nested textfield has an error message) - the behavior tests #hasErrorMessage() on the group, which returns false (since the group itself doesn't have an error message) - the behavior continues processing with a null value
WICKET-b61fe92c$$Wicket generates invalid HTML by expanding col tags$$hi,  I just noticed that wicket expands col tags, even though the (x)html specifications forbids it.  take this markup as an example:  <table>     <colgroup>         <col width="20%" />         <col width="80%" />     </colgroup>     <tbody>         <tr>             <td>I take a fifth of the available space</td>             <td>I take four fifth of the available space</td>         </tr>     </tbody> </table>  Instead of return this as-is, it get's converted to:  <table>     <colgroup>         <col width="20%"></col>         <col width="80%"></col>     </colgroup>     <tbody>         <tr>             <td>I take a fifth of the available space</td>             <td>I take four fifth of the available space</td>         </tr>     </tbody> </table>  But the specifications mention that col tags must not have end tags. This may be related to WICKET-2765, as this seems to be the point when col was added to the OpenCloseTagExpander class. Note that it is ok to have a non closing col tag in html (self-closing in xhtml). It's all about generating a separated end tag.  This happens in wicket 6.8, but I guess it's relevant to all versions down to wicket 1.4.  Specs for reference:  http://www.w3.org/TR/1999/REC-html401-19991224/struct/tables.html#edef-COL http://www.w3.org/TR/html-markup/col.html  Kind regards,  Konrad
WICKET-44a4132f$$Broken Link in Tomcat because of Page Mount$$I post this message on the user mailing List (http://apache-wicket.1842946.n4.nabble.com/Broken-Link-in-Tomcat-because-of-Page-Mount-tt4659663.html) and Martin Grigorov asked me, to create a ticket on Jira.  Broken Link in Tomcat because of Page Mount  Following situation: -I have a Wicket Application(6.8.0) which runs under the context "webapp" on a Tomcat 7.0.41 -I mount a Page with two parameters (this is important) in the WicketApplication. 	mountPage("/mount/ {parameter1}/ {parameter2}", MountedPage.class); -The mounted Page(MountedPage.class) has only a simple Link -There are two links on the HomePage to the mounted Page.  They are declared as follows:   	add(new Link<Void>("link") { 			@Override 			public void onClick() { 				setResponsePage(MountedPage.class, linkParameters); 			} 	});  	add(new Link<Void>("brokenLink") { 			@Override 			public void onClick() { 				setResponsePage(new MountedPage(linkParameters)); 			} 	}); 	 I deploy this Application as a war file on a Tomcat under the context "webapp". When I call the first Link on the HomePage and then the Link on the mounted Page, everything works fine.  But if I call the second Link and then the Link on the mounted Page, the link is broken. The context is missing in the generated link 	http://localhost:8080/wicket/bookmarkable/com.mycompany.LinkedPage  Does anyone have an idea, why the second link does not work on Tomcat?  I add a Quickstart and the war file as attachment.  Ps: Both links works fine in Jetty.  Pss:If I remove the mount command, both links will work in  Tomcat too.
WICKET-6122df49$$Minified css/js gets compressed$$Given an application with a resource reference to a minified script, i.e. html5.js and html5.min.js.  When the ResourceRequestHandler responds  it will set compress to false, if the resource reference was PackageResourceReference but it will not change compression if the resource reference was JavaScriptResourceReference.   PackageResourceReference handles minified resources more or less correctly (if they are minified, they should not be further compressed), but this behavior is overwritten in its subclasses.
WICKET-3d2393c7$$Minified name resolves incorrectly if default resource reference is used$$In PackageResourceReference.  When a default reference to a minified resource is used (i.e. the resource wasn't mounted) the resource reference name includes '.min'.   When trying to resolve the minified name, another '.min' is appended, resulting in the minified name resolving to 'html5.min.min.js'.   As a result, the PackageResourceReference concludes that the resource was not minified, and adds compression.
WICKET-6ce34ccf$$Minified name resolves incorrectly if default resource reference is used$$In PackageResourceReference.  When a default reference to a minified resource is used (i.e. the resource wasn't mounted) the resource reference name includes '.min'.   When trying to resolve the minified name, another '.min' is appended, resulting in the minified name resolving to 'html5.min.min.js'.   As a result, the PackageResourceReference concludes that the resource was not minified, and adds compression.
WICKET-a9e56e1e$$Url can't parse urls with username and password$$Url tries to parse the password as the portnumber, because it's after the :, resulting in the following exception: java.lang.NumberFormatException: For input string: "23dc429c-4ffa-4e99-8e24-984571f4c3b6@digdag-rest-dev2.topicusonderwijs.nl" 	java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) 	java.lang.Integer.parseInt(Integer.java:492) 	java.lang.Integer.parseInt(Integer.java:527) 	org.apache.wicket.request.Url.parse(Url.java:276) 	org.apache.wicket.request.Url.parse(Url.java:192) 	org.apache.wicket.protocol.http.servlet.ServletWebResponse.encodeRedirectURL(ServletWebResponse.java:212) 	org.apache.wicket.protocol.http.servlet.ServletWebResponse.sendRedirect(ServletWebResponse.java:236) 	org.apache.wicket.protocol.http.BufferedWebResponse SendRedirectAction.invoke(BufferedWebResponse.java:400) 	org.apache.wicket.protocol.http.BufferedWebResponse.writeTo(BufferedWebResponse.java:588) 	org.apache.wicket.protocol.http.HeaderBufferingWebResponse.stopBuffering(HeaderBufferingWebResponse.java:60) 	org.apache.wicket.protocol.http.HeaderBufferingWebResponse.flush(HeaderBufferingWebResponse.java:97) 	org.apache.wicket.protocol.http.WicketFilter.processRequestCycle(WicketFilter.java:269) 	org.apache.wicket.protocol.http.WicketFilter.processRequest(WicketFilter.java:201) 	org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:282)
WICKET-0eb596df$$FencedFeedbackPanel is broken with RefreshingView(and it's implementations)$$FencedFeedbackPanel doesn't work correctly if inner form(s) are in RefreshingView(or it's implementations).. in this case outerform feedbackpanel just starts including messages meant for inner feedbackpanel. with ListView FencedFeedbackPanel works correctly.. actually one user(Mike Dundee) created this issue in quickview https://github.com/vineetsemwal/quickview/issues/19  so in that link he has described his problem and pasted the code you can use to reproduce ... there i have also explained why it's broken with RefreshingView and it's implementations currently(it's a little complex so i am trying to avoid explaining all again ,also english is not my first language :-) )   thank you !
WICKET-c863b032$$CryptoMapper encrypts external URLs in ResourceReferences making the resources inaccessible$$Short Description:   CryptoMapper encrypts links to resources with URLs of the form:  - http://domain/path/script.js  - /local/absolute/path/script.js  Additionally there might be some inconsistencies in handling URLs in instances of ResourceReference.  The problem occurs when JavaScript resources are included in the following way:  @Override public void renderHead(IHeaderResponse response) { 	super.renderHead(response); 	 	UrlResourceReference reference = new UrlResourceReference(Url.parse("http://domain/path/script.js")); 	response.render(reference); }  The resulting JavaScript links can't be loaded (404 is returned) when CryptoMapper is used.  This is a minor problem, because the following always works for JavaScript files not served by Wicket ("external JavaScript files"):  response.render(new StringHeaderItem("<script type=\"text/javascript\" src=\"//domain/myPath/manual.js\"></script>");   Ways to reproduce:     A code example for wicket-examples is attached (example.zip)   Local URLs:      http://localhost:8080/enc/index      http://localhost:8080/unenc/index   Possible fix:    - disable encryption for URLs beginning with '/', '<schema>://' and '//' and not served/filtered by Wicket   (  - define different reference classes for external files and files served/filtered by Wicket, issue warnings when a wrong URL type is supplied by the user or treat URLs beginning with '/', '<schema>://' and '//' differently  )  Thank you
WICKET-ded3c583$$Wicket doesn't encrypt links and Ajax URLs for mounted pages when CryptoMapper is used$$URL encryption does not work in Wicket links and Ajax URLs.  For links the URL appears unencrypted in the href attribute value and is only later forwarded to the encrypted URL using a 302 response.  I am uploading a quickstart.
WICKET-3fc7234e$$Url.canonical() breaks when there are two consecutive "parent" segments followed by a normal segment$$assertEquals("a/d", Url.parse("a/b/c/../../d").canonical().getPath());   breaks with : Expected :a/d Actual   :a/b/../d
WICKET-61122bab$$org.apache.wicket.util.string.StringValue#equals broken$$The #equals implementation for org.apache.wicket.util.string.StringValue is broken. The following throws an exception instead of just printing 'false':  StringValue val = StringValue.valueOf("bla", Locale.FRANCE); StringValue val2 = StringValue.valueOf("bla", Locale.CANADA); System.out.println(val.equals(val2));   This part of #equals Objects.isEqual(locale, stringValue.locale)  should probably be replaced with something like (locale == stringValue.locale || (locale != null && locale.equals(stringValue.locale))  -> Objects.isEqual is not suitable to determine equality of Locale
WICKET-19e7c1cd$$XmlPullParser fails to properly parse from String with encoding declaration$$When parsing from a string, XmlPullParser fails if the encoding from the XML declaration is different than the system's file encoding.  Examples:    -Dfile.encoding=ISO-8859-1    parser.parse("<?xml encoding='UTF-8' ?><span id='umlaut-äöü'></span>");     -Dfile.encoding=UTF-8    parser.parse("<?xml encoding='ISO-8859-1' ?><span id='umlaut-äöü'></span>");  Both fail because the string is read with the system's file encoding while the parser expects the stream to be encoded in the declarated encoding.
WICKET-6cefb9f8$$Behaviors#internalAdd(Behavior) erroneously gets id for stateless behaviors$$see http://markmail.org/thread/jtd4zn527r343jbm
WICKET-87fa630f$$BOM in UTF markup file breaks encoding detection$$I have project with internationalization and experienced this problem with one of the pages with non-english content. Page had UTF-8 encoding, but my JVM encoding is different. I always use "<?xml encoding ... ?>" to specify encoding for markup pages (and "MarkupSettings.defaultMarkupEncoding" is not set).  Unexpectedly I got problem with bad encoding on page. After several hours of debugging I found what source of this issue was UTF BOM (Byte order mark) at the beggining of file and inability of "XmlReader" to process it. "XmlReader.getXmlDeclaration" tries to match xml declaration with regular expression, but fails because of BOM. After that encoding defaults to JVM encoding.  It's possible to use "org.apache.commons.io.input.BOMInputStream" to handle BOM or you could handle it manually inside "XmlReader".  PS: issue found with Wicket 1.5.10 and I see same code in 6.12.0 without BOM handling, so I added it to "Affects Version/s", but no proof-in-code available from me at this moment.
WICKET-e350f19e$$PropertyValidator ignoring groups with the @NotNull annotation only$$When using groups in your JSR303 compliant classes, Wicket does not honor the groups for the @NotNull annotation.
WICKET-fb45a781$$Page not recognized as stateless although stateful component is hidden in #onConfigure()$$Page#stateless gets cached. If Page#isStateless() is called before rendering, a page might not be considered stateless although in #onConfigure() all stateful components are hidden.
WICKET-8ccb1f6d$$IResourceCachingStrategy implementations should only set caching if version matches$$Implementations of IResourceCachingStrategy (FilenameWithVersionResourceCachingStrategy and QueryStringWithVersionResourceCachingStrategy) should only set cache duration to maximum if the version matches. Currently, if a user requests a resource with an arbitrary version, the version will be cached for one year (WebResponse.MAX_CACHE_DURATION). So people could polute proxy caches with potentially upcoming version.
WICKET-a382917f$$TimeOfDay.valueOf(Calendar, Time) and TimeOfDay.valueOf(Time) incorrectly use 12-hour clock$$TimeOfDay.valueOf(Calendar, Time) is implemented as:     return militaryTime(time.getHour(calendar), time.getMinute(calendar), time.getSecond(calendar));  This is flawed because Time.getHour() is implemented as:     return get(calendar, Calendar.HOUR); and Calendar.HOUR is for the 12-hour clock. The result is that TimeOfDay.valueOf(Calendar, Time) incorrectly only returns 12-hour results, not 24-hour results. This affects TimeOfDay.valueOf(Time) as well since it is implemented in terms of the previously-named method.  One fix would be to change Time.getHour() to use Calendar.HOUR_OF_DAY. Since Time doesn't have an am/pm indicator this seems reasonable. An alternate, more localized fix would be to change TimeOfDay.valueOf(Calendar, Time) to call time.get(Calendar.HOUR_OF_DAY) to get the hour value.
WICKET-2abc18f1$$TableTree's NodeBorder does not properly close divs$$NodeBorder fails to properly close generated <div>s.
WICKET-a3a5a40f$$onBeforeRender called too early on stateless page$$I'm having a problem with a ListView that displays an outdated list. In my test, the ListView uses a Model that returns a static variable just to make sure the model is independent from any page instance. As far as I can tell, this problem has nothing to do with the model, but with the way Wicket prepares for a request listener invocation.  The exact setup is this: - the page contains a ListView and (outside of the list) a Link that adds an item to the list in its onClick(). The list itself is stored in a static variable. - the page is stateless - the page's components are created in onInitialize()  Result: The list doesn't show the most recently added item. Reloading the original page shows the correct list. Note that by "reloading" I mean entering the page's original URL since the browser's address bar now contains the request listener URL due to the page being stateless.  This is how I think is happens: - Initially rendering the page works fine. The page is then discarded since it's stateless. - Clicking on the link creates a new page instance to invoke the link's request listener. - IPageAndComponentProvider.getComponent() cannot find the link yet since it is not created until onInitialize() has been called. - as a consequence, it calls page.internalInitialize() and internalPrepareForRender(false) - this creates the link, but it also creates the ListView and prepares it for rendering. This in turn polls the ListView's model and creates list items. It also marks the ListView as "prepared for render", which is the crucial point. - The link's request listener runs and adds an item to the list. - After the request listener handler, the page render handler runs - That handler renders the page, including the ListView - ... but it doesn't call onBeforeRender() on the ListView anymore, because it's already marked as "prepared for render"! So it doesn't pick up the new, up-to-date list from its model.  I'm not sure if I'm "doing it wrong", but then it doesn't seem quite right that onBeforeRender() gets called before invoking the listener, but not actually before rendering. There's probably some kind of logic behind the decision to run onBeforeRender() only when this hasn't yet happened, right? Is there a general way to "unprepare" the component in onClick()?  --- Re: #internalPrepareForRender(false) should not mark the page as rendered (thus the false parameter).  The problem is, I think, not that the component is being marked as *rendered*, but as *prepared for render*. From class Component:  protected void onBeforeRender() {   setFlag(FLAG_PREPARED_FOR_RENDER, true);   onBeforeRenderChildren();   setRequestFlag(RFLAG_BEFORE_RENDER_SUPER_CALL_VERIFIED, true); }  Note the first line. This causes subsequent invocations of internalBeforeRender() to skip the relevant part.
WICKET-c1c1f794$$ListenerInterfaceRequestHandler#respond throws ComponentNotFoundException as a side-effect$$The following exception occurs instead of a generic WicketRuntimeException:  16:27:56.181 WARN  (RequestCycle.java:343) Handling the following exception [qtp9826071-207] org.apache.wicket.core.request.handler.ComponentNotFoundException: Could not find component 'xyz' on page 'class MyPage’        at org.apache.wicket.core.request.handler.PageAndComponentProvider.getComponent(PageAndComponentProvider.java:182) ~[org.apache.wicket.core_6.12.0.jar:6.12.0]        at org.apache.wicket.core.request.handler.ListenerInterfaceRequestHandler.getComponent(ListenerInterfaceRequestHandler.java:90) ~[org.apache.wicket.core_6.12.0.jar:6.12.0]        at org.apache.wicket.core.request.handler.ListenerInterfaceRequestHandler.respond(ListenerInterfaceRequestHandler.java:231) ~[org.apache.wicket.core_6.12.0.jar:6.12.0]        at org.apache.wicket.request.cycle.RequestCycle HandlerExecutor.respond(RequestCycle.java:861) ~[org.apache.wicket.core_6.12.0.jar:6.12.0]        at org.apache.wicket.request.RequestHandlerStack.execute(RequestHandlerStack.java:64) ~[org.apache.wicket.request_6.12.0.jar:6.12.0]        at org.apache.wicket.request.cycle.RequestCycle.execute(RequestCycle.java:261) [org.apache.wicket.core_6.12.0.jar:6.12.0]        at org.apache.wicket.request.cycle.RequestCycle.processRequest(RequestCycle.java:218) [org.apache.wicket.core_6.12.0.jar:6.12.0]        at org.apache.wicket.request.cycle.RequestCycle.processRequestAndDetach(RequestCycle.java:289) [org.apache.wicket.core_6.12.0.jar:6.12.0]        at org.apache.wicket.protocol.http.WicketFilter.processRequestCycle(WicketFilter.java:259) [org.apache.wicket.core_6.12.0.jar:6.12.0]  in fact, this is a side effect, if you look at the code:         @Override        public void respond(final IRequestCycle requestCycle)        {              final IRequestablePage page = getPage();              final boolean freshPage = pageComponentProvider.isPageInstanceFresh();              final boolean isAjax = ((WebRequest)requestCycle.getRequest()).isAjax();              IRequestableComponent component = null;              try              {                     component = getComponent();              }              catch (ComponentNotFoundException e)              {                     // either the page is stateless and the component we are looking for is not added in the                     // constructor                     // or the page is stateful+stale and a new instances was created by pageprovider                     // we denote this by setting component to null                     component = null;              }              if ((component == null && freshPage) ||                      (component != null && getComponent().getPage() == page))               {              [....]              }               else               {                      throw new WicketRuntimeException("Component " + getComponent() +                            " has been removed from page.");               }         }  You see that getComponent() is called twice.  1) Once guarded by a catch   - and - 2) once unguarded  So if the component can't be found AND freshPage is false, as a sideeffect instead of the WicketRuntimeException with the removed message a componentnotfound exception is raised as a side effect.  I see two possible solutions for this  a) either it is intentional that a ComponentNotFoundException is thrown, then it should be thrown from the catch block like               catch (ComponentNotFoundException e)              {                     if (!freshPage) {                        throw e;                     }              }  b) if it is unintentionall in the else case ther should be a simple check like this   if (component == null) {                         throw new WicketRuntimeException("Component for path " + getPath() +                           " and page "+page.getClass().getName()+" has been removed from page.");                     } else {                        throw new WicketRuntimeException("Component " + component +                           " has been removed from page.");                     }   Beside this: it would be a good idea to mention at least the page class in either case.
WICKET-813d8bee$$Ajax behavior on component with setRenderBodyOnly(true) don't get called - improve warning$$When you put AJAX behavior on component with setRenderBodyOnly(true) and try to call it with callback script it won't get called and no error / warning is displayed. See attached quickstart sample. Just unzipp and run with: mvn jetty:run  Navigate browser to http://localhost:8080/ When you try to click on labels AJAX behavior should get called. But it won't. This kind of behavior is correct (i assume). But i think user should be warned that behavior can't be called. I think proper place is somewhere on server side? But I don't know where exactly put the warning.  Now only message is in Wicket Ajax Debug window - "Ajax request stopped because of precondition check". I had to debug wicket javascript to find what precondition check failed. Maybe more detailed message in default precondition check would be useful too?
WICKET-ecdfc124$$WebPageRenderer must not render full page in Ajax requests$$WebPageRenderer renders the full page when WebRequest#shouldPreserveClientUrl() is true or RedirectStrategy.NEVER_REDIRECT is configured.  For Ajax request this means that wicket-ajax-js will not be able to parse the HTML response.
WICKET-a79ed51e$$WebPageRenderer should honor RedirectPolicy.ALWAYS_REDIRECT more consistently$$In WebPageRenderer shouldPreserveClientUrl() currently has precedence over RedirectPolicy.ALWAYS_REDIRECT.  This can lead to confusion or unexpected behavior when RedirectPolicy.ALWAYS_REDIRECT is explicitely set, but for some reason shouldPreserveClientUrl() returns true and thus no redirect is performed due to the logic in WebPageRenderer.  A fix for this particular problem could be implemented in  WebPageRenderer as of Wicket 6.12.0 by changing line 211 to:                  || (shouldPreserveClientUrl && getRedirectPolicy() != RedirectPolicy.ALWAYS_REDIRECT)) //   Note that this problem is slightly related to WICKET-5484. Both fixes combined the line could look like this:                  || (shouldPreserveClientUrl && !isAjax && getRedirectPolicy() != RedirectPolicy.ALWAYS_REDIRECT)) //
WICKET-724066f4$$NPE in JsonUtils when the value is null$$Most part of org.apache.wicket.ajax.json.JsonUtils.asArray(Map<String, Object> map) is trying carefully avoid null value. But there is following line  else if (value.getClass().isArray())  which cause NPE in case of empty value for some key.   P.S. Will provide patch.
WICKET-825da305$$Ignore the path parameters when reading the page class$$http://localhost:8080/linkomatic/wicket/bookmarkable/org.apache.wicket.examples.linkomatic.Page3;myjsessionid=123456 leads to :  WARN  - AbstractRepeater           - Child component of repeater org.apache.wicket.markup.repeater.RepeatingView:area has a non-safe child id of page1. Safe child ids must be composed of digits only. WARN  - WicketObjects              - Could not resolve class [org.apache.wicket.examples.linkomatic.Page3;blass=koko] java.lang.ClassNotFoundException: org/apache/wicket/examples/linkomatic/Page3;blass=koko 	at java.lang.Class.forName0(Native Method) 	at java.lang.Class.forName(Class.java:264) 	at org.apache.wicket.application.AbstractClassResolver.resolveClass(AbstractClassResolver.java:108) 	at org.apache.wicket.core.util.lang.WicketObjects.resolveClass(WicketObjects.java:72) 	at org.apache.wicket.core.request.mapper.AbstractComponentMapper.getPageClass(AbstractComponentMapper.java:139) 	at org.apache.wicket.core.request.mapper.BookmarkableMapper.parseRequest(BookmarkableMapper.java:118) 	at org.apache.wicket.core.request.mapper.AbstractBookmarkableMapper.mapRequest(AbstractBookmarkableMapper.java:292) 	at org.apache.wicket.request.mapper.CompoundRequestMapper.mapRequest(CompoundRequestMapper.java:152) 	at org.apache.wicket.request.cycle.RequestCycle.resolveRequestHandler(RequestCycle.java:190) ...  Such request at the moment works only if the the path parameter name is 'jsessionid'
WICKET-6cceff44$$DefaultPropertyResolver does not respect JavaBean conventions$$The property name code should handle the isPropertyName pattern  if(getterName.startsWith("get")) { 	name = getterName.substring(3, 4).toLowerCase() + getterName.substring(4); } else { 	name = getterName.substring(2, 3).toLowerCase() + getterName.substring(3); }  Workaround: providing my own property resolver.
WICKET-c2e12216$$FormComponent.updateCollectionModel  does not handle unmodifiableList$$FormComponent.updateCollectionModel should handle situation, when getter returns unmodifiable list.  Proposed solution:  			formComponent.modelChanging(); 			booelan isChanged; 			try { 				collection.clear(); 				if (convertedInput != null) 				{ 					collection.addAll(convertedInput); 				} 				isChanged = true; 			catch (Exception e) 			{ 				// ignore this exception as Unmodifiable list does not allow change 				 				logger.info("An error occurred while trying to modify list attached to " + formComponent, e); 			}  			try 			{ 				if(isChanged)				 					formComponent.getModel().setObject(collection); 				else  					// TODO: create here collection as non-abstract successor of setObject declared argument 					formComponent.getModel().setObject(new ArrayList(convertedInput)); 				isChanged = true; 			} 			catch (Exception e) 			{ 				// ignore this exception because it could be that there 				// is not setter for this collection. 				logger.info("An error occurred while trying to set the new value for the property attached to " + formComponent, e); 			} 			// at least one update method should pass successfully			 			if(isChanged) 				formComponent.modelChanged(); 			else 				throw new RuntimeException("An error occurred while trying to modify value for the property attached to " + formComponent);
WICKET-5b730c0b$$Failing HTTPS redirect to RequireHttps annotated pages with ONE_PASS_RENDER strategy$$Activated JS: Start the quickstart -> Press the submit buttons -> See the secured page with https!  Deactivates JS: (NoScript Firefox Plugin): Start the quickstart -> Press the submit buttons -> See the secured page BUT with HTTP!  There was no proper https redirect.  If I change the rendering strategy to REDIRECT_TO_BUFFER everything works fine, but if I change the strategy to ONE_PASS_RENDER the https forwarding does't work anymore. But only if I deactivate all scripts...  Regards, Dmitriy
WICKET-f1af9e03$$Adding behavior in component instantiation listener causes Page.onInitialize() being called even if constructor throws an exception$$Page.onInitialize() will be called even if constructor throws an exception in case below code is added in wicket WebApplication.init(): getComponentInstantiationListeners().add(new IComponentInstantiationListener() {               @Override               public void onInstantiation(Component component) {                   component.add(new Behavior() {                    });               }                         });  It seems that the instantiation listener adds the behavior to the page at very start of the page constructor, and then the page is marked as dirty to cause onInitialize() being called afterwards.
WICKET-aa82ccfc$$A 404 error occurs when using a CryptoMapper$$Under certain prerequisites a 404 error occurs.  The prerequisites are: - A _CryptoMapper_ is used as _RequestMapper_ - _SecuritySettings.enforceMounts_ is set to true - Class _SomePage_ is *not* annotated with _@MountPath_  Reason: In _BookmarkableMapper.parseRequest_ (called indirectly by _CryptoMapper.mapRequest_) the method _matches_ returns _false_, as _reverseUrl_ is the *encrypted URL* (_rootRequestMapper_ is a _CryptoMapper_) but _BookmarkableMapper.matches_ expects a *decrypted URL*.  _BookmarkableMapper_ - lines 132 ff.: {code} Url reverseUrl = application.getRootRequestMapper().mapHandler( 	new RenderPageRequestHandler(new PageProvider(pageClass))); if (!matches(request.cloneWithUrl(reverseUrl))) { 	return null; } {code} 	 As a result _BookmarkableMapper.mapRequest_ and hence _CryptoMapper.mapRequest_ returns _null_ resulting in a 404 error.
WICKET-204849bc$$PackageMapper - Could not resolve class$$It seems that the PackageMapper try to resolve much more than it is supposed to do, for instance if I've 2 pages test1/TestPage1 and test2/TestPage2 then it tries to resolve test2/TestPage1 when I reach the page1...   WARN  - WicketObjects              - Could not resolve class [com.mycompany.test2.TestPage1] java.lang.ClassNotFoundException: com.mycompany.test2.TestPage1     at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1714)     at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1559)     at java.lang.Class.forName0(Native Method)     at java.lang.Class.forName(Class.java:270)     at org.apache.wicket.application.AbstractClassResolver.resolveClass(AbstractClassResolver.java:108)     at org.apache.wicket.core.util.lang.WicketObjects.resolveClass(WicketObjects.java:71)     at org.apache.wicket.core.request.mapper.AbstractComponentMapper.getPageClass(AbstractComponentMapper.java:134)     at org.apache.wicket.core.request.mapper.PackageMapper.parseRequest(PackageMapper.java:152)     at org.apache.wicket.core.request.mapper.AbstractBookmarkableMapper.mapRequest(AbstractBookmarkableMapper.java:322)     at org.apache.wicket.request.mapper.CompoundRequestMapper.mapRequest(CompoundRequestMapper.java:152)     at org.apache.wicket.request.cycle.RequestCycle.resolveRequestHandler(RequestCycle.java:189)     at org.apache.wicket.request.cycle.RequestCycle.processRequest(RequestCycle.java:219)     at org.apache.wicket.request.cycle.RequestCycle.processRequestAndDetach(RequestCycle.java:293)     at org.apache.wicket.protocol.http.WicketFilter.processRequestCycle(WicketFilter.java:261)     at org.apache.wicket.protocol.http.WicketFilter.processRequest(WicketFilter.java:203)     at org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:284)
WICKET-44f4782a$$PackageMapper - Could not resolve class$$It seems that the PackageMapper try to resolve much more than it is supposed to do, for instance if I've 2 pages test1/TestPage1 and test2/TestPage2 then it tries to resolve test2/TestPage1 when I reach the page1...   WARN  - WicketObjects              - Could not resolve class [com.mycompany.test2.TestPage1] java.lang.ClassNotFoundException: com.mycompany.test2.TestPage1     at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1714)     at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1559)     at java.lang.Class.forName0(Native Method)     at java.lang.Class.forName(Class.java:270)     at org.apache.wicket.application.AbstractClassResolver.resolveClass(AbstractClassResolver.java:108)     at org.apache.wicket.core.util.lang.WicketObjects.resolveClass(WicketObjects.java:71)     at org.apache.wicket.core.request.mapper.AbstractComponentMapper.getPageClass(AbstractComponentMapper.java:134)     at org.apache.wicket.core.request.mapper.PackageMapper.parseRequest(PackageMapper.java:152)     at org.apache.wicket.core.request.mapper.AbstractBookmarkableMapper.mapRequest(AbstractBookmarkableMapper.java:322)     at org.apache.wicket.request.mapper.CompoundRequestMapper.mapRequest(CompoundRequestMapper.java:152)     at org.apache.wicket.request.cycle.RequestCycle.resolveRequestHandler(RequestCycle.java:189)     at org.apache.wicket.request.cycle.RequestCycle.processRequest(RequestCycle.java:219)     at org.apache.wicket.request.cycle.RequestCycle.processRequestAndDetach(RequestCycle.java:293)     at org.apache.wicket.protocol.http.WicketFilter.processRequestCycle(WicketFilter.java:261)     at org.apache.wicket.protocol.http.WicketFilter.processRequest(WicketFilter.java:203)     at org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:284)
WICKET-5efb8091$$Unable to find markup for children of deeply nested IComponentResolvers during Ajax response$$Component hierarchy: Page -> WebMarkupContainer -> IComponentResolver (that uses Page to resolve) and Page -> Panel.  Markup hierarchy: Page -> WebMarkupContainer -> IComponentResolver -> Panel.  When rendering whole page, it works, because it is markup driven. Wicket encounters ComponentTag for Panel and resolves the Panel using IComponentResolver, which retrieves the Panel from the Page.  When you add the Panel to an AjaxRequestTarget, the render is component driven. In order to render the Panel, we must retrieve the markup for the Panel from its parent MarkupContainer, which happens to be the Page.  Markup.java around line 230 skips to closing tags of ComponentTag, so when Page gets to the opening tag of the WebMarkupContainer, it skips to the closing tag of the WebMarkupContainer, and so passes over the ComponentTag for Panel without noticing it. There is actually another check, in DefaultMarkupSourcingStrategy, that tries to fetch from all the "transparent" components in the markup container, but this is not good enough, because in our example, the IComponentResolver is not actually a direct child of the Panel's parent, to it is never used to try find the markup.  One solution might be to traverse the tree, and attempt to find the markup from all IComponentResolving MarkupContainers, but we should be careful. I'm a bit concerned at how various parts of Wicket just assume that an IComponentResolver is transparent and resolves from its direct parent only.  If we do go down the route of traversing the tree to find IComponentResolvers, then try find the markup from each of them, we really should add a check in AbstractMarkupSourcingStrategy#searchMarkupInTransparentResolvers() to check that the Component that the IComponentResolver resolves for the markup id is the same component for which we are looking for markup.  This is a difficult one. I am working around it for the mean time, just recording the difficulty here. Will try make a patch when I can.
WICKET-57d8f051$$Rescheduling the same ajax timer behavior causes memory leak in the browser$$AbstractAjaxTimerBehavior uses JavaScript setTimeout() function to do its job. It has a #stop() method that clears the timeout but if the timeout is re-scheduled without being cleared a memory leak is observed in the browser.
WICKET-cd414fa5$$Dequeueing problem when there is TransparentWebMarkupContainer around <wicket:child/>$$While testing 7.0.0-M1 release I've found an issue with wicket-bootstrap's sample application.  Here is a minified version of it that reproduces the problem. The two important things are: - a TransparentWebMarkupContainer (TWMC) is attached to <html> tag in the base page - the sub page is requested  It appears that dequeueing logic cannot find the closing tag of the TWMC and thinks that </wicket:child> is the closing tag.
WICKET-5cdc1c8d$$Stateless/Statefull pages - incorrect behaviour$$Please advise how to do in following situation or confirm that's a bug and should be fixed.  There is a page (login page) with stateless form. That page has lots of common components (menu and etc.). There are some stateful components in the components tree that are visible only for signed in users: but once user isn't signed in - that components are hidden. That's why page is becoming "stateless" (no visible components) and form prepared correspondingly. But when form data is submitted: during obtaining of form component to process request - wicket thinks that page actually is stateful. As a result - the page is recreated and fully rendered - instead of processing of the form.  There is a workaround: setStatelessHint(false). But imho reason is a little bit another: 1) After construction of page: page is stateful - because of some stateful components are in the tree. 2) After initialization of page: page is still stateful - because there are that stateful components 3) After configuration of page (method onConfigure) - page is becoming stateless - because all stateful components marked as invisible. 4) Form has been rendered as stateless - with no version number is in the URL. 5) Page can'be reconstructed correctly because of p.1 and p.2  I think that stateless flag should be precalculated right after initialization step and should be changed due to some stuff in "configuration" methods.  What do you think?  Will provide "quick start" in near future!
WICKET-1fb66533$$ServletWebResponse#encodeUrl() makes absolute Urls relative$$When an absolute (full) URL is passed to ServletWebResponse#encodeUrl(), it will be returned relative if the container encodes a session ID.
WICKET-240ab3c3$$OnChangeAjaxBehavior attached to DropDownChoice produces two Ajax requests in Chrome v35$$I have a DropDownChoice with attached OnChangeAjaxBehavior, like this: {code:borderStyle=solid} new DropDownChoice<>("dd", new Model<>(), Arrays.asList( "First", "Second"))     .add( new OnChangeAjaxBehavior() {         @Override         protected void onUpdate(AjaxRequestTarget target) {             System.out.println( "update" );         } }); {code}  When selecting any of drop down options, two Ajax requests being generated. It behaves OK in IE, FF and Chrome v34, only Chrome v35 is affected
WICKET-d558004b$$Problem with setting of IComponentInheritedModel and FLAG_INHERITABLE_MODEL$$Described in the mailing list: [http://mail-archives.apache.org/mod_mbox/wicket-users/201407.mbox/%3CCAF2_608c8TOZjprV8Md15KJpRET6YQdXHe%3DwRzF-y5G_zAXcDg%40mail.gmail.com%3E]  I'm aware of the another issue ([https://issues.apache.org/jira/browse/WICKET-3413]) which dealt with the exact same code - and I believe there was a mistake in the solution that leads to this issue.  Please see the attached quickstart (including a JUnit test) to reproduce the error.
WICKET-f539c18c$$PropertyResolver does not scan for NotNull in annotation tree$$When annotating a field of a bean with e.g. org.hibernate.validator.constraints.NotEmpty, this implies javax.validation.constraints.NotNull, but PropertyValidator only checks for the annotations immediately on the filed not the tree of annotations. As a result Wicket does not mark the field as required in the UI, which it should.  Also PropertyResolver.findNotNullConstraints() is not even protected, so cannot be patched in a simple way.  So as a solution I suggest changing findNotNullConstraints() to be protected and rather be something like findConstraints(filter), or findConstraints(clazz), and then in that method method recursively invoking getComposingConstraints to get all constraints, but collecting only those of interest. Possibly some care needs to be taken to prevent infinite recursion where constraints are composed of each other (if that compiles).
WICKET-9aec4f33$$@SpringBean(name="something", required=false) still throws org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'something' is defined$$Example:  {code} public class TwitterLoginLink extends StatelessLink<Void> { 			 	@SpringBean(name="twitterMgr", required=false) 	private TwitterManager twitterMgr; {code}  still throws:  {code} org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'twitterMgr' is defined at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeanDefinition(DefaultListableBeanFactory.java:641) at org.springframework.beans.factory.support.AbstractBeanFactory.getMergedLocalBeanDefinition(AbstractBeanFactory.java:1159) at org.springframework.beans.factory.support.AbstractBeanFactory.isSingleton(AbstractBeanFactory.java:418) at org.springframework.context.support.AbstractApplicationContext.isSingleton(AbstractApplicationContext.java:1002) at org.apache.wicket.spring.SpringBeanLocator.isSingletonBean(SpringBeanLocator.java) at org.apache.wicket.spring.injection.annot.AnnotProxyFieldValueFactory.getFieldValue(AnnotProxyFieldValueFactory.java:141) at org.apache.wicket.injection.Injector.inject(Injector.java:111) at org.apache.wicket.spring.injection.annot.SpringComponentInjector.inject(SpringComponentInjector.java:124) at org.apache.wicket.spring.injection.annot.SpringComponentInjector.onInstantiation(SpringComponentInjector.java:130) at org.apache.wicket.application.ComponentInstantiationListenerCollection 1.notify(ComponentInstantiationListenerCollection.java:38) at org.apache.wicket.application.ComponentInstantiationListenerCollection 1.notify(ComponentInstantiationListenerCollection.java:34) at org.apache.wicket.util.listener.ListenerCollection.notify(ListenerCollection.java:80) at org.apache.wicket.application.ComponentInstantiationListenerCollection.onInstantiation(ComponentInstantiationListenerCollection.java:33) at org.apache.wicket.Component.<init>(Component.java:686) at org.apache.wicket.MarkupContainer.<init>(MarkupContainer.java:121) at org.apache.wicket.markup.html.WebMarkupContainer.<init>(WebMarkupContainer.java:52) at org.apache.wicket.markup.html.link.AbstractLink.<init>(AbstractLink.java:57) at org.apache.wicket.markup.html.link.AbstractLink.<init>(AbstractLink.java:44) at org.apache.wicket.markup.html.link.Link.<init>(Link.java:105) at org.apache.wicket.markup.html.link.StatelessLink.<init>(StatelessLink.java:42) at org.soluvas.web.login.twitter.TwitterLoginLink.<init>(TwitterLoginLink.java:40) at org.soluvas.web.login.DedicatedLoginPanel FormSignIn.<init>(DedicatedLoginPanel.java:90) at org.soluvas.web.login.DedicatedLoginPanel.onInitialize(DedicatedLoginPanel.java:58) at org.apache.wicket.Component.fireInitialize(Component.java:876) at org.apache.wicket.MarkupContainer 3.component(MarkupContainer.java:967) at org.apache.wicket.MarkupContainer 3.component(MarkupContainer.java:963) at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:192) at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:875) at org.apache.wicket.MarkupContainer.internalInitialize(MarkupContainer.java:962) at org.apache.wicket.Page.isPageStateless(Page.java:463) at org.apache.wicket.core.request.mapper.AbstractBookmarkableMapper.getPageInfo(AbstractBookmarkableMapper.java:447) at org.apache.wicket.core.request.mapper.AbstractBookmarkableMapper.mapHandler(AbstractBookmarkableMapper.java:391) at org.apache.wicket.core.request.mapper.MountedMapper.mapHandler(MountedMapper.java:395) at org.apache.wicket.request.mapper.CompoundRequestMapper.mapHandler(CompoundRequestMapper.java:215) at org.apache.wicket.request.cycle.RequestCycle.mapUrlFor(RequestCycle.java:429) at org.apache.wicket.request.handler.render.WebPageRenderer.respond(WebPageRenderer.java:273) at org.apache.wicket.core.request.handler.RenderPageRequestHandler.respond(RenderPageRequestHandler.java:175) at org.apache.wicket.request.cycle.RequestCycle HandlerExecutor.respond(RequestCycle.java:862) at org.apache.wicket.request.RequestHandlerStack.execute(RequestHandlerStack.java:64) at org.apache.wicket.request.cycle.RequestCycle.execute(RequestCycle.java:261) at org.apache.wicket.request.cycle.RequestCycle.processRequest(RequestCycle.java:218) at org.apache.wicket.request.cycle.RequestCycle.processRequestAndDetach(RequestCycle.java:289) at org.apache.wicket.protocol.http.WicketFilter.processRequestCycle(WicketFilter.java:259) at org.apache.wicket.protocol.http.WicketFilter.processRequest(WicketFilter.java:201) at org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:282) at org.soluvas.web.site.SecuredWicketAtmosphereHandler CustomFilterChain.doFilter(SecuredWicketAtmosphereHandler.java:199) at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:449) at org.apache.shiro.web.servlet.AbstractShiroFilter 1.call(AbstractShiroFilter.java:365) at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90) at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83) at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:383) at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362) at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125) at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:344) at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:261) at org.soluvas.web.site.SecuredWicketAtmosphereHandler CustomFilterChain.doFilter(SecuredWicketAtmosphereHandler.java:199) at org.soluvas.web.site.SecuredWicketAtmosphereHandler CustomFilterChain.invokeFilterChain(SecuredWicketAtmosphereHandler.java:185) at org.soluvas.web.site.SecuredWicketAtmosphereHandler.onRequest(SecuredWicketAtmosphereHandler.java:91) at org.atmosphere.cpr.AsynchronousProcessor.action(AsynchronousProcessor.java:187) at org.atmosphere.cpr.AsynchronousProcessor.suspended(AsynchronousProcessor.java:98) at org.atmosphere.container.Tomcat7CometSupport.service(Tomcat7CometSupport.java:96) at org.atmosphere.cpr.AtmosphereFramework.doCometSupport(AtmosphereFramework.java:1809) at org.atmosphere.cpr.AtmosphereServlet.event(AtmosphereServlet.java:255) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilterEvent(ApplicationFilterChain.java:484) at org.apache.catalina.core.ApplicationFilterChain.doFilterEvent(ApplicationFilterChain.java:377) at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220) at org.apache.catalina.core.StandardContextValve.__invoke(StandardContextValve.java:123) at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java) at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:502) at org.apache.catalina.core.StandardHostValve.__invoke(StandardHostValve.java:171) at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java) at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:99) at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:953) at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:118) at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:408) at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1023) at org.apache.coyote.AbstractProtocol AbstractConnectionHandler.process(AbstractProtocol.java:589) at org.apache.tomcat.util.net.AprEndpoint SocketWithOptionsProcessor.run(AprEndpoint.java:1810) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) {code}  Workaround:  {code}     @Autowire(required=false)     private TwitterManager twitterMgr; {code}
WICKET-8e794fc4$$@Inject should require the bean dependency instead of setting null$$When using {{@SpringBean}}, if the bean cannot be injected then Wicket will throw {{Exception}}.  However current behavior if when using {{@Inject}} inside component, the field will be left as null. This is inconsistent behavior with what CDI spec and how the "real" Spring does it.  Wicket should change its behavior so that {{@Inject}} is always required. If the dependency is optional the user can use {{@SpringBean(required=false)}} as always.
WICKET-2ac29d30$$Nested Redirects and REDIRECT_TO_BUFFER$$When the render strategy is REDIRECT_TO_BUFFER, redirects cannot be nested. After the second redirect, Wicket renders the buffered first page in preference to the second page. The relevant code is in WebPageRenderer.respond:  {noformat} 		if (bufferedResponse != null) 		{ 			logger.warn("The Buffered response should be handled by BufferedResponseRequestHandler"); 			// if there is saved response for this URL render it 			bufferedResponse.writeTo((WebResponse)requestCycle.getResponse()); 		} {noformat}  The attached quickstart demonstrates the issue. Simply navigate to the home page. The observed behavior is that Page1 is displayed, but I expect Page2 to be displayed.  I can work around the issue by calling WebApplication.getAndRemoveBufferedResponse() to clear the render buffer, but I am uneasy with this solution since it seems like I am playing with Wicket internals; albeit the function is public.
WICKET-f45ce896$$WebApplication#unmount() unmounts the whole compound mapper if some of its inner ones matches$$From dev@ mailing lists: http://markmail.org/message/wmdgbrhvrvaeygvr  WebApplication.unmount() calls getRootRequestMapperAsCompound(), and  calls unmount() on that.  getRootRequestMapperAsCompound() checks if the root request mapper is a  compound, if not, wraps it in a compound, sets the compound as root and  returns the compound.  CompoundRequestMapper.unmount() identifies which of the mappers added  directly to the compound handle the url, and removes them.  The problem: If the original root mapper was a single wrapper, or layer of wrappers,  with the actual mounted mapper wrapped some levels down, then the whole  wrapper is removed, not just the specific MountedMapper that is wrapped.  This has the effect of removing every single mapper, leaving root mapper  as an empty compound.  I would like to attempt to provide a patch to fix this, but would like  guidance on the approach. I have come up with three approaches:  1. Introduce interface IWrappedRequestMapper. This will be an interface  which has one method: IRequestMapper getWrappedRequestMapper(). We can  then have all wrapper mappers implement this and work down the tree to  find the correct MountedMapper (wicket 6) to remove.  2. Have WebApplication hold a reference to a specific  CompoundRequestMapper, and have all mount()/unmount() operations add and  remove from this mapper. This compound would need to be added to the  default list during init. This makes it complicated to work out how to  do things like have CryptoMapper not apply to mounted pages.  3. Add method unmount() to IRequestMapper, so that wrappers can  delegate. This obviously can only be done in wicket 7, but we're making  mounting a problem of every single request mapper, when not even  Application cares about mounting.
WICKET-087c0a26$$WebSocketRequestHandler is not set as a scheduled and thus RequestCycle#find(AjaxRequestTarget.class) doesn't work$$As discussed at https://groups.google.com/d/topic/wicket-jquery-ui/fw6TdyO5o18/discussion AbstractWebSocketProcessor doesn't schedules the WebSocketRequestHandler in the request cycle and thus it is not reachable for user code via RequestCycle#find(Class) API.  Additionally the configured application RequestCycle listeners are not notified.
WICKET-0f8a6d75$$ResourceUtils.getLocaleFromFilename cannot handle filenames with classifiers$$When I try to get PathLocale with ResourceUtils, than get wrong values, when the files contains '.' in name.  Example: 'jquery.test.js'  PathLocale.path=jquery, PathLocale.locale = null  or jquery.test_hu.js'. PathLocale.path=jquery, PathLocale.locale = null  That's why I'd like to use  jquery.test_hu.js' as resource, the ResourceStreamLocator try to find  jquery.test_hu_hu_HU.js, jquery.test_hu_hu.js, and after  jquery.test_hu.js. Because the  ResourceStreamLocator.locate  		PathLocale data = ResourceUtils.getLocaleFromFilename(path); 		if ((data != null) && (data.locale != null)) 		{ 			path = data.path; 			locale = data.locale; 		} doesn't work in this case.  Should change the  ResourceUtils  	public static PathLocale getLocaleFromFilename(String path) { 		int pos = path.indexOf('.'); ---------------- To                 int pos = path.lastIndexOf('.');
WICKET-5837817c$$OnChangeAjaxBehavior should listen for both 'inputchange' and 'change' events for TextField and TextArea$$WICKET-5603 introduced a regression that a TextField using OnChangeAjaxBehavior doesn't work anymore when used as date picker, or Select2. The problem is that usually extensions like DatePicker and Select2 will fire 'change' event when they update the text input.  OnChangeAjaxBehavior should use both 'inputchange" and "change" events for TextField and TextArea components.
WICKET-145da021$$SecuritySettings.setEnforceMounts() does not work when the mounted mapper is not in the root compound mapper$$BookmarkableMapper.isPageMounted() assumes that all mounted mappers are in Application.getRootRequestMapperAsCompound(). Sometimes the mappers make a tree structure, with multiple compounds existing, sometimes separated by wrappers, like HttpsMapper and CryptoMapper.  Because of this, BookmarkableMapper fails to realise that a page is mounted and so does not enforce mounting.
WICKET-2fc6a395$$Method Strings.join doesn't work correctly if separator is empty.$$If we use an empty separator ("") to join strings, the first character of any fragment is truncated. Es "foo", "bar", "baz" became "ooaraz".
WICKET-b92591f6$$Queueing component in autocomponent$$There is an exception when a component is added to queue when its parent is an auto component  <body> 		<a href="panier.html"> 			<span wicket:id="inlink"></span> 		</a> 	</body>   Last cause: Unable to find component with id 'inlink' in [TransparentWebMarkupContainer [Component id = wicket_relative_path_prefix_1]] 	Expected: 'wicket_relative_path_prefix_1:inlink'. 	Found with similar names: ''
WICKET-3cc3fe95$$Component queuing breaks with html tags that don't require close tag.$$Component queuing try to skip to close tag also for those tags that don't have one. This leads to a EmptyStackException (see ArrayListStack#peek).
WICKET-71674df5$$Problem with WICKET-4441 and RestartResponseAtInterceptPageException$$WICKET-4441 introduced an issue when our app has an authorization strategy and user is logged out. If user tries to access a protected url/page, RestartResponseAtInterceptPageException is handled by DefaultExceptionMapper and leads to exception page instead of redirecting user.
WICKET-bcea89fc$$NullPointerException in IntHashMap$$I was looking through a tester's log file to track down a separate issue. I came across a {{NullPointerException}} with {{IntHashMap}}, apparently when the server was shutting down.  See also WICKET-5584, which also deals with a {{NullPointerException}} with {{IntHashMap}}, and also seems to relate to a {{null}} {{modCount}} (judging by the line number).  {noformat} INFO  (ExampleServer) [2014-11-06 00:49:24,979] - com.example.server.ExampleServer.stopServer(ExampleServer.java:268): Stopping server. INFO  (ServerConnector) [2014-11-06 00:49:24,982] - org.eclipse.jetty.server.AbstractConnector.doStop(AbstractConnector.java:306): Stopped ServerConnector@3b7d3a38{HTTP/1.1}{0.0.0.0:8099} INFO  (Application) [2014-11-06 00:49:24,983] - org.apache.wicket.Application.destroyInitializers(Application.java:588): [org.apache.wicket.protocol.http.WicketFilter-55b0dcab] destroy: Wicket core library initializer INFO  (Application) [2014-11-06 00:49:24,983] - org.apache.wicket.Application.destroyInitializers(Application.java:588): [org.apache.wicket.protocol.http.WicketFilter-55b0dcab] destroy: Wicket extensions initializer ERROR (DiskDataStore) [2014-11-06 00:49:24,988] - org.apache.wicket.pageStore.DiskDataStore.saveIndex(DiskDataStore.java:282): Couldn't write DiskDataStore index to file C:\Windows\SERVIC~2\NETWOR~1\AppData\Local\Temp\org.apache.wicket.protocol.http.WicketFilter-55b0dcab-filestore\DiskDataStoreIndex. java.lang.NullPointerException 	at org.apache.wicket.util.collections.IntHashMap HashIterator.<init>(IntHashMap.java:777) 	at org.apache.wicket.util.collections.IntHashMap EntryIterator.<init>(IntHashMap.java:871) 	at org.apache.wicket.util.collections.IntHashMap EntryIterator.<init>(IntHashMap.java:871) 	at org.apache.wicket.util.collections.IntHashMap.newEntryIterator(IntHashMap.java:896) 	at org.apache.wicket.util.collections.IntHashMap EntrySet.iterator(IntHashMap.java:1055) 	at org.apache.wicket.util.collections.IntHashMap.writeObject(IntHashMap.java:1128) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:483) 	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988) 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432) 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548) 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509) 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432) 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548) 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509) 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432) 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) 	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348) 	at java.util.HashMap.internalWriteEntries(HashMap.java:1777) 	at java.util.HashMap.writeObject(HashMap.java:1354) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:483) 	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988) 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496) 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432) 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178) 	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348) 	at org.apache.wicket.pageStore.DiskDataStore.saveIndex(DiskDataStore.java:274) 	at org.apache.wicket.pageStore.DiskDataStore.destroy(DiskDataStore.java:106) 	at org.apache.wicket.pageStore.AsynchronousDataStore.destroy(AsynchronousDataStore.java:118) 	at org.apache.wicket.pageStore.AbstractPageStore.destroy(AbstractPageStore.java:53) 	at org.apache.wicket.pageStore.AbstractCachingPageStore.destroy(AbstractCachingPageStore.java:102) 	at org.apache.wicket.page.PageStoreManager.destroy(PageStoreManager.java:437) 	at org.apache.wicket.Application.internalDestroy(Application.java:659) 	at org.apache.wicket.protocol.http.WebApplication.internalDestroy(WebApplication.java:607) 	at org.apache.wicket.protocol.http.WicketFilter.destroy(WicketFilter.java:605) 	at org.eclipse.jetty.servlet.FilterHolder.destroyInstance(FilterHolder.java:173) 	at org.eclipse.jetty.servlet.FilterHolder.doStop(FilterHolder.java:151) 	at org.eclipse.jetty.util.component.AbstractLifeCycle.stop(AbstractLifeCycle.java:89) 	at org.eclipse.jetty.util.component.ContainerLifeCycle.stop(ContainerLifeCycle.java:143) 	at org.eclipse.jetty.util.component.ContainerLifeCycle.doStop(ContainerLifeCycle.java:162) 	at org.eclipse.jetty.server.handler.AbstractHandler.doStop(AbstractHandler.java:73) 	at org.eclipse.jetty.servlet.ServletHandler.doStop(ServletHandler.java:230) 	at org.eclipse.jetty.util.component.AbstractLifeCycle.stop(AbstractLifeCycle.java:89) 	at org.eclipse.jetty.util.component.ContainerLifeCycle.stop(ContainerLifeCycle.java:143) 	at org.eclipse.jetty.util.component.ContainerLifeCycle.doStop(ContainerLifeCycle.java:162) 	at org.eclipse.jetty.server.handler.AbstractHandler.doStop(AbstractHandler.java:73) 	at org.eclipse.jetty.security.SecurityHandler.doStop(SecurityHandler.java:411) 	at org.eclipse.jetty.security.ConstraintSecurityHandler.doStop(ConstraintSecurityHandler.java:457) 	at org.eclipse.jetty.util.component.AbstractLifeCycle.stop(AbstractLifeCycle.java:89) 	at org.eclipse.jetty.util.component.ContainerLifeCycle.stop(ContainerLifeCycle.java:143) 	at org.eclipse.jetty.util.component.ContainerLifeCycle.doStop(ContainerLifeCycle.java:162) 	at org.eclipse.jetty.server.handler.AbstractHandler.doStop(AbstractHandler.java:73) 	at org.eclipse.jetty.server.session.SessionHandler.doStop(SessionHandler.java:127) 	at org.eclipse.jetty.util.component.AbstractLifeCycle.stop(AbstractLifeCycle.java:89) 	at org.eclipse.jetty.util.component.ContainerLifeCycle.stop(ContainerLifeCycle.java:143) 	at org.eclipse.jetty.util.component.ContainerLifeCycle.doStop(ContainerLifeCycle.java:162) 	at org.eclipse.jetty.server.handler.AbstractHandler.doStop(AbstractHandler.java:73) 	at org.eclipse.jetty.server.handler.ContextHandler.doStop(ContextHandler.java:833) 	at org.eclipse.jetty.servlet.ServletContextHandler.doStop(ServletContextHandler.java:215) 	at org.eclipse.jetty.util.component.AbstractLifeCycle.stop(AbstractLifeCycle.java:89) 	at org.eclipse.jetty.util.component.ContainerLifeCycle.stop(ContainerLifeCycle.java:143) 	at org.eclipse.jetty.util.component.ContainerLifeCycle.doStop(ContainerLifeCycle.java:162) 	at org.eclipse.jetty.server.handler.AbstractHandler.doStop(AbstractHandler.java:73) 	at org.eclipse.jetty.server.Server.doStop(Server.java:456) 	at org.eclipse.jetty.util.component.AbstractLifeCycle.stop(AbstractLifeCycle.java:89) 	at com.example.server.ExampleServer.stopServer(ExampleServer.java:269) 	at com.example.server.ExampleServer.stop(ExampleServer.java:279) INFO  (ContextHandler) [2014-11-06 00:49:24,990] - org.eclipse.jetty.server.handler.ContextHandler.doStop(ContextHandler.java:863): Stopped o.e.j.s.ServletContextHandler@63f259c3{/,null,UNAVAILABLE} {noformat}
WICKET-0374c040$$AjaxRequestAttributes extra parameters aren't properly handled in getCallbackFunction()$$extra parameters of an Ajax behaviour can be accessed by getRequest().getRequestParameters().getParameterValue(key)  but if one uses getCallbackFunction() of an AbstractDefaultAjaxBehavior, these parameters get screwed and can no longer be accessed in the same manner.   the problem seems to be the merge in attrs.ep = Wicket.merge(attrs.ep, params);
WICKET-cf6172bd$$PageParametersEncoder should not decode parameters with no name$$From dev@ mailing list: http://markmail.org/message/khuc2v37aakzyfth  PageParametersEncoder should ignore query parameters like "&=&" and "&=value" because they make no sene and lead to exceptions later at PageParameters#add() call.
WICKET-7b8b6767$$Multiple events in AjaxEventBehavior with prefix 'on'$$if multiple events are used and one starts with "on", it only works if it is the first one, because of:  {code} 		if (event.startsWith("on")) 		{ 			event = event.substring(2); 		} {code}  Why are events possible to start with "on" ?   Is this legacy? Perhaps should be removed for Wicket 7 ?
WICKET-b6259e5f$$arraycopy with bad length in AbstractRequestLogger:172$$When clicking on DebugBar org.apache.wicket.devutils.inspector.LiveSessionsPage NullPointerException is thrown. After investigating the reason I think AbstractRequestLogger:172 arraycopy params cause it.  {{arraycopy(requestWindow, 0, copy, requestWindow.length - oldestPos, indexInWindow);}} Should be changed to: {{arraycopy(requestWindow, 0, copy, requestWindow.length - oldestPos, oldestPos);}}
WICKET-b1f4e6a3$$URL IPv6 parsing$$There is an issue with native IPv6 address parsing. "https://[::1]/myapp", URL parsing fails:  org.apache.wicket.request.Url.parse("https://[::1]/myapp") generates an exception: java.lang.NumberFormatException: For input string: "1]" at java.lang.NumberFormatException.forInputString( NumberFormatException.java:65) at java.lang.Integer.parseInt(Integer.java:492)  However, "https://[::1]:80/myapp" works as expected.
WICKET-e93fdd5a$$Last-modified header of external markup is ignored$$When using external base markup(in my case a drupal page with a wicket:child element in it) this markup is supposed to be cached after first fetch. For subsequent requests the last-modified header is checked to see if the markup has changed and when it has the markup is fetched again.  This does not work, Connections.getLastModified(URL url) always returns 0 when the URL is a http url(in fact, when url.openConnection returns a sun.net.www.protocol.http.HttpURLConnection.  Solution could be to not setDoInput to false on this URLConnection(tested that)
WICKET-b80f6640$$LongConverter converts some values greater than Long.MAX_VALUE$$Currently it's possible to submit some values via Long Textfield<Long> that are greater than Long.MAX_VALUE. This will produce converted input and model update with value of Long.MAX_VALUE  I'm not sure what the behavior should be - imho throwing ConversionException seems fair as the input isn't a valid Long.  The reason seems to be precision loss during Double.valueOf(input) execution while converting, and then comparing to Long.MAX_VALUE  using Long.doubleValue() in *AbstractNumberConverter*, which by casting leads to to the same precision loss and the numbers are seemingly equal during comparison of ranges.  Maybe using BigDecimals for parsing could help here.  The quickstart is available at [https://github.com/zeratul021/wicket-number-conversion].  For the fastest demonstration I extended Wicket's _longConversion()_ test-case in *ConvertersTest*: [https://github.com/zeratul021/wicket-number-conversion/blob/master/src/test/java/com/github/zeratul021/wicketnumberconversion/ConvertersTest.java#L300]
WICKET-8c83c5c5$$NPE in FormComponent#updateCollectionModel in case of no converted input and unmodifiable collection$$There are 2 issues with FormComponent#updateCollectionModel. 1) converted input is not checked for null before wrapping it to ArrayList 2) converted input is not checked for null then model returns unmodifiable collection. The both issues causes NPE.
WICKET-cd3b9234$$Feedback messages not cleared for invisible/disabled form components on submit.$$Having:  - IFeedbackMessageFilter.NONE used as the default application's feedback message cleanup filter (in order to make feedback messages not to disappear after page refresh, i.e. persistent) - form with validatable component whose enabled/visible state may be dynamically changed by user (e.g. checkbox "send me email" and text field "email")  First, user tries to submit form having invalid value - as the result validation error occurs.  Then user makes that component invisible and retries form submitting - as the result no validation errors, but form wasn't submitted.  This happens because that component still has error feedback message got from first submit. Note that when using default application's feedback message cleanup filter, form is successfully submitted.  Probably, feedback messages should be cleared for invisible/disabled form components on submit, as it done for visible/enabled components in FormComponent.validate()
WICKET-2d9ebf9a$$Parsing of ChinUnionPay credit card should use the first 6 characters$$User report:  A China UnionPay number has to start with 622 (622126-622925) and has to have a length between 16 and 19. The source code of CreditCardValidator is:    220   	private boolean isChinaUnionPay(String creditCardNumber)   221   	{   222   		cardId = CreditCardValidator.INVALID;   223   		boolean returnValue = false;   224      225   		if ((creditCardNumber.length() >= 16 && creditCardNumber.length() <= 19) &&   226   			(creditCardNumber.startsWith("622")))   227   		{   228   			int firstDigits = Integer.parseInt(creditCardNumber.substring(0, 5));   229   			if (firstDigits >= 622126 && firstDigits <= 622925)   230   			{   231   				cardId = CreditCardValidator.CHINA_UNIONPAY;   232   				returnValue = true;   233   			}   234   		}   235      236   		return returnValue;   237   	} The problem is on the line 228 because the substring returns the first 5 digits and it is compared to 6 digits, so "firstDigits" is always < than 622126. The fix is to do #substring(0, 6).
WICKET-b00920f3$$StackOverflowError after form submit with a validation error$$I was not able to find a cause or make a small quickstart, but it has something to do with a form validation, my workaround was to setDefaultFormProcessing(false) or not use required TextFields.  It can be reproduced on https://github.com/krasa/DevSupportApp/tree/stackOverflow  1) run StartVojtitkoDummy 2) go to http://localhost:8765/wicket/bookmarkable/krasa.release.frontend.TokenizationPage 3) click on "Generate Release json" button  - instead of SOE, it should give a validation error, probably even on fields which I would not want to validate, but that's just because I've made the page badly...     {code} java.lang.StackOverflowError: null ... 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:162) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:162) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:162) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.Component.internalRender(Component.java:2344) 	at org.apache.wicket.Component.render(Component.java:2307) 	at org.apache.wicket.ajax.XmlAjaxResponse.writeComponent(XmlAjaxResponse.java:128) 	at org.apache.wicket.ajax.AbstractAjaxResponse.writeComponents(AbstractAjaxResponse.java:218) 	at org.apache.wicket.ajax.AbstractAjaxResponse.writeTo(AbstractAjaxResponse.java:150) 	at org.apache.wicket.ajax.AjaxRequestHandler.respond(AjaxRequestHandler.java:359) 	at org.apache.wicket.request.cycle.RequestCycle HandlerExecutor.respond(RequestCycle.java:865) 	at org.apache.wicket.request.RequestHandlerStack.execute(RequestHandlerStack.java:64) 	at org.apache.wicket.request.RequestHandlerStack.execute(RequestHandlerStack.java:97) 	at org.apache.wicket.request.cycle.RequestCycle.execute(RequestCycle.java:265) 	at org.apache.wicket.request.cycle.RequestCycle.processRequest(RequestCycle.java:222) 	at org.apache.wicket.request.cycle.RequestCycle.processRequestAndDetach(RequestCycle.java:293) 	at org.apache.wicket.protocol.ws.AbstractUpgradeFilter.processRequestCycle(AbstractUpgradeFilter.java:59) 	at org.apache.wicket.protocol.http.WicketFilter.processRequest(WicketFilter.java:203) 	at org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:284) 	at org.eclipse.jetty.servlet.ServletHandler CachedChain.doFilter(ServletHandler.java:1652) 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585) 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143) 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:577) 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:223) 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1125) 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515) 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185) 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1059) 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97) 	at org.eclipse.jetty.server.Server.handle(Server.java:497) 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:310) 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:248) 	at org.eclipse.jetty.io.AbstractConnection 2.run(AbstractConnection.java:540) 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:620) 	at org.eclipse.jetty.util.thread.QueuedThreadPool 3.ru {code}
WICKET-ffdd0864$$StackOverflowError after form submit with a validation error$$I was not able to find a cause or make a small quickstart, but it has something to do with a form validation, my workaround was to setDefaultFormProcessing(false) or not use required TextFields.  It can be reproduced on https://github.com/krasa/DevSupportApp/tree/stackOverflow  1) run StartVojtitkoDummy 2) go to http://localhost:8765/wicket/bookmarkable/krasa.release.frontend.TokenizationPage 3) click on "Generate Release json" button  - instead of SOE, it should give a validation error, probably even on fields which I would not want to validate, but that's just because I've made the page badly...     {code} java.lang.StackOverflowError: null ... 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:81) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:74) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy 1.component(AbstractMarkupSourcingStrategy.java:66) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:144) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:162) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:162) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:162) 	at org.apache.wicket.util.visit.Visits.visitChildren(Visits.java:123) 	at org.apache.wicket.MarkupContainer.visitChildren(MarkupContainer.java:862) 	at org.apache.wicket.markup.html.panel.AbstractMarkupSourcingStrategy.searchMarkupInTransparentResolvers(AbstractMarkupSourcingStrategy.java:65) 	at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.getMarkup(DefaultMarkupSourcingStrategy.java:99) 	at org.apache.wicket.MarkupContainer.getMarkup(MarkupContainer.java:453) 	at org.apache.wicket.Component.getMarkup(Component.java:755) 	at org.apache.wicket.Component.internalRender(Component.java:2344) 	at org.apache.wicket.Component.render(Component.java:2307) 	at org.apache.wicket.ajax.XmlAjaxResponse.writeComponent(XmlAjaxResponse.java:128) 	at org.apache.wicket.ajax.AbstractAjaxResponse.writeComponents(AbstractAjaxResponse.java:218) 	at org.apache.wicket.ajax.AbstractAjaxResponse.writeTo(AbstractAjaxResponse.java:150) 	at org.apache.wicket.ajax.AjaxRequestHandler.respond(AjaxRequestHandler.java:359) 	at org.apache.wicket.request.cycle.RequestCycle HandlerExecutor.respond(RequestCycle.java:865) 	at org.apache.wicket.request.RequestHandlerStack.execute(RequestHandlerStack.java:64) 	at org.apache.wicket.request.RequestHandlerStack.execute(RequestHandlerStack.java:97) 	at org.apache.wicket.request.cycle.RequestCycle.execute(RequestCycle.java:265) 	at org.apache.wicket.request.cycle.RequestCycle.processRequest(RequestCycle.java:222) 	at org.apache.wicket.request.cycle.RequestCycle.processRequestAndDetach(RequestCycle.java:293) 	at org.apache.wicket.protocol.ws.AbstractUpgradeFilter.processRequestCycle(AbstractUpgradeFilter.java:59) 	at org.apache.wicket.protocol.http.WicketFilter.processRequest(WicketFilter.java:203) 	at org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:284) 	at org.eclipse.jetty.servlet.ServletHandler CachedChain.doFilter(ServletHandler.java:1652) 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585) 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143) 	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:577) 	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:223) 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1125) 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515) 	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185) 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1059) 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97) 	at org.eclipse.jetty.server.Server.handle(Server.java:497) 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:310) 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:248) 	at org.eclipse.jetty.io.AbstractConnection 2.run(AbstractConnection.java:540) 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:620) 	at org.eclipse.jetty.util.thread.QueuedThreadPool 3.ru {code}
WICKET-def03add$$StackOverflowError when calling getObject() from load() in LDM$$The fix for WICKET-5772 caused an infinite loop when calling getObject() from inside load() in LoadableDetachableModel. While of course unwise to do so and nobody in their right mind would do so directly, such a cycle can be triggered through a series of unrelated calls emanating from load().
WICKET-03663750$$Page header isn't rendered for pages where URL has changed during render$$Due to the changes in WICKET-5309, a page is re-rendered when any of the URL segments is modified during the request:  From WebPageRenderer.java:  {code} 	// the url might have changed after page has been rendered (e.g. the 	// stateless flag might have changed because stateful components 	// were added) 	final Url afterRenderUrl = requestCycle 		.mapUrlFor(getRenderPageRequestHandler());  	if (beforeRenderUrl.getSegments().equals(afterRenderUrl.getSegments()) == false) 	{ 		// the amount of segments is different - generated relative URLs 		// will not work, we need to rerender the page. This can happen 		// with IRequestHandlers that produce different URLs with 		// different amount of segments for stateless and stateful pages 		response = renderPage(afterRenderUrl, requestCycle); 	}  	if (currentUrl.equals(afterRenderUrl)) {code}  The re-render causes the <head> section to be empty because it was already rendered in the first try.
WICKET-86066852$$Queuing a component within an enclosure$$Queueing doesn't work if component is in a enclosure tag.   <wicket:enclosure child="inlink"> <a href="panier.html"> 	<span wicket:id="inlink"></span> </a> </wicket:enclosure>
WICKET-31c88569$$Queuing a component in head$$Queuing a component which is in the head section doesn't work : <head> 	<meta charset="utf-8" /> 	<title wicket:id="titre">Test</title> </head>
WICKET-d547fcd4$$ResourceUtils.getLocaleFromFilename can't handle minimized resources well$$I think the ResourceUtils.getLocaleFromFilename(String path) has the order of locale and minimization wrong: It currently parses: File.min_Lang_Coun_Var.ext while the typical convention is File_Lang_Coun_Var.min.ext Surely considering the ResourceUtils.getMinifiedName() method which does work according to convention.
WICKET-8b7946d8$$CachingResourceLocator lookup key doesn't take strict into account$$CachingResourceLocator uses a CacheKey to store lookups for resources.   With e.g.  - b_nl.js - b.js  When e.g. a strict resource lookup for b.js with locale "us" is performed, it will store the not-found for locale "us" under the cache key. The cache key consists of resource name, locale, style and variation. However when you search non-strict for locale "us", the resource locator should find the non-localized resource "b.js", but since a matching key for the lookup was stored for this particular resource, it will fail.
WICKET-294b0b2f$$When using Servlet 3.0 filter Wicket calculates filter path wrong$$When using a servlet 3.0 filter with annotations Wicket calculates the filter path wrong causing it to not match any pages other than the home page.  e.g.  {code} @WebFilter(value = "/web/*", initParams = {@WebInitParam(name = "applicationClassName", value = "com.example.CheesrApplication")}) public class CheesrFilter extends WicketFilter { } {code}  Will cause Wicket to create a filter path of /web/*/ instead of the expected /web.
WICKET-eb125865$$Significant Performance Degradation From Wicket 6.20.0 to Wicket 7.0.0$$I am experiencing a significant performance degradation for component adds in Wicket 7.0.0, once the component tree for a page gets reasonably large.  The attached quick start can be used to reproduce the issue.  Please note that NUM_ROWS is set to 10,000 to exaggerate the performance degradation as the size of the component tree increases.  The same degradation (to a lesser extent) can be viewed with a smaller NUM_ROWS variable.  In Wicket 6.20.0, as the size of the component tree increases, the cost of add() remains relatively constant time-wise.  In Wicket 7.0.0, a component add () is much more expensive (and actually makes our internal web application unusable) with form submits taking more than two or three minutes to come back from the server.  Here's some timing examples.    =============================================================================================================  NUM_ROWS = 5000 Wicket 6.20.0 -> ~200 milliseconds of server side rendering (before browser paints HTML). Wicket 7.0.0 -> ~ 10 seconds of server side rendering  NUM_ROWS = 10000 Wicket 6.20.0 -> ~ 500 milliseconds of server side rendering Wicket 7.0.0 -> ~ 40 seconds of server side rendering  =============================================================================================================  The attached quickstart can be used to reproduce the issue on your side.  My guess is that this has to do with the new component queuing feature that was added as part of Wicket 7.0.0.
WICKET-a255bbca$$BaseWicketTester#startComponentInPage fails for pages with <wicket:header-items></wicket:header> placeholder$$I am using the {{[BaseWicketTester.html#startComponentInPage(C)|https://ci.apache.org/projects/wicket/apidocs/6.x/org/apache/wicket/util/tester/BaseWicketTester.html#startComponentInPage(C)]}} to validate individual components/panels.  I am overriding the {{[BaseWicketTester.html#createPage()|https://ci.apache.org/projects/wicket/apidocs/6.x/org/apache/wicket/util/tester/BaseWicketTester.html#createPage()]}} and {{[BaseWicketTester.html#createPageMarkup(java.lang.String)|https://ci.apache.org/projects/wicket/apidocs/6.x/org/apache/wicket/util/tester/BaseWicketTester.html#createPageMarkup(java.lang.String)]}} methods to return a dummy page that contains a placeholder for components-to-be-tested. The dummy page extends my base page class.  My base page class makes use of the [<wicket:header-items/>|https://cwiki.apache.org/confluence/display/WICKET/Wicket's+XHTML+tags#Wicket'sXHTMLtags-Elementwicket:header-items] placeholder tag.  When attempting to use {{[BaseWicketTester.html#createPage()|https://ci.apache.org/projects/wicket/apidocs/6.x/org/apache/wicket/util/tester/BaseWicketTester.html#createPage()]}} method, the method fails with the following error message: |Error while parsing the markup for the autogenerated page: More than one <wicket:header-items/> detected in the <head> element. Only one is allowed.  If I remove the {{[<wicket:header-items/>|https://cwiki.apache.org/confluence/display/WICKET/Wicket's+XHTML+tags#Wicket'sXHTMLtags-Elementwicket:header-items]}} placeholder tag from my base page class, the test runs successfully.  The test only fails when using the {{[BaseWicketTester.html#startComponentInPage(C)|https://ci.apache.org/projects/wicket/apidocs/6.x/org/apache/wicket/util/tester/BaseWicketTester.html#startComponentInPage(C)]}}, which only accepts one argument. If I use the {{[BaseWicketTester.html#startComponentInPage(C, org.apache.wicket.markup.IMarkupFragment)|https://ci.apache.org/projects/wicket/apidocs/6.x/org/apache/wicket/util/tester/BaseWicketTester.html#startComponentInPage(C, org.apache.wicket.markup.IMarkupFragment)]}} and pass in the {{MarkupFragment}} of the test class as the second argument, then the test runs successfully, e.g.  {code} tester.startComponentInPage(new MyPanel(DummyPanelPage.TEST_PANEL_ID),  new MyDummyPanelPage(new PageParameters()).getMarkup()); {code}  It would seem that the {{[<wicket:header-items/>|https://cwiki.apache.org/confluence/display/WICKET/Wicket's+XHTML+tags#Wicket'sXHTMLtags-Elementwicket:header-items]}} placeholder tag clashes with the {{[ContainerInfo|https://ci.apache.org/projects/wicket/apidocs/6.x/org/apache/wicket/markup/ContainerInfo.html]}} class used by the testing framework, but this is by no means my area of expertise.  I am attaching a quick-start app with a {{TestHomePage}} test class that reproduces the issue.  Thank you in advance!
MNG-1bdeeccc$$dependency with scope:system & flag optional = true doesn't appear in the class path$$Dependency with scope:system & flag  optional = true doesn't appear in the class path Try to call m2 install  or compiler:compile in the attached project. Compilation will fail ...  best regards JuBu
MNG-4e955c05$$Profile activation by os doesn't work$$Profile activation by os doesn't work.  OperatingSystemProfileActivator is missing in components.xml.  Implementation of OperatingSystemProfileActivator.isActive is wrong.
MNG-b68c84b8$$<pluginManagement><dependencies> is not propagated to child POMs$$<executions> section in <pluginManagement> isn't propagated to child POMs (as <configuration> is). The workaround is to use <plugins> with <inherited>true</inherited> Is this on purpose ?
MNG-5d99b35c$$Dependency excludes apply to every subsequent dependency, not just the one it is declared under.$$If you specify ANY dependency excludes, all dependencies after that one in the pom will also exclude what you specified.  They appear to be cumulative on every dependency in that they bleed over into sibling dependencies.   It's easy to test if you add an exclusion to a random dependency. This exclusion should exclude a required transitive dependency that is included by a dependency lower in the list.  You will find that the dependency lower in the list no longer includes the required dependency because it is using the filter you declared in the other dependency.
MNG-faa5cf27$$legacy layout tag in a profile does not show up in child pom.$$the legacy layout tag in a profile does not show up in an inherited pom.  Given the following pom.xml:  <project>   <modelVersion>4.0.0</modelVersion>   <groupId>xxx</groupId>   <artifactId>yyy</artifactId>   <version>1.0-SNAPSHOT</version>   <packaging>pom</packaging>   <profiles>     <profile>       <id>maven-1</id>       <activation>         <property>           <name>maven1</name>         </property>       </activation>       <distributionManagement>         <repository>           <id>maven-1-repo</id>           <name>Maven1 Repository</name>           <url>sftp://...</url>           <layout>legacy</layout>         </repository>       </distributionManagement>         </profile>   </profiles> </project>  gives for:  mvn projecthelp:effective-pom -Dmaven1  the following result:  ...   <distributionManagement>     <repository>       <id>maven-1-repo</id>       <name>Maven1 Repository</name>       <url>sftp://...</url>       <layout>legacy</layout>     </repository>   </distributionManagement> </project>  which is CORRECT, however if I inherit from this pom with the following pom.xml:  <project>   <parent>     <groupId>xxx</groupId>     <artifactId>yyy</artifactId>     <version>1.0-SNAPSHOT</version>   </parent>   <modelVersion>4.0.0</modelVersion>   <groupId>uuu</groupId>   <artifactId>vvv</artifactId>   <version>2.0-SNAPSHOT</version> </project>  gives for:  mvn projecthelp:effective-pom -Dmaven1  the following result:  ...   <distributionManagement>     <repository>       <id>maven-1-repo</id>       <name>Maven1 Repository</name>       <url>sftp://...</url>     </repository>   </distributionManagement> </project>  which is INCORRECT, the layout tag is missing.  This issue may be related to:  http://jira.codehaus.org/browse/MNG-731 http://jira.codehaus.org/browse/MNG-1756
MNG-24db0eb9$$Dependencies in two paths are not added to resolution when scope needs to be updated in the nearest  due to any of nearest parents$$scopes are not correctly calculated for this case  my pom: a compile, b test a: c compile, d compile b: d compile  then d ends in test scope because d is closer to my project through the path b-d  I think scope importance should also be taken into account
MNG-806eaeb0$$Dependencies in two paths are not added to resolution when scope needs to be updated in the nearest  due to any of nearest parents$$scopes are not correctly calculated for this case  my pom: a compile, b test a: c compile, d compile b: d compile  then d ends in test scope because d is closer to my project through the path b-d  I think scope importance should also be taken into account
MNG-ad38e46b$$Reporting inheritance does not work properly$$I have a parent project and some subprojects. The parent project's pom has:  <reporting>    <excludeDefaults>true</excludeDefaults> </reporting>  However, it does not get inherited to subprojects and so all default reports are generated for all subprojects.  I'm not sure if this is a bug or a lack of feature but I would be good to have it.  FYI, I'm speaking about here: http://svn.apache.org/viewcvs.cgi/directory/trunks/apacheds/
MNG-778f044e$$<pluginManagement><plugins><plugin><dependencies> do not propogate to child POM plugins (potentially scoped to only affecting child POM plugins that live within a <profile>)$$<pluginManagement><plugins><plugin><dependencies> do not propogate to child POM plugins.  Kenny believe this works OKAY if the childs are using the parent <pluginManagement> preconfigured plugins in their main <build> section however it does NOT work if the childs are trying to use those preconfigured plugins via their own <profiles>. Configuration propogates through okay but dependencies are missing and have to be respecified in the child POMs.
MNG-cc859f5c$$Multiple Executions of Plugin at Difference Inhertiance levels causes plugin executions to run multiple times$$Can occur in a variety of ways, but the attached test case shows a parent pom defining an antrun-execution, and then a child specifying another execution with a different id.  Both executions run twice when running from the child.  I believe this is the same as Kenney Westerhof's comment: http://jira.codehaus.org/browse/MNG-2054#action_62477
MNG-f0fcef7e$$1.0-beta-3 should be < 1.0-SNAPSHOT$$None
MNG-b92af0e4$$Improve handling of "no plugin version found" error after intermittent errors$$If you follow the instructions at http://maven.apache.org/guides/getting-started/index.html#How%20do%20I%20make%20my%20first%20Maven%20project? to use the archetype plugin to create a new project skeleton, the suggested command line fails as below.  It seems there is a typo of some kind in the suggested command line, I am not familiar enough with maven 2 to know for sure.  Graham-Leggetts-Computer:~/src/standard/alchemy/maven minfrin  mvn archetype:create -DgroupId=za.co.standardbank.alchemy -DartifactId=alchemy-trader    [INFO] Scanning for projects... [INFO] Searching repository for plugin with prefix: 'archetype'. [INFO] ------------------------------------------------------------------------ [ERROR] BUILD ERROR [INFO] ------------------------------------------------------------------------ [INFO] The plugin 'org.apache.maven.plugins:maven-archetype-plugin' does not exist or no valid version could be found [INFO] ------------------------------------------------------------------------ [INFO] For more information, run Maven with the -e switch [INFO] ------------------------------------------------------------------------ [INFO] Total time: 1 second [INFO] Finished at: Tue Jun 27 09:43:14 SAST 2006 [INFO] Final Memory: 1M/2M [INFO] ------------------------------------------------------------------------
MNG-06090da4$$update policy 'daily' not honored$$under certain circumstances, the '<updatePolicy>daily</updatePolicy>' isn't honored.  This is the case where the remote metadata file doesn't exist, or contains <version>RELEASE/LATEST (which should never happen)..  The timestamp used to compare for the update is 0L, because the local file doesn't exist. Then the remote file is retrieved, which also doesn't exist, and no metadatafile is created. The next time an up2date check is done, again against timestamp 0 for a non-existent file.  This means that if you define a custom snapshot repo in settings.xml or a pom, and you have 500 transitive deps, the repo's that don't have that artifact are consulted 500 times for each mvn invocation.  A build that normally takes about 20 seconds takes more than 10 minutes because of this bug.
MNG-d7422212$$DefaultArtifactCollector changes the version of the originatingArtifact if it's in the dependencyManagement with another version$$DefaultDependencyTreeBuilder https://svn.apache.org/repos/asf/maven/shared/trunk/maven-dependency-tree/src/main/java/org/apache/maven/shared/dependency/tree/DefaultDependencyTreeBuilder.java  calls collect like this              collector.collect( project.getDependencyArtifacts(), project.getArtifact(), managedVersions, repository,                                project.getRemoteArtifactRepositories(), metadataSource, null,                                Collections.singletonList( listener ) );  Problem:  This pom  http://repo1.maven.org/maven2/org/codehaus/plexus/plexus-component-api/1.0-alpha-22/plexus-component-api-1.0-alpha-22.pom extends http://repo1.maven.org/maven2/org/codehaus/plexus/plexus-containers/1.0-alpha-22/plexus-containers-1.0-alpha-22.pom that in dependencyManagement has org.codehaus.plexus:plexus-component-api:1.0-alpha-19  so during collect project.getArtifact().getVersion() is changed to the managedVersion instead of the original one  Either this is a bug or an exception should be thrown when originatingArtifact is in managedVersions
MNG-5ffd8903$$Resolution of version ranges with non-snapshot bounds can resolve to a snapshot version$$Contrary to the 2.0 design docs:  "Resolution of dependency ranges should not resolve to a snapshot (development version) unless it is included as an explicit boundary." -- from http://docs.codehaus.org/display/MAVEN/Dependency+Mediation+and+Conflict+Resolution#DependencyMediationandConflictResolution-Incorporating%7B%7BSNAPSHOT%7D%7Dversionsintothespecification  The following is equates to true:  VersionRange.createFromVersionSpec( "[1.0,1.1]" ).containsVersion( new DefaultArtifactVersion( "1.1-SNAPSHOT" ) )  The attached patch only allows snapshot versions to be contained in a range if they are equal to one of the boundaries.  Note that this is a strict equality, so [1.0,1.2-SNAPSHOT] will not contain 1.1-SNAPSHOT.
MNG-56cd921f$$Error message is misleading if a missing plugin parameter is of a type like List$$Here is a sample output I got when I was working on the changes-plugin:  {code} [INFO] One or more required plugin parameters are invalid/missing for 'changes:announcement-mail'  [0] inside the definition for plugin: 'maven-changes-plugin'specify the following:  <configuration>   ...   <smtpHost>VALUE</smtpHost> </configuration>.  [1] inside the definition for plugin: 'maven-changes-plugin'specify the following:  <configuration>   ...   <toAddresses>VALUE</toAddresses> </configuration>. {code}  Notice the second parameter toAdresses. It is of the type List, so the correct configuration would be something like this  {code} <configuration>   ...   <toAddresses>     <toAddress>VALUE</toAddress>   </toAddresses> </configuration>. {code}  I haven't found where in the code base the handling of List/Map/Array parameters is. That code could probably be borrowed/reused in maven-core/src/main/java/org/apache/maven/plugin/PluginParameterException.java which is the class responsible for formating the above messages.
MNG-f6f4ef5e$$Error message is misleading if a missing plugin parameter is of a type like List$$Here is a sample output I got when I was working on the changes-plugin:  {code} [INFO] One or more required plugin parameters are invalid/missing for 'changes:announcement-mail'  [0] inside the definition for plugin: 'maven-changes-plugin'specify the following:  <configuration>   ...   <smtpHost>VALUE</smtpHost> </configuration>.  [1] inside the definition for plugin: 'maven-changes-plugin'specify the following:  <configuration>   ...   <toAddresses>VALUE</toAddresses> </configuration>. {code}  Notice the second parameter toAdresses. It is of the type List, so the correct configuration would be something like this  {code} <configuration>   ...   <toAddresses>     <toAddress>VALUE</toAddress>   </toAddresses> </configuration>. {code}  I haven't found where in the code base the handling of List/Map/Array parameters is. That code could probably be borrowed/reused in maven-core/src/main/java/org/apache/maven/plugin/PluginParameterException.java which is the class responsible for formating the above messages.
MNG-912a565f$$Null Pointer Exception when mirrorOf missing from mirror in settings.xml$$When attempting to generate any archetype from the mvn archetype:generate command I get a null pointer exception thrown if I have mirrors defined in my settings.xml file but fail to have the mirrorOf element set.   The stack trace for the archetype generation is:  Choose a number: (1/2/3/4/5/6/7/8/9/10/11/12/13/14/15/16/17/18/19/20/21/22/23/2 4/25/26/27/28/29/30/31/32/33/34/35/36/37/38/39/40/41/42/43/44) 15: : 6 [INFO] ------------------------------------------------------------------------ [ERROR] BUILD FAILURE [INFO] ------------------------------------------------------------------------ [INFO] : java.lang.NullPointerException null [INFO] ------------------------------------------------------------------------ [INFO] Trace org.apache.maven.BuildFailureException at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoals(Defa ultLifecycleExecutor.java:579) at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeStandalone Goal(DefaultLifecycleExecutor.java:512) at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoal(Defau ltLifecycleExecutor.java:482) at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoalAndHan dleFailures(DefaultLifecycleExecutor.java:330) at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeTaskSegmen ts(DefaultLifecycleExecutor.java:227) at org.apache.maven.lifecycle.DefaultLifecycleExecutor.execute(DefaultLi fecycleExecutor.java:142) at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:336) at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:129) at org.apache.maven.cli.MavenCli.main(MavenCli.java:287) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl. java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces sorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:585) at org.codehaus.classworlds.Launcher.launchEnhanced(Launcher.java:315) at org.codehaus.classworlds.Launcher.launch(Launcher.java:255) at org.codehaus.classworlds.Launcher.mainWithExitCode(Launcher.java:430)  at org.codehaus.classworlds.Launcher.main(Launcher.java:375) Caused by: org.apache.maven.plugin.MojoFailureException at org.apache.maven.archetype.mojos.CreateProjectFromArchetypeMojo.execu te(CreateProjectFromArchetypeMojo.java:202) at org.apache.maven.plugin.DefaultPluginManager.executeMojo(DefaultPlugi nManager.java:451) at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoals(Defa ultLifecycleExecutor.java:558) ... 16 more [INFO] ------------------------------------------------------------------------ [INFO] Total time: 7 seconds [INFO] Finished at: Wed May 28 17:49:39 EST 2008 [INFO] Final Memory: 8M/14M [INFO] ------------------------------------------------------------------------  C:\Documents and Settings\frank\My Documents\Development\Sandbox>mvn -v Maven version: 2.0.9 Java version: 1.5.0_08 OS name: "windows xp" version: "5.1" arch: "x86" Family: "windows"  The mirrored settings from the settings.xml file are:  <mirrors> <mirror> <id>nexus-central</id> <url>http://maven.ho.bushlife.com.au:8081/nexus/content/groups/public</url> </mirror> </mirrors>   As a user you receive a null pointer exception because of something missing in the settings.xml file.  At the very least you should receive an error message indicating the problem. If you can have a situation where the mirrorOf setting is optional, then it should not be throwing a null pointer exception but handling it better.
MNG-2169c4a3$$POM validator allows <scope>optional</scope> but it is not valid.$$In my project I did a mistake and I wrote {code} <dependency> 	<groupId>org.slf4j</groupId> 	<artifactId>slf4j-log4j12</artifactId> 	<version>1.5.0</version> 	<scope>optional</scope> </dependency> {code}  but in fact I intended to write {code} <dependency> 	<groupId>org.slf4j</groupId> 	<artifactId>slf4j-log4j12</artifactId> 	<version>1.5.0</version> 	<optional>true</optional> </dependency> {code}  I'm very surprised that Maven doesn't detect such a mistake during the validate phase. Could it be possible to add a check to allow only valid scopes.  Thanks
MNG-0f3d4d24$$Uninterpolated expressions should cause an error for dependency versions$$I declared a dependency as such: {noformat}<dependency>     <groupId>org.slf4j</groupId>     <artifactId>slf4j-api</artifactId>     <version> {slf4j.version}</version> </dependency>{noformat}   But did not define the *slf4j.version* property. Obviously  {...} is an expression and if the expression is not resolved, why allow it? Here was the output:  {noformat} Downloading: http://repo1.maven.org/maven2/org/slf4j/slf4j-api/ {slf4j.version}/slf4j-api- {slf4j.version}.pom{noformat}   In terms of usability, an obvious expression should be interpolated. If it cannot, it should be a runtime error.
MNG-269c956e$$[regression] Wagon manager does not respect instantiation strategy of wagons$$Calling {{WagonManager.getWagon()}} multiple times from the same thread (and with the same thread context class loader) will always return the same wagon instance, even if the wagon uses "per-lookup" instantiation.
MNG-8cb04253$$[regression] Profile activation based on JDK version range fails if current version is close to range boundary$$The POM snippet {code:xml} <profiles>   <profile>     <id>test</id>     <activation>       <jdk>[1.5,1.6)</jdk>     </activation>   </profile> </profiles> {code} yields {noformat} [ERROR]   The project org.apache.maven.its.mng:test:0.1 has 1 error [ERROR]     Failed to determine activation for profile test: For input string: "0_07" java.lang.NumberFormatException: For input string: "0_07"         at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)         at java.lang.Integer.parseInt(Integer.java:456)         at java.lang.Integer.parseInt(Integer.java:497)         at o.a.m.model.profile.activation.JdkVersionProfileActivator.getRelationOrder(JdkVersionProfileActivator.java:124)         at o.a.m.model.profile.activation.JdkVersionProfileActivator.isInRange(JdkVersionProfileActivator.java:96)         at o.a.m.model.profile.activation.JdkVersionProfileActivator.isActive(JdkVersionProfileActivator.java:70) {noformat} when run with JDK 1.6.0_07 in this case.
MNG-f5ebc72d$$Profile activation based on JRE version misbehaves if java.version has build number$$For this POM snippet {code:xml} <profiles>   <profile>     <id>test</id>     <activation>       <jdk>[1.6,)</jdk>     </activation>   </profile> </profiles> {code} running "mvn help:active-profiles -V" delivers: {noformat} >mvn help:active-profiles -V Apache Maven 2.2.1 (r801777; 2009-08-06 21:16:01+0200) Java version: 1.6.0_07 Java home: D:\Programme\Java\jdk-1.6.0_07\jre Default locale: de_DE, platform encoding: Cp1252 OS name: "windows xp" version: "5.1" arch: "x86" Family: "windows" [INFO] Scanning for projects... [INFO] Searching repository for plugin with prefix: 'help'. [INFO] ------------------------------------------------------------------------ [INFO] Building Maven Integration Test :: MNG- [INFO]    task-segment: [help:active-profiles] (aggregator-style) [INFO] ------------------------------------------------------------------------ [INFO] [help:active-profiles {execution: default-cli}] [INFO] Active Profiles for Project 'org.apache.maven.its.mng:test:jar:0.1':  There are no active profiles. {noformat} This is due to the version 1.6.0_07 which when parsed with Maven version rules is considered older than 1.6.
MNG-03a383e3$$maven fails on IBM JDK 1.5.0 with exception IllegalAccessException: Field is final$$On Windows XP, and IBM JDK 1.5.0, maven 2.2.1 fails with the exception IllegalAccessException: Field is final. (See the complete stacktrace is at the end)  How to duplicate: 1. (IMPORTANT) Delete maven local repository at <user home>\.m2\repository directory completely! 2. Unzip myapp.zip 3. Run command "mvn package -e"  More info: The exception will happen when maven trying to set value to some static final fields. Re-run the command will see another new exception (old exception will not happend again). If you try maven in debug mode (to debug it with Eclipse), the exception will NOT appear. The complete information about maven, JDK, etc. are in the attached file: ibm.output.txt. The other output file (sun.output.txt) is the successful result when running using Sun JDK 1.5.0  Root cause: In StringSearchModelInterpolator.java of maven 2.2.1, there is a code snippet that tries to using Reflection to change values of fields (even if the fields are final)  Line 175:    fields[i].setAccessible( true ); Line 189:   fields[i].set( target, interpolated );  If fields[i] is a final field, on Sun JDK 1.5.0, the operation is successful but on IBM JDK 1.5.0, an exception will be thrown.     ================ Complete stacktrace ========================== org.apache.maven.lifecycle.LifecycleExecutionException: Unable to build project for plugin 'org.apache.maven.plugins:maven-compiler-plugin': Failed to interpolate field: public static final java.lang.String org.codehaus.plexus.util.xml.Xpp3Dom.CHILDREN_COMBINATION_MODE_ATTRIBUTE on class: org.codehaus.plexus.util.xml.Xpp3Dom for project unknown:maven-compiler-plugin at Artifact [org.apache.maven.plugins:maven-compiler-plugin:pom:2.0.2] 	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.verifyPlugin(DefaultLifecycleExecutor.java:1557) 	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.getMojoDescriptor(DefaultLifecycleExecutor.java:1851) 	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.bindLifecycleForPackaging(DefaultLifecycleExecutor.java:1311) 	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.constructLifecycleMappings(DefaultLifecycleExecutor.java:1275) 	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoal(DefaultLifecycleExecutor.java:534) 	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoalAndHandleFailures(DefaultLifecycleExecutor.java:387) 	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeTaskSegments(DefaultLifecycleExecutor.java:348) 	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.execute(DefaultLifecycleExecutor.java:180) 	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:328) 	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:138) 	at org.apache.maven.cli.MavenCli.main(MavenCli.java:362) 	at org.apache.maven.cli.compat.CompatibleMain.main(CompatibleMain.java:60) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:615) 	at org.codehaus.classworlds.Launcher.launchEnhanced(Launcher.java:315) 	at org.codehaus.classworlds.Launcher.launch(Launcher.java:255) 	at org.codehaus.classworlds.Launcher.mainWithExitCode(Launcher.java:430) 	at org.codehaus.classworlds.Launcher.main(Launcher.java:375) Caused by: org.apache.maven.plugin.InvalidPluginException: Unable to build project for plugin 'org.apache.maven.plugins:maven-compiler-plugin': Failed to interpolate field: public static final java.lang.String org.codehaus.plexus.util.xml.Xpp3Dom.CHILDREN_COMBINATION_MODE_ATTRIBUTE on class: org.codehaus.plexus.util.xml.Xpp3Dom for project unknown:maven-compiler-plugin at Artifact [org.apache.maven.plugins:maven-compiler-plugin:pom:2.0.2] 	at org.apache.maven.plugin.DefaultPluginManager.checkRequiredMavenVersion(DefaultPluginManager.java:293) 	at org.apache.maven.plugin.DefaultPluginManager.verifyVersionedPlugin(DefaultPluginManager.java:205) 	at org.apache.maven.plugin.DefaultPluginManager.verifyPlugin(DefaultPluginManager.java:184) 	at org.apache.maven.plugin.DefaultPluginManager.loadPluginDescriptor(DefaultPluginManager.java:1642) 	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.verifyPlugin(DefaultLifecycleExecutor.java:1540) 	... 19 more Caused by: org.apache.maven.project.InvalidProjectModelException: Failed to interpolate field: public static final java.lang.String org.codehaus.plexus.util.xml.Xpp3Dom.CHILDREN_COMBINATION_MODE_ATTRIBUTE on class: org.codehaus.plexus.util.xml.Xpp3Dom for project unknown:maven-compiler-plugin at Artifact [org.apache.maven.plugins:maven-compiler-plugin:pom:2.0.2] 	at org.apache.maven.project.DefaultMavenProjectBuilder.buildInternal(DefaultMavenProjectBuilder.java:884) 	at org.apache.maven.project.DefaultMavenProjectBuilder.buildFromRepository(DefaultMavenProjectBuilder.java:255) 	at org.apache.maven.plugin.DefaultPluginManager.checkRequiredMavenVersion(DefaultPluginManager.java:277) 	... 23 more Caused by: org.apache.maven.project.interpolation.ModelInterpolationException: Failed to interpolate field: public static final java.lang.String org.codehaus.plexus.util.xml.Xpp3Dom.CHILDREN_COMBINATION_MODE_ATTRIBUTE on class: org.codehaus.plexus.util.xml.Xpp3Dom 	at org.apache.maven.project.interpolation.StringSearchModelInterpolator InterpolateObjectAction.traverseObjectWithParents(StringSearchModelInterpolator.java:318) 	at org.apache.maven.project.interpolation.StringSearchModelInterpolator InterpolateObjectAction.run(StringSearchModelInterpolator.java:135) 	at org.apache.maven.project.interpolation.StringSearchModelInterpolator InterpolateObjectAction.run(StringSearchModelInterpolator.java:102) 	at java.security.AccessController.doPrivileged(AccessController.java:192) 	at org.apache.maven.project.interpolation.StringSearchModelInterpolator.interpolateObject(StringSearchModelInterpolator.java:80) 	at org.apache.maven.project.interpolation.StringSearchModelInterpolator.interpolate(StringSearchModelInterpolator.java:62) 	at org.apache.maven.project.DefaultMavenProjectBuilder.processProjectLogic(DefaultMavenProjectBuilder.java:990) 	at org.apache.maven.project.DefaultMavenProjectBuilder.buildInternal(DefaultMavenProjectBuilder.java:880) 	... 25 more Caused by: java.lang.IllegalAccessException: Field is final 	at sun.reflect.UnsafeQualifiedStaticObjectFieldAccessorImpl.set(UnsafeQualifiedStaticObjectFieldAccessorImpl.java:77) 	at java.lang.reflect.Field.set(Field.java:684) 	at org.apache.maven.project.interpolation.StringSearchModelInterpolator InterpolateObjectAction.traverseObjectWithParents(StringSearchModelInterpolator.java:189) 	... 32 more
MNG-c6529932$$Requiring multiple profile activation conditions to be true does not work$$According to the documentation at http://www.sonatype.com/books/mvnref-book/reference/profiles-sect-activation.html a profile is activated when all activation conditions are met (which makes sense of course). But when I try to use this it does not work. It seems maven does an OR instead of an AND (which is not rearly as useful and is the opposite of what the documentation says at the previous link).  For example, if I have one profile that is activated like this:  {code:xml}         <activation>             <activeByDefault>false</activeByDefault>             <os>                <name>linux</name>             </os>          </activation>{code}  and another profile that is activated like this:  {code:xml}        <activation>             <activeByDefault>false</activeByDefault>             <os>                <name>linux</name>             </os>             <property>                 <name>release</name>                 <value>true</value>             </property>          </activation>{code}  Then I would expect the second profile to only be activated if the OS is linux and the release property is defined.  When I run 'mvn help:active-profiles' however, maven shows that both profiles are active even though the release property is not defined.
MNG-83389c34$$NullPointerException thrown from DefaultPluginRealmCache#pluginHashCode method if project-level plugin dependency misses version$$As a user i would like to see better error reporting from DefaultPluginRealmCache#pluginHashCode method  Currently it calculates hash value based on a dependency metadata, but if I omit version it fails with NullPointer exception.  It would be more user friendly to validate metadata prior to calculating hash value and to display more meaningful error to the end user.  Test scenario:  - configure plugin and create dependencies  - add dependency but DO NOT specify version  - run maven such that plugin is invoked maven will fail without reporting which dependency doesn't have version
MNG-bb39b480$$Missing Error during pom validation: "You cannot have two plugin executions with the same (or missing) <id/> elements."$$Maven 2.2.1 gives an error "You cannot have two plugin executions with the same (or missing) <id/> elements.", if there are two executions for the same plugin without an id element.  Maven 3.0 beta1 doesn't throw this usefull validation error. It uses the first configuration for both executions.  A relatet issue, where this error has occured with an attached example: http://jira.codehaus.org/browse/MRPM-76
MNG-8cdb461f$$Plugin-level dependency scope causes some plugin classpaths to be incorrect$$Plugin-level dependencies should use RUNTIME scope at all times. Using any other scope may alter the weighting given to the subgraph-choice algorithm used in transitive dependency resolution.   Plugin-level dependencies use compile scope by default. When transitive resolution takes place, compile scope takes precedence over runtime scope, causing the transitive dependency sub-graph of the plugin-level dependency to be activated over those of the plugin itself.  This happens even when the plugin's transitive dep is NEARER to the top level than the one brought in by that plugin-level dependency itself.  The result is that when a dep that's farther away is chosen over a nearer one, it can then be disabled by Maven choosing to disable its parent dep (the one that brought it in) in another part of the transitive resolution process.  This is a very subtle case where Maven is doing the wrong thing. The attached test case should make it clearer.
MNG-3fca2bb2$$Interpolation error due to cyclic expression for one of the POM coordinates gets needlessly repeated$$This simple POM {code:xml} <project>   <modelVersion>4.0.0</modelVersion>    <groupId> {groupId}</groupId>   <artifactId>test</artifactId>   <version>0.1</version>   <packaging>jar</packaging>    <distributionManagement>     <repository>       <id>maven-core-it</id>       <url>file:/// {basedir}/repo</url>     </repository>   </distributionManagement> </project> {code} causes the following model errors: {noformat} [ERROR]     Resolving expression: ' {groupId}': Detected the following recursive expression cycle: [groupId] -> [Help 2] [ERROR]     Resolving expression: ' {groupId}': Detected the following recursive expression cycle: [groupId] -> [Help 2] [ERROR]     Resolving expression: ' {groupId}': Detected the following recursive expression cycle: [groupId] -> [Help 2] [ERROR]     Resolving expression: ' {groupId}': Detected the following recursive expression cycle: [groupId] -> [Help 2] [ERROR]     Resolving expression: ' {groupId}': Detected the following recursive expression cycle: [groupId] -> [Help 2] [ERROR]     Resolving expression: ' {groupId}': Detected the following recursive expression cycle: [groupId] -> [Help 2] [ERROR]     Resolving expression: ' {groupId}': Detected the following recursive expression cycle: [groupId] -> [Help 2] [ERROR]     Resolving expression: ' {groupId}': Detected the following recursive expression cycle: [groupId] -> [Help 2] [ERROR]     Resolving expression: ' {groupId}': Detected the following recursive expression cycle: [groupId] -> [Help 2] [ERROR]     Resolving expression: ' {groupId}': Detected the following recursive expression cycle: [groupId] -> [Help 2] [ERROR]     Resolving expression: ' {groupId}': Detected the following recursive expression cycle: [groupId] -> [Help 2] [ERROR]     'groupId' with value ' {groupId}' does not match a valid id pattern. @ line 25, column 12 {noformat} Note the excessive repetition of the groupId related cycle although this expression actually appears only once in the POM.
MNG-1c3abfba$$Versions in pom.xml are not checked for invalid characters$$It seems that the pom.xml is not checking if the version contains invalid characters. If I have following fragment inside pom:  <dependency>   <groupId>junit</groupId>   <artifactId>junit</artifactId>   <version>>>3.5</version> </dependency>  then Maven is trying to download JUnit version >>3.5
MNG-691a03a7$$MavenProject#clone() doubles active profiles$$The error occured in our Project with more than 60 submodules and aggregating JavaDoc. Due to the forking of the LifeCycle many clones of the MavenProject object seem to be performed. Since MavenProject#clone doubles the entries in the list of active profiles we start with one active profile and after a few dozen clones the list of active profiles exceeds 10.000.000 elements. This will than kill the VM by OOME.   mavenProject.getActiveProfiles().size() == 1  mavenProject.clone().getActiveProfiles().size() == 2  mavenProject.clone().clone().getActiveProfiles().size() == 4 and so on
MNG-469d0096$$With a resource directory as . maven raise an java.lang.StringIndexOutOfBoundsException:217$$I exexute a release:prepare-with-pom I debug this execution and I found when the directory is equal to basedir.getPath() an exception is raised.  And I have this definition in my pom.xml       <resource>         <directory>.</directory>         <includes>           <include>plugin.xml</include>           <include>plugin.properties</include>           <include>icons/**</include>         </includes>       </resource>  Trace:  [ERROR] Failed to execute goal org.apache.maven.plugins:maven-release-plugin:2.1.1:prepare-with-pom (default-cli) on project servicelayer: Execution default-cli of goal org.apache.maven.plugins:maven-release-plugin:2.1.1:prepare-with-pom failed: String index out of range: -1 -> [Help 1] org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-release-plugin:2.1.1:prepare-with-pom (default-cli) on project servicelayer: Execution default-cli of goal org.apache.maven.plugins:maven-release-plugin:2.1.1:prepare-with-pom failed: String index out of range: -1         at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:211)         at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:148)         at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:140)         at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)         at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)         at org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)         at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)         at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:316)         at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:153)         at org.apache.maven.cli.MavenCli.execute(MavenCli.java:451)         at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:188)         at org.apache.maven.cli.MavenCli.main(MavenCli.java:134)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)         at java.lang.reflect.Method.invoke(Method.java:597)         at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:290)         at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:230)         at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:409)         at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:352) Caused by: org.apache.maven.plugin.PluginExecutionException: Execution default-cli of goal org.apache.maven.plugins:maven-release-plugin:2.1.1:prepare-with-pom failed: String index out of range: -1         at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:116)         at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:195)         ... 19 more Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: -1         at java.lang.String.substring(String.java:1937)         at java.lang.String.substring(String.java:1904)         at org.apache.maven.project.path.DefaultPathTranslator.unalignFromBaseDirectory(DefaultPathTranslator.java:217)         at org.apache.maven.project.path.DefaultPathTranslator.unalignFromBaseDirectory(DefaultPathTranslator.java:181)         at org.apache.maven.shared.release.phase.GenerateReleasePomsPhase.createReleaseModel(GenerateReleasePomsPhase.java:274)         at org.apache.maven.shared.release.phase.GenerateReleasePomsPhase.generateReleasePom(GenerateReleasePomsPhase.java:141)         at org.apache.maven.shared.release.phase.GenerateReleasePomsPhase.generateReleasePoms(GenerateReleasePomsPhase.java:129)         at org.apache.maven.shared.release.phase.GenerateReleasePomsPhase.execute(GenerateReleasePomsPhase.java:105)         at org.apache.maven.shared.release.phase.GenerateReleasePomsPhase.execute(GenerateReleasePomsPhase.java:92)         at org.apache.maven.shared.release.DefaultReleaseManager.prepare(DefaultReleaseManager.java:203)         at org.apache.maven.shared.release.DefaultReleaseManager.prepare(DefaultReleaseManager.java:140)         at org.apache.maven.shared.release.DefaultReleaseManager.prepare(DefaultReleaseManager.java:103)         at org.apache.maven.plugins.release.PrepareReleaseMojo.prepareRelease(PrepareReleaseMojo.java:279)         at org.apache.maven.plugins.release.PrepareWithPomReleaseMojo.execute(PrepareWithPomReleaseMojo.java:47)         at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:107)         ... 20 more [ERROR] [ERROR] Re-run Maven using the -X switch to enable full debug logging. [ERROR]
MNG-c4002945$$PluginDescriptorBuilder doesn't populate expression/default-value fields for mojo parameters$$As noted by Guo Du in his patch for MPH-81, the mojo descriptors created by the {{PluginDescriptorBuilder}} come out with parameter descriptors that always return null for the parameter expression and default value.
MNG-a7d9b689$$MavenPluginManager serves m2e partially initialized mojo descriptors in some cases$$Not sure if this affect command line maven, but m2e sometimes gets MojoDescriptor instances with null classRealm and implementationClass. Looks like the problem has to do with incomplete plugin descriptor setup when after reused cached plugin realm. I'll try to provide a fix and corresponding unit test.
MNG-2eb419ed$$MavenProject.getParent throws undocumented ISE$$http://bugzilla-attachments-197994.netbeans.org/bugzilla/attachment.cgi?id=107899 shows a stack trace encountered when calling {{MavenProject.getParent}} on a project with some errors (probably POMs missing in the local repository).  This method has no Javadoc comment, so it is hard to know exactly what it is permitted/supposed to do, but {{hasParent}} implies that {{null}} is a valid return value, and there is no {{throws IllegalStateException}} clause. The attached patch brings the behavior in line with that signature. (I think I got the {{PlexusTestCase}} infrastructure working with all the required wiring but it may be possible to simplify the test case.)  Cleaner might be to just declare {{getParent}} (and also {{hasParent}}?) to throw {{ProjectBuildingException}}, though this would be a source-incompatible change. (Only binary-incompatible for clients which are already catching {{IllegalStateException}}!)
MNG-87884c7b$$MavenProject.getTestClasspathElements can return null elements$$A broken {{MavenProject}} can return null values in certain lists, which seems incorrect (at least it is undocumented). Root cause of: http://netbeans.org/bugzilla/show_bug.cgi?id=205867
MNG-ed651a4d$$MavenProject.getTestClasspathElements can return null elements$$A broken {{MavenProject}} can return null values in certain lists, which seems incorrect (at least it is undocumented). Root cause of: http://netbeans.org/bugzilla/show_bug.cgi?id=205867
MNG-712c4fff$$DefaultPluginDescriptorCache does not retain pluginDescriptor dependencies$$PluginDescriptor dependencies is defined in META-INF/maven/plugin.xml, so there is no reason not to retain this field when storing and retrieving PluginDescriptor instances to and from descriptor cache. The attribute can be used in embedding scenarios and for command line builds to quickly determine if plugin has certain dependencies or not, without having to fully resolve plugin dependencies. I'll commit a fix with corresponding unit test shortly.
MNG-c53d95ce$$DefaultPluginDescriptorCache does not retain pluginDescriptor dependencies$$PluginDescriptor dependencies is defined in META-INF/maven/plugin.xml, so there is no reason not to retain this field when storing and retrieving PluginDescriptor instances to and from descriptor cache. The attribute can be used in embedding scenarios and for command line builds to quickly determine if plugin has certain dependencies or not, without having to fully resolve plugin dependencies. I'll commit a fix with corresponding unit test shortly.
MNG-c225847e$$failure to resolve pom artifact from snapshotVersion in maven-metadata.xml$$We're using Artifactory on the server side, and ivy / sbt to publish artifacts  upstream.  After publishing several -SNAPSHOT versions of a project, trying to use it from Maven, resulted in a warning and ultimately a build failure because it cannot determine the dependencies:  {code}[WARNING] The POM for com.foo:bar:jar:0.4.0-20130404.093655-3 is missing, no dependency information available{code}  This is the corresponding maven-metadata-snapshots.xml: {code:xml} <metadata>   <groupId>com.foo</groupId>   <artifactId>bar</artifactId>   <version>0.4.0-SNAPSHOT</version>   <versioning>     <snapshot>       <timestamp>20130404.090532</timestamp>       <buildNumber>2</buildNumber>     </snapshot>     <lastUpdated>20130404093657</lastUpdated>     <snapshotVersions>       <snapshotVersion>         <extension>pom</extension>         <value>0.4.0-20130404.090532-2</value>         <updated>20130404090532</updated>       </snapshotVersion>       <snapshotVersion>         <extension>jar</extension>         <value>0.4.0-20130404.093655-3</value>         <updated>20130404093655</updated>       </snapshotVersion>     </snapshotVersions>   </versioning> </metadata> {code}  As you can see, the <value> for the jar artifact and the pom artifact differ:  0.4.0-20130404.093655-3 0.4.0-20130404.090532-2  Apparently, artifactory optimizes the case when an artifact doesn't change; it does not create a new file, but just links to the existing one.  Maven, however, takes a shortcut and makes the erroneous assumption that the values for pom and jar artifact always match up.  The attached patch fixes this.
MNG-bef7fac6$$NPE error when building a reactor with duplicated artifacts$$Using v3.2.1 when building a malformed project containing a duplicated groupId:artifactId I got this rather unhelpful error:  {code} [ERROR] Internal error: java.lang.NullPointerException -> [Help 1] org.apache.maven.InternalErrorException: Internal error: java.lang.NullPointerException 	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:167) 	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584) 	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:213) 	at org.apache.maven.cli.MavenCli.main(MavenCli.java:157) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:601) 	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289) 	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229) 	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415) 	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356) Caused by: java.lang.NullPointerException 	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:270) 	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155) 	... 11 more {code}  The more helpful error should have been: {code} org.apache.maven.project.DuplicateProjectException: Project 'com.foo.bar:foo-bar:2.15.0' is duplicated in the reactor 	at org.apache.maven.project.ProjectSorter.<init>(ProjectSorter.java:93) 	at org.apache.maven.DefaultProjectDependencyGraph.<init>(DefaultProjectDependencyGraph.java:53) 	at org.apache.maven.DefaultMaven.createProjectDependencyGraph(DefaultMaven.java:819) 	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:268) 	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155) 	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584) 	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:213) 	at org.apache.maven.cli.MavenCli.main(MavenCli.java:157) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:601) 	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289) 	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229) 	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415) 	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356) {code}
MNG-af1ecd5f$$version of "..." causes InternalErrorException.$$The following dependency causes InternalErrorException.      <dependency>       <groupId>any</groupId>       <artifactId>any</artifactId>       <version>...</version>     </dependency>  This can be confusing to a new maven user trying to get a dependency to work.  A patch is attached that fixes the problem.
MNG-cdb8ad6d$${maven.build.timestamp} uses incorrect ISO datetime separator$$Separator must be {{T}} and not {{-}} if we predefine ISO 8601 datetime format. Additionally, {{Z}} should be added to denote UTC time zone.
MNG-96337372$$WeakMojoExecutionListener callbacks invoked multiple times in some cases$$When the same WeakMojoExecutionListener instance is injected under multiple component Key's, the instance before/after execution callbacks are invoked multiple times.
MNG-3d2d8619$$Parallel Builds can build in wrong order$$Fixed JDK8 IT failure for MavenITmng3004ReactorFailureBehaviorMultithreadedTest#testitFailFastSingleThread  It turns out the execution order of the modules in the build can be incorrect, in some cases severely incorrect. For parallel builds this can have all sorts of interesting side effects such as classpath appearing to be intermittently incorrect, missing jars/resources and similar.  The -am options and -amd options may simply fail with the incorrect build order because expected dependencies have not been built and actual dependencies may not have been built.  The underlying problem was that ProjectDependencyGraph#getDownstreamProjects and getUpstreamProjects did not actually obey the reactor build order as defined by ProjectDependencyGraph#getSortedProjects, even though the javadoc claims they should.  This has only worked by accident on earlier JDK's and might not have worked at all (basically depends on Set iteration order being equal to insertion order). JDK8 has slightly different iteration order, which caused the IT failure.  This problem may be the root cause of MNG-4996 and any other issue where the modules build in incorrect order.  The bug affects:  parallel builds command line -am (--also-make) option command line -amd (also-make-dependents) option  On all java versions, although visibility might be somewhat different on different jdks.  Added simple unit test that catches the problem.
MNG-2d0ec942$$ToolchainManagerPrivate.getToolchainsForType() returns toolchains that are not of expected type$$found while working on maven-toolchains-plugin 1.1: {noformat}[INFO] Required toolchain: fake-type [ other-attribute='other-value' attribute='value' ] [DEBUG] Toolchain JDK[/home/opt/jdk1.5] is missing required property: other-attribute [DEBUG] Toolchain JDK[/home/opt/jdk1.6] is missing required property: other-attribute [DEBUG] Toolchain JDK[/home/opt/jdk1.7] is missing required property: other-attribute [DEBUG] Toolchain JDK[/home/opt/jdk1.8] is missing required property: other-attribute [ERROR] No toolchain matched for type fake-type [INFO] Required toolchain: another-fake-type [ any ] [INFO] Toolchain matched for type another-fake-type: JDK[/home/opt/jdk1.5]{noformat}  these jdk toochains should not have been ever tested again non-jdk type requirement
MNG-ce6f0bfd$$unexpected InvalidArtifactRTException from ProjectBuilder#build$$Calling into ProjectBuilder#build(File, ProjectBuildingRequest) results in InvalidArtifactRTException below if project pom.xml has managed dependency without <version>. Although the pom is invalid, I expected to get ProjectBuildingException that includes location of problematic dependency, similar to what I get during command line build.  {code} org.apache.maven.artifact.InvalidArtifactRTException: For artifact {org.apache.maven.its:a:null:jar}: The version cannot be empty. 	at org.apache.maven.artifact.DefaultArtifact.validateIdentity(DefaultArtifact.java:148) 	at org.apache.maven.artifact.DefaultArtifact.<init>(DefaultArtifact.java:123) 	at org.apache.maven.bridge.MavenRepositorySystem.XcreateArtifact(MavenRepositorySystem.java:695) 	at org.apache.maven.bridge.MavenRepositorySystem.XcreateDependencyArtifact(MavenRepositorySystem.java:613) 	at org.apache.maven.bridge.MavenRepositorySystem.createDependencyArtifact(MavenRepositorySystem.java:121) 	at org.apache.maven.project.DefaultProjectBuilder.initProject(DefaultProjectBuilder.java:808) 	at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:174) 	at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:118) ... {code}
MNG-6ab41ee8$$inconsistent classloading for extensions=true plugins$$Maven creates two class realms for build extensions plugins. One realm is used to contribute core extensions and the other to execute plugins goals. The two realms have slightly different classpath, with extensions realm not "seeing" classes from other extensions and not resolving reactor dependencies. The two realms are mostly independent and have duplicate copies of components, including duplicate copies of singletons. This results in multiple invocation of singleton components in some cases. This also results inconsistent/unexpected component wiring.
CAMEL-205420e2$$Multicast with pipeline may cause wrong aggregated exchange$$This is a problem when using 2 set of nested pipeline and doing a transform as the first processor in that pipeline {code}                 from("direct:start").multicast(new SumAggregateBean())                     .pipeline().transform(bean(IncreaseOne.class)).bean(new IncreaseTwo()).to("log:foo").end()                     .pipeline().transform(bean(IncreaseOne.class)).bean(new IncreaseTwo()).to("log:bar").end()                 .end()                 .to("mock:result"); {code}
CAMEL-f7dd2fff$$RouteBuilder - Let if fail if end user is configuring onException etc after routes$$All such cross cutting concerns must be defined before routes.  We should throw an exception if end user has configured them after routes, which is currently not supported in the DSL.
CAMEL-4badd9c5$$Property resolve in EIP does not work when in a sub route.$$The 2.5 feature: "The EIP now supports property placeholders in the String based options (a few spots in Java DSL where its not possible). For example:  <convertBodyTo type="String" charset="{{foo.myCharset}}"/>" does not work correctly when ie nested in a <choice> tag.  See discussion: http://camel.465427.n5.nabble.com/Camel-2-5-Propertyplaceholders-and-Spring-DSL-still-not-working-td3251608.html#a3251608  Example route:  This works:  <route>          <from uri="direct:in" />          <convertBodyTo type="String" charset="{{charset.external}}" />	         <log message="Charset: {{charset.external}}" />          <to uri="mock:out" />  </route>   This fails:  <route>          <from uri="direct:in" />          <choice>                  <when>                          <constant>true</constant>                          <convertBodyTo type="String" charset="{{charset.external}}" />	                 </when>          </choice>          <to uri="mock:out" />  </route>
CAMEL-0919a0f6$$@OutHeaders in bean binding issue with InOnly MEP$$When you invoke a bean with a method signature like this in Camel 2.5.0/HEAD, the in and out message both are null (the "Hello!" value just disappears):  {code:java}     public String doTest(@Body Object body, @Headers Map headers, @OutHeaders Map outHeaders) {         return "Hello!";     } {code}  The same thing without the headers works OK:  {code:java}     public String doTest(@Body Object body) {         return "Hello!";     } {code} See camel-core/src/test/java/org/apache/camel/component/bean/BeanWithHeadersAndBodyInject3Test.java
CAMEL-18e1a142$$Splitter and Multicast EIP marks exchange as exhausted to early if exception was thrown from an evaluation$$See nabble http://camel.465427.n5.nabble.com/Cannot-handle-Exception-thrown-from-Splitter-Expression-tp3286043p3286043.html
CAMEL-8433e6db$$Splitter - Exchange.CORRELATION_ID should point back to parent Exchange id$$See nabble http://camel.465427.n5.nabble.com/Splitted-exchange-has-incorrect-correlation-ID-tp3289354p3289354.html
CAMEL-320545cd$$DefaultCamelContext.getEndpoint(String name, Class<T> endpointType) throws Nullpointer for unknown endpoint$$The method getEndpoint throws an NullPointerException when it's called with an unknown endpoint name:  java.lang.NullPointerException 	at org.apache.camel.impl.DefaultCamelContext.getEndpoint(DefaultCamelContext.java:480) 	at org.apache.camel.impl.DefaultCamelContextTest.testGetEndPointByTypeUnknown(DefaultCamelContextTest.java:95)  The patch is attached.
CAMEL-e76d23b0$$Undefined header results in Nullpointer when expression is evaluated$$If you define a filter for a header that is not defined like  from("p:a").filter(header("header").in("value")).to("p:b");  it results in a NullPointerException:  {code} 2010-12-15 10:07:45,920 [main] ERROR DefaultErrorHandler            -  Failed delivery for exchangeId: 0215-1237-1292404064936-0-2.  Exhausted after delivery attempt: 1 caught: java.lang.NullPointerException 	at org.apache.camel.builder.ExpressionBuilder 40.evaluate(ExpressionBuilder.java:955) 	at org.apache.camel.impl.ExpressionAdapter.evaluate(ExpressionAdapter.java:36) 	at org.apache.camel.builder.BinaryPredicateSupport.matches(BinaryPredicateSupport.java:54) 	at org.apache.camel.builder.PredicateBuilder 5.matches(PredicateBuilder.java:127) 	at org.apache.camel.processor.FilterProcessor.process(FilterProcessor.java:46) 	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:70) 	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98) 	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89) {code}  This test reproduces the problem: {code} public void testExpressionForUndefinedHeader(){     Expression type = ExpressionBuilder.headerExpression("header");     Expression expression = ExpressionBuilder.constantExpression("value");     Expression convertToExpression = ExpressionBuilder.convertToExpression(expression, type);     convertToExpression.evaluate(exchange, Object.class); } {code}
CAMEL-b345dd82$$Route scoped onException may pick onException from another route if they are the same class type$$If you have a clash with route scoped onException and have the exact same class, then the key in the map isn't catering for this. And thus a 2nd route could override the 1st route onException definition.  For example:  from X route A   onException IOException  from Y route B   onException IOException  The map should contain 2 entries, but unfortunately it only contain 1. This only happens when its an exact type match.
CAMEL-b4606700$$Splitter Component: Setting 'streaming = "true"' breaks error handling$$Setting 'streaming = "true"' breaks error handling: If an exception is thrown in a processor, the exception in the subExchange is copied to the original exchange in MulticastProcessor line 554. In Splitter line 140 the original exchange is copied, including the exception that was thrown while processing the previous exchange. This prevents all subsequent exchanges from being processed successfully.
CAMEL-41e4b5b9$$scala - xpath not working together with choice/when$$When using the Scala DSL, xpath expressions inside when() do not work as expected. As an example: {code:none}      "direct:a" ==> {      choice {         when (xpath("//hello")) to ("mock:english")         when (xpath("//hallo")) {           to ("mock:dutch")           to ("mock:german")         }          otherwise to ("mock:french")       }     }  // Send messages "direct:a" ! ("<hello/>", "<hallo/>", "<hellos/>") {code}  Here we should receive 1 message in each of the mocks. For whatever reason, all 3 messages go to mock:english. Similar routes work as expected with the Java DSL.
CAMEL-b56d2962$$Aggregation fails to call onComplete for exchanges if the aggregation is after a bean or process.$$When creating a route that contains an aggregation, if that aggregation is preceded by a bean or process, it will fail to call AggregateOnCompletion.onComplete(). I've attached a unit test that can show you the behavior. Trace level loggging will need to be enabled to see the difference. With the call to the bean, it won't show the following log entry: {noformat}TRACE org.apache.camel.processor.aggregate.AggregateProcessor - Aggregated exchange onComplete: Exchange[Message: ab]{noformat} If you remove the bean call, it'll start calling onComplete() again.  What I've noticed is that if this call is not made, it ends up in a memory leak since the inProgressCompleteExchanges HashSet in AggregateProcessor never has any exchange ID's removed.
CAMEL-050c542e$$MethodCallExpression doesn't validate whether the method exists for all cases$$I tried to refactor  {code:title=org.apache.camel.model.language.MethodCallExpression.java}     public Expression createExpression(CamelContext camelContext) {         Expression answer;          if (beanType != null) {                         instance = ObjectHelper.newInstance(beanType);             return new BeanExpression(instance, getMethod(), parameterType); // <--         } else if (instance != null) {             return new BeanExpression(instance, getMethod(), parameterType); // <--         } else {             String ref = beanName();             // if its a ref then check that the ref exists             BeanHolder holder = new RegistryBean(camelContext, ref);             // get the bean which will check that it exists             instance = holder.getBean();             answer = new BeanExpression(ref, getMethod(), parameterType);         }          // validate method         validateHasMethod(camelContext, instance, getMethod(), parameterType);          return answer;     } {code}  to  {code:title=org.apache.camel.model.language.MethodCallExpression.java}     public Expression createExpression(CamelContext camelContext) {         Expression answer;          if (beanType != null) {                         instance = ObjectHelper.newInstance(beanType);             answer = new BeanExpression(instance, getMethod(), parameterType); // <--         } else if (instance != null) {             answer = new BeanExpression(instance, getMethod(), parameterType); // <--         } else {             String ref = beanName();             // if its a ref then check that the ref exists             BeanHolder holder = new RegistryBean(camelContext, ref);             // get the bean which will check that it exists             instance = holder.getBean();             answer = new BeanExpression(ref, getMethod(), parameterType);         }          // validate method         validateHasMethod(camelContext, instance, getMethod(), parameterType);          return answer;     } {code}  so that the created BeanExpression is also validate if you provide the bean type or an instance. With this change, some tests in org.apache.camel.language.SimpleTest fails. I'm not sure whether the tests are faulty or if it's a bug. Also not sure whether this should fixed in 2.6.
CAMEL-02626724$$Inconsistent filename value when move attribute is used with File component$$Unless I miss a point, when I use the following endpoint, the file:name value is incorrect and is equal to file:absolute.path  <endpoint id="fileEndpoint" uri="file: {queue.input.folder}?recursive=true&amp;include=.*\.dat&amp;move= {queue.done.folder}/ simple{file:name}&amp;moveFailed= {queue.failed.folder}/ simple{file:name}" />   {queue.input.folder},  {queue.done.folder} and  {queue.failed.folder} are absolute paths resolved by Spring.  In fact, Camel tries to move the file to  {queue.done.folder}/ {queue.input.folder}/ simple{file:name} I've also tried using  simple{header.CamelFileName} instead of  simple{file:name} and it gives the same result.  For now, I've found a workaround using a processor which put the CamelFileName header value into a "destFile" property  <endpoint id="fileEndpoint" uri="file: {queue.input.folder}?recursive=true&amp;include=.*\.dat&amp;move= {queue.done.folder}/ simple{property.destFile}&amp;moveFailed= {queue.failed.folder}/ simple{property.destFile}" />
CAMEL-2a3f3392$$Endpoints may be shutdown twice as they are tracked in two lists in CamelContext$$Endpoint is a Service which means they are listed in both a endpoint and service list. They should only be listed in the endpoint list.  This avoids issues with endpoints may be shutdown twice when Camel shutdown.  See nabble http://camel.465427.n5.nabble.com/QuartzComponent-do-not-delete-quartz-worker-threads-when-shutdown-Camel-tp3393728p3393728.html
CAMEL-4c37e773$$interceptFrom and from(Endpoint) don't work together$$When using interceptFrom(String) together with from(Endpoint), the below Exception occurs during the routes building process. Looking at RoutesDefinition.java:217 reveals, that the FromDefintion just created has no URI. That causes the comparison to all the interceptFroms' URIs to fail. As far as I can tell, the way to fix this would be to add {{setUri(myEndpoint.getEndpointUri())}} in the constructor {{FromDefinition(Endpoint endpoint)}}.  Below the stack trace, there is a unit test that demonstrates the issue. Until it if fixed, it can be easily circumvented by adding the commented-out line, and then change to {{from("myEndpoint")}}. {code} org.apache.camel.ResolveEndpointFailedException: Failed to resolve endpoint: null due to: null 	at org.apache.camel.util.EndpointHelper.matchEndpoint(EndpointHelper.java:109) 	at org.apache.camel.model.RoutesDefinition.route(RoutesDefinition.java:217) 	at org.apache.camel.model.RoutesDefinition.from(RoutesDefinition.java:167) 	at org.apache.camel.builder.RouteBuilder.from(RouteBuilder.java:101) 	at dk.mobilethink.adc2.endpoint.UnsetUriTest 1.configure(UnsetUriTest.java:18) 	at org.apache.camel.builder.RouteBuilder.checkInitialized(RouteBuilder.java:318) 	at org.apache.camel.builder.RouteBuilder.configureRoutes(RouteBuilder.java:273) 	at org.apache.camel.builder.RouteBuilder.addRoutesToCamelContext(RouteBuilder.java:259) 	at org.apache.camel.impl.DefaultCamelContext.addRoutes(DefaultCamelContext.java:612) 	at org.apache.camel.test.CamelTestSupport.setUp(CamelTestSupport.java:111) 	at junit.framework.TestCase.runBare(TestCase.java:132) 	at org.apache.camel.test.TestSupport.runBare(TestSupport.java:65) 	at junit.framework.TestResult 1.protect(TestResult.java:110) 	at junit.framework.TestResult.runProtected(TestResult.java:128) 	at junit.framework.TestResult.run(TestResult.java:113) 	at junit.framework.TestCase.run(TestCase.java:124) 	at junit.framework.TestSuite.runTest(TestSuite.java:232) 	at junit.framework.TestSuite.run(TestSuite.java:227) 	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83) 	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:49) 	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197) Caused by: java.lang.NullPointerException 	at org.apache.camel.util.UnsafeUriCharactersEncoder.encode(UnsafeUriCharactersEncoder.java:56) 	at org.apache.camel.util.URISupport.normalizeUri(URISupport.java:162) 	at org.apache.camel.util.EndpointHelper.matchEndpoint(EndpointHelper.java:107) 	... 24 more {code}  {code} package dk.mobilethink.adc2.endpoint;  import org.apache.camel.Endpoint; import org.apache.camel.builder.RouteBuilder; import org.apache.camel.test.CamelTestSupport;  public class UnsetUriTest extends CamelTestSupport { 	@Override 	protected RouteBuilder createRouteBuilder() throws Exception {  		return new RouteBuilder() { 			public void configure() throws Exception { 				interceptFrom("URI1").to("irrelevantURI");  				Endpoint myEndpoint = getContext().getComponent("direct").createEndpoint("ignoredURI"); 				 //				getContext().addEndpoint("myEndpoint", myEndpoint); 				from(myEndpoint) 					.inOnly("log:foo"); 			} 		}; 	}  	public void testNothing() { } } {code}
CAMEL-ff2713d1$$Recipient list with parallel processing doesn't reuse aggregation threads$$When I'm using recipient list in parallel mode {{aggregateExecutorService}} in {{MulticastProcessor}} doesn't reuse threads and is creating one new thread per each request.  To reproduce this bug simply add a loop to {{RecipientListParallelTest.testRecipientListParallel()}} test: {code:title=RecipientListParallelTest.java|borderStyle=solid}     public void testRecipientListParallel() throws Exception {         for (int i = 0; i < 10000; i++) {             MockEndpoint mock = getMockEndpoint("mock:result");             mock.reset();             mock.expectedBodiesReceivedInAnyOrder("c", "b", "a");             template.sendBodyAndHeader("direct:start", "Hello World", "foo", "direct:a,direct:b,direct:c");             assertMockEndpointsSatisfied();         }     } {code}  In the logs you can find: {code} 2011-02-28 13:22:30,984 [) thread #0 - RecipientListProcessor-AggregateTask] DEBUG MulticastProcessor             - Done aggregating 3 exchanges on the fly. 2011-02-28 13:22:31,984 [) thread #4 - RecipientListProcessor-AggregateTask] DEBUG MulticastProcessor             - Done aggregating 3 exchanges on the fly. 2011-02-28 13:22:32,984 [) thread #8 - RecipientListProcessor-AggregateTask] DEBUG MulticastProcessor             - Done aggregating 3 exchanges on the fly. 2011-02-28 13:22:34,000 [ thread #12 - RecipientListProcessor-AggregateTask] DEBUG MulticastProcessor             - Done aggregating 3 exchanges on the fly. 2011-02-28 13:22:35,000 [ thread #14 - RecipientListProcessor-AggregateTask] DEBUG MulticastProcessor             - Done aggregating 3 exchanges on the fly. 2011-02-28 13:22:36,000 [ thread #15 - RecipientListProcessor-AggregateTask] DEBUG MulticastProcessor             - Done aggregating 3 exchanges on the fly. 2011-02-28 13:22:37,015 [ thread #16 - RecipientListProcessor-AggregateTask] DEBUG MulticastProcessor             - Done aggregating 3 exchanges on the fly. 2011-02-28 13:22:38,015 [ thread #17 - RecipientListProcessor-AggregateTask] DEBUG MulticastProcessor             - Done aggregating 3 exchanges on the fly. {code}
CAMEL-c1b2f2f8$$Auto mock endpoints should strip parameters to avoid confusing when accessing the mocked endpoint$$If you use mocking existing endpoints, which is detailed here http://camel.apache.org/mock.html  We should stip parameters of the mocked endpoint, eg {{file:xxxx?noop=true}}. eg so the mocked endpoint would be {{mock:file:xxxx}} without any of the parameters.  Otherwise the mock endpoint expects those parameters is part of the mock endpoint and will fail creating the mock endpoint.
CAMEL-5225e6e3$$ManagementNamingStrategy - Should normalize ObjectName to avoid using illegal characters$$For example when using JMS in the loanbroaker example. There us a colon in the JMS queue name which is invalid char in JMX.  2011-03-06 08:26:55,859 [main           ] WARN  ManagedManagementStrategy      - Cannot check whether the managed object is registered. This exception will be ignored. javax.management.MalformedObjectNameException: Could not create ObjectName from: org.apache.camel:context=vesta.apache.org/camel-1,type=threadpools,name=JmsReplyManagerTimeoutChecker[queue2:parallelLoanRequestQueue]. Reason: javax.management.MalformedObjectNameException: Invalid character ':' in value part of property 	at org.apache.camel.management.DefaultManagementNamingStrategy.createObjectName(DefaultManagementNamingStrategy.java:315)[camel-core-2.7-SNAPSHOT.jar:2.7-SNAPSHOT]
CAMEL-9319e139$$org.apache.camel.component.file.strategy.MarkerFileExclusiveReadLockStrategy is not thread-safe$$MarkerFileExclusiveReadLockStrategy is not thread-safe. When I run  a File endpoint with more than one thread the MarkerFileExclusiveReadLockStrategy only deletes the last file to start being processed.   The MarkerFileExclusiveReadLockStrategy uses global variables:  private File lock;  private String lockFileName;  and gives them values on the acquireExclusiveReadLock method. When another thread calls the releaseExclusiveReadLock method it uses the global variables to delete the locked file. That means that if another thread came and called the acquireExclusiveReadLock it would have changed the values on the global variables.   If lock and lockFileName are not global variables the problem seems to disappear and I can a multithreaded File endpoint and not locked file is left undeleted.
CAMEL-52106681$$Camel should reset the stream cache if the useOriginalInMessage option is true$${code} --- src/main/java/org/apache/camel/processor/RedeliveryErrorHandler.java	(revision 1083672) +++ src/main/java/org/apache/camel/processor/RedeliveryErrorHandler.java	(working copy) @@ -591,18 +591,23 @@          // is the a failure processor to process the Exchange          if (processor != null) {   -            // reset cached streams so they can be read again -            MessageHelper.resetStreamCache(exchange.getIn()); -              // prepare original IN body if it should be moved instead of current body              if (data.useOriginalInMessage) {                  if (log.isTraceEnabled()) {                      log.trace("Using the original IN message instead of current");                  }                  Message original = exchange.getUnitOfWork().getOriginalInMessage();                  exchange.setIn(original);              }  +            // reset cached streams so they can be read again +            MessageHelper.resetStreamCache(exchange.getIn()); {code}
CAMEL-de9399f3$$Adding type converter should clear misses map for the given type$$See nabble http://camel.465427.n5.nabble.com/addTypeConverter-does-not-clear-misses-in-BaseTypeConverterRegistry-tp4288871p4288871.html
CAMEL-b9094cb5$$Stopping a route should not stop context scoped error handler$$When stopping a route using .stopRoute from CamelContext or JMX etc. then the error handler should not be stopped if its a context scoped error handler, as it would be re-used.  We should defer stopping those resources till Camel is shutting down.
CAMEL-cbffff59$$type converters should return NULL for Double.NaN values instead of 0$$see this discussion...http://camel.465427.n5.nabble.com/XPath-for-an-Integer-td4422095.html  Update the ObjectConverter.toXXX() methods to check for Double.NaN and return NULL instead of relying on Number.intValue()
CAMEL-4efddb3f$$URISupport - Normalize URI should support parameters with same key$$See nabble http://camel.465427.n5.nabble.com/Problems-with-jetty-component-and-posts-with-more-then-one-value-for-a-field-tp4576908p4576908.html  The end user is using jetty producer component to send a HTTP POST/GET to some external client. In the endpoint uri he have the parameters, and there are 2 times {{to}} as parameter key. Currently Camel loses the 2nd {{to}} parameter.
CAMEL-96e40c3c$$header added using an EventNotifier is not present at AggregationStrategy for http endpoints$$A new header added using an EventNotifier is not present when the exchange is aggregated with an AggregationStrategy. This is happening only if the enpoint type is http, ftp doesn't have this issue.  This was working with an early version of 2.8.0-SNAPSHOT  Following the EventNotifier code used.  {code:title=ExchangeSentEventNotifier.java|borderStyle=solid} public class ExchangeSentEventNotifier extends EventNotifierSupport {  	@Override 	protected void doStart() throws Exception {         /*          *  filter out unwanted events          *  we are interested only in ExchangeSentEvent          */         setIgnoreCamelContextEvents(true);         setIgnoreServiceEvents(true);         setIgnoreRouteEvents(true);         setIgnoreExchangeCreatedEvent(true);         setIgnoreExchangeCompletedEvent(true);         setIgnoreExchangeFailedEvents(true);         setIgnoreExchangeSentEvents(false);		 	}  	@Override 	protected void doStop() throws Exception {  	}  	@Override 	public boolean isEnabled(EventObject event) { 		return event instanceof ExchangeSentEvent; 	}  	@Override 	public void notify(EventObject event) throws Exception {     	if(event.getClass() == ExchangeSentEvent.class){             ExchangeSentEvent eventSent = (ExchangeSentEvent)event;                          log.debug("Took " + eventSent.getTimeTaken() + " millis to send to: " + eventSent.getEndpoint());              //storing time taken to the custom header                         eventSent.getExchange().getIn().setHeader("x-time-taken", eventSent.getTimeTaken());                  	} 		 	}  } {code}
CAMEL-7345fefc$$It's hardly possible to use all expression of the Simple language to create file names in the file component$$Sometimes it can be necessary to use custom headers to create a file name.  For example, I declare my file endpoint in the following manner:  {code} <route id="fileReader">     <from uri="file://rootFolder?move=.backup&amp;moveFailed=.error/ {header.CustomHeader}" />     <to uri="file://out"/> </route> {code}  The header "CustomHeader" cannot be read because of the following snippets of code in the org.apache.camel.component.file.GenericFile  {code} /**  * Bind this GenericFile to an Exchange  */ public void bindToExchange(Exchange exchange) {     exchange.setProperty(FileComponent.FILE_EXCHANGE_FILE, this);     GenericFileMessage<T> in = new GenericFileMessage<T>(this);     exchange.setIn(in);     populateHeaders(in); }  /**  * Populates the {@link GenericFileMessage} relevant headers  *  * @param message the message to populate with headers  */ public void populateHeaders(GenericFileMessage<T> message) {     if (message != null) {         message.setHeader(Exchange.FILE_NAME_ONLY, getFileNameOnly());         message.setHeader(Exchange.FILE_NAME, getFileName());         message.setHeader("CamelFileAbsolute", isAbsolute());         message.setHeader("CamelFileAbsolutePath", getAbsoluteFilePath());          if (isAbsolute()) {             message.setHeader(Exchange.FILE_PATH, getAbsoluteFilePath());         } else {             // we must normalize path according to protocol if we build our own paths             String path = normalizePathToProtocol(getEndpointPath() + File.separator + getRelativeFilePath());             message.setHeader(Exchange.FILE_PATH, path);         }          message.setHeader("CamelFileRelativePath", getRelativeFilePath());         message.setHeader(Exchange.FILE_PARENT, getParent());          if (getFileLength() >= 0) {             message.setHeader("CamelFileLength", getFileLength());         }         if (getLastModified() > 0) {             message.setHeader(Exchange.FILE_LAST_MODIFIED, new Date(getLastModified()));         }     } } {code}  As you can see a new "in" message is created and not all the headers from the original message are copied to it.
CAMEL-f39bc60d$$Exeptions cannot be propagated to the parent route when using LogEIP$$Here is unit test that demonstrates the problem. For the unit test pass successfully it's necessary to delete LogEIP from the route.  {code} package org.apache.camel.impl;  import org.apache.camel.Exchange; import org.apache.camel.Processor; import org.apache.camel.builder.RouteBuilder; import org.apache.camel.test.junit4.CamelTestSupport; import org.junit.Test;  public class PropagateExceptionTest extends CamelTestSupport {      @Test     public void failure() throws Exception {         getMockEndpoint("mock:handleFailure").whenAnyExchangeReceived(new Processor() {             @Override             public void process(Exchange exchange) throws Exception {                 throw new RuntimeException("TEST EXCEPTION");             }         });          getMockEndpoint("mock:exceptionFailure").expectedMessageCount(1);         sendBody("direct:startFailure", "Hello World");         assertMockEndpointsSatisfied();     }      @Test     public void success() throws Exception {         getMockEndpoint("mock:handleSuccess").whenAnyExchangeReceived(new Processor() {             @Override             public void process(Exchange exchange) throws Exception {                 throw new RuntimeException("TEST EXCEPTION");             }         });          getMockEndpoint("mock:exceptionSuccess").expectedMessageCount(1);         sendBody("direct:startSuccess", "Hello World");         assertMockEndpointsSatisfied();     }      @Override     protected RouteBuilder[] createRouteBuilders() throws Exception {         return new RouteBuilder[] {                 new RouteBuilder() {                     public void configure() throws Exception {                         from("direct:startFailure")                             .onException(Throwable.class)                                 .to("mock:exceptionFailure")                                 .end()                             .to("direct:handleFailure")                             .to("mock:resultFailure");                          from("direct:handleFailure")                             .errorHandler(noErrorHandler())                             .log("FAULTY LOG")                             .to("mock:handleFailure");                     }                 },                  new RouteBuilder() {                     public void configure() throws Exception {                         from("direct:startSuccess")                             .onException(Throwable.class)                                 .to("mock:exceptionSuccess")                                 .end()                             .to("direct:handleSuccess")                             .to("mock:resultSuccess");                          from("direct:handleSuccess")                             .errorHandler(noErrorHandler())                             .to("mock:handleSuccess");                     }                 }         };     } } {code}
CAMEL-79168a23$$LifecycleStrategy should be started/stopped when CamelContext is starting/stopping$$The LifecycleStrategy strategies is not start/stopped if they are a Service, such as the DefaultManagementLifecycleStrategy
CAMEL-06a8489a$$file: consumer does not create directory$$According to http://camel.apache.org/file2.html autoCreate is true by default and should for a consumer create the directory. {noformat} autoCreate 	true 	Automatically create missing directories in the file's pathname. For the file consumer, that means creating the starting directory. For the file producer, it means the directory the files should be written to.  {noformat} This does not happen and thus a route startup would fail.
CAMEL-e38494f1$$Using custom expression in Splitter EIP which throws exception, is not triggering onException$$See nabble http://camel.465427.n5.nabble.com/Global-exception-not-invoked-in-case-of-Exception-fired-while-iterating-through-File-Splitter-td4826097.html  We should detect exceptions occurred during evaluation of the expression, and then cause the splitter EIP to fail asap.
CAMEL-f98ac676$$Exceptions are not propagated to the parent route when endpoint cannot be resolved in the RoutingSlip EIP$$Here is the unit test to reproduce the issue  {code} package org.test;  import org.apache.camel.builder.RouteBuilder; import org.apache.camel.component.mock.MockEndpoint; import org.apache.camel.test.junit4.CamelTestSupport; import org.junit.Test;  public class RecipientListTest extends CamelTestSupport {      public static class Router {         public String findEndpoint() {             return "unresolved://endpoint";         }     }      @Test     public void recipientList() throws Exception {         MockEndpoint endpoint = getMockEndpoint("mock://error");         endpoint.expectedMessageCount(1);          sendBody("direct://parent", "Hello World!");          assertMockEndpointsSatisfied();     }      @Override     protected RouteBuilder createRouteBuilder() throws Exception {         return new RouteBuilder() {             @Override             public void configure() throws Exception {                 from("direct://parent")                     .onException(Throwable.class)                         .to("mock://error")                     .end()                     .to("direct://child");                  from("direct://child")                     .errorHandler(noErrorHandler())                     .routingSlip(bean(Router.class));             }         };     }  } {code}
CAMEL-8e3450f4$$Header not set after dead letter queue handles unmarshal error$$We have a route which unmarshals a soap msg into an object.  On that route is a dead letter queue error handler.  That DLQ sets headers on the message used later for error reporting.  If the error is thrown by the marshaller, the *first header* that we try to set is wiped out.  The 2nd header is set with no problem.  If an error is thrown by something other than the marshaller, the correct headers are set.  See attached project with failed test case (canSetHeadersOnBadXmlDeadLetter)
CAMEL-9e05f77f$$simple predicate fails to introspect the exception in an onException clause using onWhen$$The bug occured in the 2.6.0 version of Camel I'm using. I haven't test it against the latest version but I've checked the sources and it doesn't seem to have change since.  Given a camel route, with a onException clause like this :  {code} this.onException(MyException.class)     .onWhen(simple(" {exception.myExceptionInfo.aValue} == true"))     ... {code}  MyException is a customed exception like this :  {code:title=MyException.java} public class MyException extends Exception {    ....    public MyExceptionInfo getMyExceptionInfo() {      ...    } } {code}  What I've observed is that when BeanExpression.OgnlInvokeProcessor.process iterate through the methods to calls, it does : {code}                 // only invoke if we have a method name to use to invoke                 if (methodName != null) {                     InvokeProcessor invoke = new InvokeProcessor(holder, methodName);                     invoke.process(resultExchange);                      // check for exception and rethrow if we failed                     if (resultExchange.getException() != null) {                         throw new RuntimeBeanExpressionException(exchange, beanName, methodName, resultExchange.getException());                     }                      result = invoke.getResult();                 } {code}  It successfully invoke the method : invoke.process(resultExchange); But it checks for exception in the exchange. Since we are in an exception clause, there is an actual exception (thrown by the application, but unrelated with the expression language search) and it fails  There is a simple workaround for that : writing his own predicate class to test wanted conditions
CAMEL-df9f4a6a$$Using AuthorizationPolicy on a Route prevents Processors from being exposed via JMX$$Using AuthorizationPolicy on a route (e.g., using .policy(myAuthPolicy) in a Java DSL) prevents that processors on this route are exposed via JMX.   Steps to reproduce:  -) Start the Camel app in the attached test case (MyRouteBuilder) -) Open JConsole -) Connect to the corresponding local process -) Under "processors" only the processors from the route without the policy are shown, but not the ones from the route where a policy is used
CAMEL-c408c3ed$$Can't find splitter bean in registry using multiple camel contexts with "vm" endpoint$$The splitter component can use a bean with a "split method". It seems that this "split bean" is handled as expression and resolved lately using Camel Context from current exchange.  If I send an exchange using a separate CamelContext ("client")  <camelContext id="client" xmlns="http://camel.apache.org/schema/spring"> </camelContext>  to a route defined in another CamelContext ("server") using in-memory transport like "direct" or "vm"  <camelContext id="server" xmlns="http://camel.apache.org/schema/spring">     <route id="route02" trace="false" streamCache="false">      <from uri="vm:route02"/>      <split>        <method bean   ="stringLineSplitter" method="split"/>        <log    message="before sending:  {body}"/>        <inOut  uri    ="vm:route04"/>        <log    message="after sending"/>      </split>      <to uri="mock:route02"/>    </route>  </camelContext>  the test fails with   "Cannot find class: stringLineSplitter" (Camel 2.8.0).  "org.apache.camel.NoSuchBeanException - No bean could be found in the registry for: stringLineSplitter" (Camel 2.9-SNAPSHOT)  If I understood Camel right it fails because it tries to resolve this bean based on client Camel Context which is still set at the current exchange send from "client" to "server" but it doesn't contain the bean.  If I send an exchange using same "client" CamelContext to another route in "server" CamelContext involving "external" components like "jms" (ActiveMQ)  <camelContext id="server" xmlns="http://camel.apache.org/schema/spring">     <route id="route03" trace="false" streamCache="false">      <from uri="jms:queue:route03"/>      <split>        <method bean   ="stringLineSplitter" method="split"/>        <log    message="before sending:  {body}"/>        <inOut  uri    ="vm:route04"/>        <log    message="after sending"/>      </split>      <to uri="mock:route03"/>    </route>  </camelContext>  the test passed successfully. It seems that "jms" component creates a new exchange using "server" CamelContext.
CAMEL-1e54865c$$When stopping CamelContext should not clear lifecycleStrategies, to make restart safely possible$$We should not clear the lifecycleStrategies on CamelContext when stop() is invoked, as if we restart by invoking start(), the lifecycle strategies should be in use again.
CAMEL-afa1d132$$Timer component does not suspend$$A route which begins with a Timer consumer does not suspend the consumer when the route is suspended.
CAMEL-8898d491$$bean component - @Handler should take precedence in a bean that implements Predicate$$If you use a bean in a Camel route, and have not specified the method name to invoke. Then Camel has to scan for suitable methods to use. And for that we have the @Handler annotation which should take precedence in this process. However if the bean implements Predicate, or Processor, then Camel will use that. However the @Handler should be used instead, as this is what the end-user expects. And also what we tell in the docs.
CAMEL-a8586a69$$Simple language - OGNL - Invoking explicit method with no parameters should not cause ambiguous exception for overloaded methods$$If you want to invoke a method on a bean which is overloaded, such as a String with toUpperCase having - toUpperCase() - toUpperCase(Locale)  Then if you specify this in a simple ognl expression as follows {code}  {body.toUpperCase()} {code}  Then Camel bean component should pick the no-parameter method as specified.
CAMEL-8cadc344$$JMX issues on WebSphere$$While setting up a Camel web application for WebSphere (7) I encountered two issues  1. Documentation: the Camel JMX docs proposes the following settings for WebSphere: {code} <camel:jmxAgent id="agent" createConnector="true" mbeanObjectDomainName="org.yourname" mbeanServerDefaultDomain="WebSphere"/> {code}  This registers the beans with the PlatformMbeanServer instead of the WebSphere MBean server. The following setup works better: {code} <camel:jmxAgent id="agent" createConnector="false" mbeanObjectDomainName="org.yourname" usePlatformMBeanServer="false" mbeanServerDefaultDomain="WebSphere"/> {code}  2. For each Camel route, the same Tracer and DefaultErrorHandler MBeans are tried to be registered over and over again. Because WebSphere changes the ObjectNames on registration,   {{server.isRegistered(name);}} in {{DefaultManagementAgent#registerMBeanWithServer}} always returns false, which causes the MBean to be re-registered, which again cause Exceptions, e.g.  {code} 14:35:48,198 [WebContainer : 4] [] WARN  - DefaultManagementLifecycleStrategy.onErrorHandlerAdd(485) | Could not register error handler builder: ErrorHandlerBuilderRef[CamelDefaultErrorHandlerBuilder] as ErrorHandler MBean. javax.management.InstanceAlreadyExistsException: org.apache.camel:cell=wdf-lap-0319Node01Cell,name="DefaultErrorHandlerBuilder(ref:CamelDefaultErrorHandlerBuilder)",context=wdf-lap-0319/camelContext,type=errorhandlers,node=wdf-lap-0319Node01,process=server1 	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:465) 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1496) 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:975) 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:929) 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324) 	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:494) 	at com.ibm.ws.management.PlatformMBeanServer.registerMBean(PlatformMBeanServer.java:484) 	at org.apache.camel.management.DefaultManagementAgent.registerMBeanWithServer(DefaultManagementAgent.java:320) 	at org.apache.camel.management.DefaultManagementAgent.register(DefaultManagementAgent.java:236) ... {code}  The web application starts up, but with a lot of exceptions in the log.  Proposal: Instead of using a Set<ObjectName> for mbeansRegistered, use a Map<ObjectName, ObjectName> where the key is the "Camel" ObjectName and the value is the actually deployed ObjectName.  I will provide a patch that illustrates the idea.
CAMEL-033eb6fe$$The file producer should use the charset encoding when writing the file if configured$$When writing to a file, we offer the charset option on the endpoint, as well the charset property set on the exchange. However in a route  that is optimized as {code} from file  to file {code}  Then we optimize to do a file move operation instead. We should detect the charset configured and then we would need to stream and write using the configured charset.
CAMEL-2db5570f$$The done file got deleted, when using the file component even if noop property set to true$$We are consuming a feed from a mounted windows network drive, where we have rw access. During the download we shouldn't touch anything so other users see the directory intact.  However even if we turn noop=true the done file got deleted after successfull conumptions
CAMEL-55c2e2d8$$SEDA/VM requires completely same URI on producer and consumer side when consumer route is adviced$$The producer side and consumer side of the SEDA (and VM) component seems to require the completely same URI to be able to communicate. Completely same meaning that all URI options must be the same on both sides. The strange thing is that this only is required when I have adviced the consumer route. 2.9.0 does not have this problem.  Attached a unit test - the producerWithDifferentUri will fail on 2.9.1 and 2.9.2. If the advice is removed it will not.
CAMEL-4cf7e80e$$URI normalization - Should detect already percent encoded values$$If an uri has a percent encoded value, eg using %20, %25 etc, then the normalization logic in Camel should detect this and keep the value as is.  Currently it would end up double encoding %25, that becomes %2525, and so forth.  Its the code in UnsafeUriCharactersEncoder that has the bug
CAMEL-93935780$$Dynamically added SEDA-route is not working$$Dynamically removing and adding a SEDA-route creates a not working route in Camel 2.10.0. It is working in 2.9.2.  Test-Code: {code} public class DynamicRouteTest extends CamelTestSupport {      @Override     protected RouteBuilder createRouteBuilder() throws Exception {         return new RouteBuilder() {              @Override             public void configure() throws Exception {                 from("seda:in").id("sedaToMock").to("mock:out");             }         };     }          @Test     public void testDynamicRoute() throws Exception {         MockEndpoint out = getMockEndpoint("mock:out");         out.expectedMessageCount(1);                  template.sendBody("seda:in", "Test Message");                  out.assertIsSatisfied();                  CamelContext camelContext = out.getCamelContext();         camelContext.stopRoute("sedaToMock");         camelContext.removeRoute("sedaToMock");                  camelContext.addRoutes(createRouteBuilder());         out.reset();         out.expectedMessageCount(1);                  template.sendBody("seda:in", "Test Message");                  out.assertIsSatisfied();              } }   {code}
CAMEL-da05f5aa$$Add support for batch consumer's empty messages to aggregator$$Aggregator supports completion based on the batch consumer data (option completionFromBatchConsumer)  Some batch consumers (eg. File) can send an empty message if there is no input (option sendEmptyMessageWhenIdle for File consumer).  Aggregator is unable to handle such messages properly - the messages are aggregated, but Aggregator never completes.   Here is the relevant fragment from AggregateProcessor.isCompleted(String, Exchange)  int size = exchange.getProperty(Exchange.BATCH_SIZE, 0, Integer.class); if (size > 0 && batchConsumerCounter.intValue() >= size) {     .... }   Please add support for this combination of options.
CAMEL-b3bb8670$$thread java DSL doesn't provide full function out of box$$thread() doesn't has extra parameter for setting the thread name, and the other thread() method doesn't add the output process rightly.
CAMEL-a57830ed$$maximumRedeliveries is inherited for other exceptions thrown while redelivering with maximumRedeliveries(-1)$$Given a route:  {code} from("direct:source")    .onException(FirstException.class)          .handled(true)          .maximumRedeliveries(-1)     .end()     .onException(SecondException.class)         .handled(true)         .to("direct:error")     .end()     .to("direct:destination"); {code}  If the consumer of direct:destination throws a FirstException, the message will be redelivered. Now if a SecondException is thrown while redelivering the message to direct:destination, it does NOT go to direct:error, as you would expect, but is redelivered again; using the same RedeliveryPolicy as for FirstException.  I have attached a test that illustrates this.  In OnExceptionDefinition.createRedeliveryPolicy, maximumRedeliveries is set to 0 if the OnExceptionDefinition has outputs and the parent RedeliveryPolicy has explicitly set maximumRedeliveries > 0. The latter check fails when maximumRedeliveries is -1 (infinite retries), and the parent RedeliveryPolicy is returned.  I have attached a patch that ensures that we don't inherit the parent maximumRedeliveries even if it is set to -1.
CAMEL-0e87b84f$$Camel proxies should not forward hashCode() method invocations$$Given a Camel proxy for an @InOnly service interface, and a route from the proxy to a JMS endpoint, calling hashCode() on the proxy throws an exception, either immediately or after a number of retries, depending on the route configuration.  See the attached test case for different scenarios.  The reason is that hashCode() is forwarded by the CamelInvocationHandler to the remote endpoint, which does not make sense in this case.
CAMEL-15d0fd9b$$Bean component - Should use try conversion when choosing method based on parameter type matching$$When the bean component has to pick among overloaded methods, then it does best matching on parameter types etc.  We should relax the type conversion to try attempt.
CAMEL-78c73502$$Using recipient list in a doTry ... doCatch situation dont work properly$$See nabble http://camel.465427.n5.nabble.com/Issue-with-doTry-doCatch-not-routing-correctly-tp5720325.html  The end user would expect that doTry .. doCatch will overrule. However it gets a bit further more complicated if the try block routes to other routes and using EIPs such as recipient list.
CAMEL-0c3c7d1b$$JMS connection leak with request/reply producer on temporary queues$$Over time I see the number of temporary queues in ActiveMQ slowly climb. Using JMX information and memory dumps in MAT, I believe the cause is a connection leak in Apache Camel.  My environment contains 2 ActiveMQ brokers in a network of brokers configuration. There are about 15 separate applications which use Apache Camel to connect to the broker using the ActiveMQ/JMS component. The various applications have different load profiles and route configurations.  In the more active client applications, I found that ActiveMQ was listing 300+ consumers when, based on my configuration, I would expect no more than 75. The vast majority of the consumers are sitting on a temporary queue. Over time, the 300 number increments by one or two over about a 4 hour period.  I did a memory dump on one of the more active client applications and found about 275 DefaultMessageListenerContainers. Using MAT, I can see that some of the containers are referenced by JmsProducers in the ProducerCache; however I can also see a large number of listener containers that are no longer being referenced at all. I was also able to match up a soft-references producer/listener endpoint with an unreferenced listener which means a second producer was created at some point.  Looking through the ProducerCache code, it looks like the LRU cache uses soft-references to producers, in my case a JmsProducer. This seems problematic for two reasons: - If memory gets constrained and the GC cleans up a producer, it is never properly stopped. - If the cache gets full and the map removes the LRU producer, it is never properly stopped.  What I believe is happening, is that my application is sending a few request/reply messages to a JmsProducer. The producer creates a TemporaryReplyManager which creates a DefaultMessageListenerContainer. At some point, the JmsProducer is claimed by the GC (either via the soft-reference or because the cache is full) and the reply manager is never stopped. This causes the listener container to continue to listen on the temporary queue, consuming local resources and more importantly, consuming resources on the JMS broker.  I haven't had a chance to write an application to reproduce this behavior, but I will attach one of my route configurations and a screenshot of the MAT analysis looking at DefaultMessageListenerContainers. If needed, I could provide the entire memory dump for analysis (although I rather not post it publicly). The leak depends on memory usage or producer count in the client application because the ProducerCache must have some churn. Like I said, in our production system we see about 12 temporary queues abandoned per client per day.  Unless I'm missing something, it looks like the producer cache would need to be much smarter to support stopping a producer when the soft-reference is reclaimed or a member of the cache is ejected from the LRU list.
CAMEL-6d63a502$$LogFormatter throws a NPE when all elements are disabled$$There are perfectly valid cases where you may want to output a log message with no elements displayed, i.e. with showExchangeId=false, showBody=false, etc.  For example, when you want to print a "signal" log line for a particular transaction and you're already using MDC logging with breadcrumbs enabled. You may already have all the info you need: logging category, severity, breadcrumbId. You are not interested in anything else.  Currently, disabling all elements leads to a NPE.
CAMEL-708e756d$$Split inside Split - Parallel processing issue - Thread is getting wrong Exchange when leaving inner split$$A small JUnit recreation case is attached. When using embedded split inside a split with parallel processing, threads are getting a wrong exchange (or wrong exchange copy) just after leaving the inner split and returning to the parent split.  In the test case, we split a file by comma in a parent split (Block split), then by line separator in inner split (Line Split).  We expect 2 files in output, each of them containing the respective Blocks.  However, once inner split is complete, each thread is supposed to add a 11th line in the result(i).txt file saying split(i) is complete.   Bug is that one of the thread ends up with parent split Exchange (copy?) from the other thread, and appends wrong information into the wrong file.  Expected: --------- (result0.txt) Block1 Line 1:Status=OK Block1 Line 2:Status=OK Block1 Line 0:Status=OK Block1 Line 4:Status=OK Block1 Line 3:Status=OK Block1 Line 8:Status=OK Block1 Line 5:Status=OK Block1 Line 6:Status=OK Block1 Line 7:Status=OK Block1 Line 9:Status=OK 0 complete  (result1.txt) Block2 Line 0:Status=OK Block2 Line 3:Status=OK Block2 Line 1:Status=OK Block2 Line 2:Status=OK Block2 Line 6:Status=OK Block2 Line 4:Status=OK Block2 Line 7:Status=OK Block2 Line 9:Status=OK Block2 Line 5:Status=OK Block2 Line 8:Status=OK 1 complete  Actual: ------- (result0.txt) Block1 Line 1:Status=OK Block1 Line 2:Status=OK Block1 Line 0:Status=OK Block1 Line 4:Status=OK Block1 Line 3:Status=OK Block1 Line 8:Status=OK Block1 Line 5:Status=OK Block1 Line 6:Status=OK Block1 Line 7:Status=OK Block1 Line 9:Status=OK 0 complete0 complete  (result1.txt) Block2 Line 0:Status=OK Block2 Line 3:Status=OK Block2 Line 1:Status=OK Block2 Line 2:Status=OK Block2 Line 6:Status=OK Block2 Line 4:Status=OK Block2 Line 7:Status=OK Block2 Line 9:Status=OK Block2 Line 5:Status=OK Block2 Line 8:Status=OK   This issue exist in 2.8.x, and probably in 2.10.x as well. This is a Splitter/MulticastProcessor or Pipeline issue but not quite familiar with the code, I am having hard time tracking it.
CAMEL-3f70d612$$NotifyBuilder should be thread safe$$In high concurrent tests the NotifyBuilder may miss a counter.
CAMEL-4a05eccf$$Aggregate EIP - Dynamic completion size should override fixed values if in exchange$$See nabble http://camel.465427.n5.nabble.com/Bug-with-completionSize-on-AggregatorProcessor-tp5721307.html
CAMEL-de6dd425$$The combination of the transacted DSL together with the <setHeader> or <setBody> prohibits to resolve the properties properly.$$Given the property {{myKey}} defined as: {code} myKey=myValue {code}  Then consider the following trivial route: {code:xml} <route>   <from uri="activemq:queue:okay" />     <transacted />     <setHeader headerName="myHeader">       <constant>{{myKey}}</constant>     </setHeader>   <to uri="mock:test" /> </route> {code}  Because of the usage of the {{transacted}} DSL the property placeholder {{{{myKey}}}} will not be resolved to {{myValue}} properly. This behaviour would disappear if you would remove the {{transacted}} DSL. And I'm observing the same behaviour using the {{setBody}} DSL as well.
CAMEL-a04674f2$$Apache Camel 2.9 Splitter with tokenize dont work with namespaces$$when trying to tokenize a stream having namespaces, no tokens are produced with inheritNamespaceTagName property.  -------------------------------------------------------------------  <route id="hrp.connectorsCtxt.sddRcvFile2"> <from                            uri="file:C:\Temp\esb\sdd\in?recursive=true&amp;preMove=.processing&amp;move=../.processed" />                     <camel:split streaming="true">                            <tokenize token="suiviDemande" inheritNamespaceTagName="suivisDemandes" xml="true"/>                            <log message=" {header.CamelSplitIndex} :  {in.body}" />                     </camel:split>              </route>  -------------------------------------------------------------------
CAMEL-e775071b$$Camel Tracer not showing some EIP names$$In order to debug Camel routes, I have enabled the Tracer as follows:         getContext().setTracing(true);  However, I have observed that some EIP names and routes are not being printed on console, making it a bit confusing to follow. As far as I know, this happens with: * process(): the processor is not printed in the tracer; it's just empty (see below) * marshall(): the marshaller name is not printed in the tracer; it's just empty (see below) * setBody(): this step is also printed empty * from("activiti:..."): this route step is not printed altogether  For simplicity, I only provide the examples for process() and marshall(), bit I can provide more information if needed.  {panel:title=Route2 Config} from("vm:processIncomingOrders")   .process(new IncomingOrdersProcessor())   .split(body())	// iterate list of Orders   .to("log:incomingOrder1?showExchangeId=true")   .process(new ActivitiStarterProcessor())   .to("log:incomingOrder2?showExchangeId=true")			   .to("activiti:activiti-camel-example"); {panel}  {panel:title=Route2 Tracer} INFO  03-12 12:09:31,899 (MarkerIgnoringBase.java:info:96)  -ID-ES-CNU2113RXH-51211-1354532898719-0-3 >>> (route2) from(vm://processIncomingOrders) -->  <<< Pattern:InOnly, [...] INFO  03-12 12:09:34,899 (IncomingOrdersProcessor.java:process:39)  -Processing incoming orders (from Web Services) [ORDER id:120 partName: wheel amount: 2 customerName: Honda Mechanics] [ORDER id:121 partName: engine amount: 4 customerName: Volvo] [ORDER id:122 partName: steering wheel amount: 3 customerName: Renault] INFO  03-12 12:09:34,900 (MarkerIgnoringBase.java:info:96)  -ID-ES-CNU2113RXH-51211-1354532898719-0-3 >>> (route2)  --> split[body] <<< Pattern:InOnly, [...] {panel}    {panel:title=Route6 config} from("direct:ordercsv")   .marshal().bindy(BindyType.Csv, "net.atos.camel.entities")   .to("file:d://cameldata/orders?fileName=orders- \{date:now:yyyyMMdd-hhmmss}.csv"); {panel}  {panel:title=Route6 Tracer} INFO  03-12 12:09:37,313 (MarkerIgnoringBase.java:info:96)  -ID-ES-CNU2113RXH-51211-1354532898719-0-8 >>> (route6) direct://ordercsv -->  <<< Pattern:InOnly, [...] INFO  03-12 12:09:37,320 (MarkerIgnoringBase.java:info:96)  -ID-ES-CNU2113RXH-51211-1354532898719-0-8 >>> (route6)  --> file://d://cameldata/orders?fileName=orders-%24%7Bdate%3Anow%3AyyyyMMdd-hhmmss%7D.csv <<< Pattern:InOnly,  [...] {panel}
CAMEL-020c451a$$endChoice() has no effect in nested choice definition$$I just upgraded from 2.10.4 to 2.11.0 and noticed that nested choice definitions started acting strangely. For example:  {code:java}             .choice()                 .when(header(Exchange.EXCEPTION_CAUGHT).isNotNull())                     // 1                     .setBody(exceptionMessage().append(SystemUtils.LINE_SEPARATOR).append(exceptionStackTrace()))                     .choice()                         .when(header(HEADER_CONTROLLER_ID).isNotNull())                             // 1a                             .setHeader(Exchange.FILE_NAME, simple(AUDIT_CONTROLLER_FAILED_FILENAME + ".error.log"))                             .to(ENDPOINT_AUDIT_DIR)                         .otherwise()                             // 1b                             .setHeader(Exchange.FILE_NAME, simple(AUDIT_FAILED_FILENAME + ".error.log"))                             .to(ENDPOINT_AUDIT_DIR)                             // INSERTING .end() here solves the issue                         .endChoice()                     .log(LoggingLevel.WARN, "DLQ written:  {in.header.CamelExceptionCaught}"                 .otherwise()                     // 2                     .log(LoggingLevel.WARN, "DLQ written" + MESSAGE_LOG_FORMAT)                 .end() {code}  I have a test that is supposed to go through 1 and 1a. However it now passes through 1 and 2! It looks like the endChoice() in 1b has no effect and the otherwise() in 2 is executed instead of 1b. Inserting and end() statement as shown seems to solve the issue, but it looks suspicious.  It's probably a regression introduced by the fix for CAMEL-5953, but I'm not 100% sure.
CAMEL-2c5a42db$$AbstractListAggregationStrategy does not work with batch completion strategy$$When my aggregator extends AbstractListAggregationStrategy, I never get aggregator completions from the batch consumer.  If I change my aggregator to be something like:  {code}     Foo foo = newExchange.getIn().getBody(Foo.class);     List<Foo> list = null;     Exchange outExchange;     if (oldExchange == null) {       list = new LinkedList<Foo>();       list.add(foo);       newExchange.getIn().setBody(list);       outExchange = newExchange;     } else {       list = oldExchange.getIn().getBody(List.class);       list.add(foo);       outExchange = oldExchange;     }     return outExchange; {code}  then it works fine.  I'm guessing this is has something to do with AbstractListAggregationStrategy messing with properties or wrapping the actual exchanges (since the batch completion is triggered based on Exchange.BATCH_SIZE property)
CAMEL-7f8a295a$$Predicates from java dsl are not dumped to xml correctly$$Predicates defined in the java dsl are not dumped to xml when using jmx.  Eg, this java dsl route: {code} from("seda:a").choice().when(header("test").isNotNull()).log("not null").end().to("mock:a"); {code}  Will be dumped as this: {code} <?xml version="1.0" encoding="UTF-8" standalone="yes"?> <route group="com.example.TestRoute" id="route1" xmlns="http://camel.apache.org/schema/spring">     <from uri="seda:a"/>     <choice id="choice23">         <when id="when24">             <expressionDefinition/>             <log message="not null" id="log20"/>         </when>     </choice>     <to uri="mock:a" id="to17"/> </route> {code}  The <expressionDefinition> element should contain the expression.  This seems similar to CAMEL-4733.
CAMEL-4209fabb$$Routing slip and dynamic router EIP - Stream caching not working$$See nabble http://camel.465427.n5.nabble.com/stream-caching-to-HTTP-end-point-tp5736608.html
CAMEL-55751402$$Routing slip and dynamic router EIP - Stream caching not working$$See nabble http://camel.465427.n5.nabble.com/stream-caching-to-HTTP-end-point-tp5736608.html
CAMEL-2d7051ed$$Tokenize XML does not support child elements with names similar to their parent$$This XML will not split on Trip, as Trip has a child which starts with Trip <Trip> <Triptype> </Triptype> </Trip>  The bug was introduced in https://issues.apache.org/jira/browse/CAMEL-6004 I believe the regex in TokenXMLExpressionIterator needs to be fixed  see enclosed test
CAMEL-ed7e7c9f$$Always got IndexOutOfBoundsException when customized id of wireTap component$$when I'm tring to execute below route: {code} from("timer:foo").wireTap("direct:a").id("wiretap_1").to("log:a"); from("direct:a").to("log:b"); {code} I always got IndexOutOfBoundsException: {color:red} Exception in thread "main" java.lang.IndexOutOfBoundsException: Index: -1 	at java.util.Collections EmptyList.get(Collections.java:3212) 	at org.apache.camel.model.ProcessorDefinition.id(ProcessorDefinition.java:1025) 	at org.talend.esb.liugang.camel.wiretap.TestWiretap 1.configure(TestWiretap.java:14) 	at org.apache.camel.builder.RouteBuilder.checkInitialized(RouteBuilder.java:322) 	at org.apache.camel.builder.RouteBuilder.configureRoutes(RouteBuilder.java:276) 	at org.apache.camel.builder.RouteBuilder.addRoutesToCamelContext(RouteBuilder.java:262) 	at org.apache.camel.impl.DefaultCamelContext.addRoutes(DefaultCamelContext.java:650) 	at org.talend.esb.liugang.camel.wiretap.TestWiretap.main(TestWiretap.java:10) {color} I tried on 2.11.1, 2.11.2-SNAPSHOT, both of them have the same problem (not sure 2.12-SNAPSHOT).
CAMEL-1fc7bd7a$$Loop EIP doesn't honour copy option in some circumstances$$Happens when the Async Routing Engine variant of the Loop logic kicks in, and there are more than two processors in the loop body, e.g.  \\ \\ {code:java} .loop(3)   .to("activemq:queue:abc?exchangePattern=InOut")   .to("activemq:queue:def?exchangePattern=InOut") .end() {code}  The wrong inflight Exchange is copied (instead of the original one), and since the implicit Pipeline has copied the OUT message from the 1st endpoint to the IN message, the original IN message is lost fully.
CAMEL-617eab1c$$Using simple language OGNL expressions doesn't work for Bean Binding when a field is null$$The following functionality doesn't work, when one of the fields is null:   http://camel.apache.org/bean-binding.html {quote} You can also use the OGNL support of the Simple expression language. Now suppose the message body is an object which has a method named asXml. To invoke the asXml method we can do as follows: {code}.bean(OrderService.class, "doSomething( {body.asXml},  {header.high})"){code}  Instead of using .bean as shown in the examples above, you may want to use .to instead as shown: {code}.to("bean:orderService?method=doSomething( {body.asXml},  {header.high})"){code} {quote}  A test case is provided. Instead of getting values of fields "foo" and "bar" respectively, the first parameter (which should be null) receives value of pojo.toString(), while the second parameter receives the correct value.
CAMEL-b92d6237$$Message history - Possible ArrayIndexOutOfBoundsException$$None
CAMEL-745a85ab$$Using @Simple (or others) bean parameter binding for boolean type should eval as predicate$$For example {code}         public void read(String body, @Simple(" {header.foo} != null") boolean foo) { {code}  The foo parameter is a boolean and thus the @Simple expression should be evaluated as a predicate and not as an Expression which happens today.
CAMEL-f412d744$$StaxConverter: encoding problems for XMLEventReader and XMLStreamReader$$StaxConverter creates XMLEventReader and XMLStreamReader always with a specified encoding. However, the encoding of the data the readers should read is not always known. Therefore exceptions occur.  The solution is easy: The encoding should not be set so that the readers can determine the encoding.
CAMEL-6b210169$$Bean Component/BeanBinding: Body as InputStream parametr (specified as  {body} in route)$$I discovered following problem (which was already shortly discussed in [Camel user forum|http://camel.465427.n5.nabble.com/Bean-component-Bean-Binding-Body-as-InputStream-parametr-specified-as-body-in-route-td5740656.ht]).  I have a "streamBodyBindingBean" bean with this method: {code} public void bodyBinding(InputStream in) throws IOException {   int byteCount = 0;   int c;   while((c = in.read()) != -1)     byteCount++;   System.out.println("ByteCount: " + byteCount); } {code}  And this route: {code} <route id="" trace="true">   <from uri="direct://body-input-stream-binding-in"/>   <to uri="bean://streamBodyBindingBean?method=bodyBinding( {body})"/>   <!-- to uri="bean://isBodyBindingBean"/-->    <to uri="mock://body-input-stream-binding-out"/> </route> {code}  And here is a way how I send exchange from test stuff: {code} ByteArrayInputStream in = new ByteArrayInputStream(   "Small body, which I want to bind as InputStream".getBytes("UTF-8") ); Exchange exchange = createExchangeWithBody(in); exchange.setPattern(ExchangePattern.InOnly); template.send("direct://body-input-stream-binding-in", exchange);  {code}  In this case I got a sysout message: {{ByteCount: 0}}, but when I used the commented variant in the route, I got expected result: {{ByteCount: 47"}}.  When I change the route and use bean component 2 times (both variant of bean method invocation), then I got:  {noformat} 2013-10-01 12:26:37.259 DEBUG {main} [SendProcessor] >>>> Endpoint[bean://isBodyBindingBean?method=bodyBinding%28%24%7Bbody%7D%29] Exchange[Message: [Body is instance of org.apache.camel.StreamCache]] ByteCount: 0 2013-10-01 12:26:37.289 DEBUG {main} [SendProcessor] >>>> Endpoint[bean://isBodyBindingBean] Exchange[Message: [Body is instance of org.apache.camel.StreamCache]] ByteCount: 47 2013-10-01 12:26:37.307 DEBUG {main} [SendProcessor] >>>> Endpoint[mock://body-input-stream-binding-out] Exchange[Message: [Body is instance of org.apache.camel.StreamCache]]  {noformat}  The strange for me is {{MethodInfo}} class, line 526: {code} // the parameter value was not already valid, but since the simple language have evaluated the expression // which may change the parameterValue, so we have to check it again to see if its now valid exp = exchange.getContext().getTypeConverter().convertTo(String.class, parameterValue); // String values from the simple language is always valid if (!valid) {   ... } {code}  The line after comment caused that my "InputStream" is transformed into String, what can be a problem in case of "big" InputStream.  I know that I can use only second variant of "bean method invocation", which is enough for my need, but I only want to point out to this situation.
CAMEL-cd40b712$$CBR - Should break out if exception was thrown when evaluating predicate$$If having a CBR and the predicate throws an exception, then the next predicate is called before error handler triggers.  We should break out when exception is detected like pipeline/multicast can do.
CAMEL-5761250c$$Error handler for SEDA producer doesn't work$$Exceptions thrown by seda producer bypass exception handling and bubble up to original caller.
CAMEL-4954d573$$FTP route with idempotent repo does not detect modified files$$Per my forum post: http://camel.465427.n5.nabble.com/inProgressRepository-Not-clearing-for-items-in-idempotentRepository-td5742613.html  I'm attempting to consume messages from an FTP server using an idempotent repository to ensure that I do not re-download a file unless it has been modified.   Here is my (quite simple) camel configuration:  {code}         <beans:bean id="downloadRepo" class="org.apache.camel.processor.idempotent.FileIdempotentRepository" >                 <beans:property name="fileStore" value="/tmp/.repo.txt"/>                 <beans:property name="cacheSize" value="25000"/>                 <beans:property name="maxFileStoreSize" value="1000000"/>         </beans:bean>          <camelContext trace="true" xmlns="http://camel.apache.org/schema/spring">                 <endpoint id="myFtpEndpoint" uri="ftp://me@localhost?password=****&binary=true&recursive=true&consumer.delay=15000&readLock=changed&passiveMode=true&noop=true&idempotentRepository=#downloadRepo&idempotentKey= simple{file:name}- simple{file:modified}" />                 <endpoint id="myFileEndpoint" uri="file:///tmp/files"/>          <route>             <from uri="ref:myFtpEndpoint" />             <to uri="ref:myFileEndpoint" />         </route> {code}  When I start my application for the first time, all files are correctly downloaded from the FTP server and stored in the target directory, as well as recorded in the idempotent repo.   When I restart my application, all files are correctly detected as being in the idempotent repo already on the first poll of the FTP server, and are not re-downloaded:   13-11-04 16:52:10,811 TRACE [Camel (camel-1) thread #0 - ftp://me@localhost] org.apache.camel.component.file.remote.FtpConsumer: FtpFile[name=test1.txt, dir=false, file=true]  2013-11-04 16:52:10,811 TRACE [Camel (camel-1) thread #0 - ftp://me@localhost] org.apache.camel.component.file.remote.FtpConsumer: This consumer is idempotent and the file has been consumed before. Will skip this file: RemoteFile[test1.txt]   However, on all subsequent polls to the FTP server the idempotent check is short-circuited because the file is "in progress":   2013-11-04 16:53:10,886 TRACE [Camel (camel-1) thread #0 - ftp://me@localhost] org.apache.camel.component.file.remote.FtpConsumer: FtpFile[name=test1.txt, dir=false, file=true] 2013-11-04 16:53:10,886 TRACE [Camel (camel-1) thread #0 - ftp://me@localhost] org.apache.camel.component.file.remote.FtpConsumer: Skipping as file is already in progress: test1.txt   I am using camel-ftp:2.11.1 (also observing same behavior with 2.12.1)  When I inspect the source code I notice two interesting things.  First, the GenericFileConsumer check that determines whether a file is already inProgress which is called from isValidFile() always adds the file to the inProgressRepository:  {code}     protected boolean isInProgress(GenericFile<T> file) {          String key = file.getAbsoluteFilePath();          return !endpoint.getInProgressRepository().add(key);      }  {code}  Second, if a file is determined to match an entry already present in the idempotent repository it is discarded (GenericFileConsumer.isValidFile() returns false).  This means it is never published to an exchange, and thus never reaches the code which would remove it from the inProgressRepository.   Since the inProgress check happens before the Idempotent Check, we will always short circuit after we get into the inprogress state, and the file will never actually be checked again.
CAMEL-f744afd9$$ProducerCache should not only stop non-singelton Producers but also shutdown them afterwards as well if the given Producer is a ShutdownableService$$Currently because of this bug the {{doShutdown}} hook of the following non-singleton Producers doesn't kick in at all:  - {{JpaProducer}} - {{Mina2Producer}}  Which could cause resources leaking.
CAMEL-6b2ffb30$$Camel FileComponent: Done file will not be removed if moveFailed option is configured and an error occurs$$Only the "real" file is moved to the directory specified with the moveFailed-option. The done file still exists in the source folder and will not be deleted.
CAMEL-37e0e6bb$$JMX - browseMessageAsXml for files does not work if includeBody is enabled$$If you use the JXM API to browse file endpoints and want to load the file content with includeBody = true, then the file is not loaded.  There is a little bug in MessgeHelper
CAMEL-4ed448c7$$JMX - Update route from xml on route mbean should update current route only$$If you do not have id of the route in the XML then Camel thinks its a new route to be added. We should ensure we handle that, and only update current route as that is the intend of this operation.  If you want to add new routes use mbean operation on camelcontext instead.
CAMEL-3244c1e5$$Using custom beans with @ManagedResource shows unavailable standard attributes$$If you have a custom bean with @ManagedResource and your own attr/ops then Camel adds its default attrs/ops which it should not as they are not available.  See screenshot
CAMEL-15e1077d$$NullPointerException at FileInputStreamCache.<init>(FileInputStreamCache.java:52) in connection with DataFormat.marshal$$Stack Trace: {code} Caused by: java.lang.NullPointerException 	at org.apache.camel.converter.stream.FileInputStreamCache.<init>(FileInputStreamCache.java:52) 	at org.apache.camel.converter.stream.CachedOutputStream.newStreamCache(CachedOutputStream.java:199) 	at org.apache.camel.processor.MarshalProcessor.process(MarshalProcessor.java:79) {code}  Error occurs, if streamCache is true and the stream is put into the file system because the spool threashold is reached.   The following is happening: The Marshall Processor handels over to the DataFromat.marshal method a CachedOutputStream instance. In the marschal method data are written into the output stream, when the spool threshold is reached the data are streamed into the file system. Finally the output stream is closed and the CachedOutputStream instance deletes the cached file during closing. The next processor tries to read the FileInputStreamCache and gets the NullPointerException.  Currently this problem can occur in the following DataFormat classes (because they close the stream, which is actually correct):  GzipDataFormat CryptoDataFormat PGPDataFormat SerializationDataFormat XMLSecurityDataFormat ZipDataFormat  My proposal is not to delete the cached file during closing the output stream. The cached file shall only be closed on the onCompletion event of the route. See attached patch.
CAMEL-00a9b02b$$CLONE - Camel Splitter eat up exceptions recorded by the underlying Scanner$$See http://camel.465427.n5.nabble.com/Trouble-with-split-tokenize-on-linux-td5721677.html for details
CAMEL-6641f182$$tokenizeXml fails when attributes have a / in them$${{tokenizeXml}} does not work or produce value xml output when attributes contain a {{/}}.  The test below will fail under 2.12.2  {code:java} import org.apache.camel.EndpointInject; import org.apache.camel.Produce; import org.apache.camel.ProducerTemplate; import org.apache.camel.builder.RouteBuilder; import org.apache.camel.component.mock.MockEndpoint; import org.apache.camel.test.junit4.CamelTestSupport; import org.junit.Test;  public class CamelTokenizeXmlTest extends CamelTestSupport {    @EndpointInject(uri = "mock:result")   protected MockEndpoint resultEndpoint;   @Produce(uri = "direct:start")   protected ProducerTemplate template;    @Test   public void testXmlWithSlash() throws Exception {     String message = "<parent><child attr='/' /></parent>";     resultEndpoint.expectedBodiesReceived("<child attr='/' />");     template.sendBody(message);     resultEndpoint.assertIsSatisfied();   }    @Override   protected RouteBuilder createRouteBuilder() {     return new RouteBuilder() {       @Override       public void configure() {         from("direct:start").split().tokenizeXML("child").to("mock:result");       }     };   } } {code}
CAMEL-7c9326f4$$Set XsltBuilder allowStax attribute to be true by default$$It could be more effective and safe to use the stax API by default.
CAMEL-cc192f87$$Set XsltBuilder allowStax attribute to be true by default$$It could be more effective and safe to use the stax API by default.
CAMEL-b6981cfd$$NPE in Aggregator when completionSize = 1$$A Camel aggregator with persistence repository cannot have a completionSize of 1. If this is configured, every message produces a NPE with the attached stacktrace.   I have also attached a small example project that shows the Exception. As soon as the completionSize is > 1, it runs fine.  This is just a minor flaw, since I cannot think about a really useful case with completionSize 1, but it worked with earlier versions of Camel.   As an alternative (if completionSize 1 should not be used), Camel could throw an error during Context startup when completionSize < 2.
CAMEL-095fa2b4$$Throttling has problems with rate changes$$When using the throttler with the header expression for controlling the rate, changing the rate does not work reliably.   Some more information can be found in the following mail thread:  http://camel.465427.n5.nabble.com/Problems-with-dynamic-throttling-td5746613.html
CAMEL-5f726d0b$$BacklogDebugger - Should not change body/header type to string$$When using the backlog debugger then updating the body/headers would currently force those to become string type.  We should preserve existing type, and allow end users to specify a new type. And also make it possible to remove body/headers as well.
CAMEL-1e33fcbc$$AbstractListAggregationStrategy : at the end of the split, the body is not replaced by the agregated list$$Using a class that extends AbstractListAggregationStrategy to rebuild a List after the completion of the split cause the body not to be replaced by the agregated list at the end of the split.  In other words (AbstractListAggregationStrategy.onCompletion(Exchange exchange) is never called.   Here is what I do :  from(HANDLE_A_LIST)//             .split(body(), new ListAggregationStrategy())// body is an arrayList of String             .to("log:foo")//             .end()// end split             // the body is a string instead of a List             .end()// end route      class ListAggregationStrategy extends AbstractListAggregationStrategy<String>     {          @Override         public String getValue(Exchange exchange)         {             return exchange.getIn().getBody();         }     }  As workaround, I use .setBody(property(Exchange.GROUPED_EXCHANGE)) after the end of the split.
CAMEL-5f78c646$$NIOConverter.toByteArray return bad data.$$Current implmentation of NIOConverter.toByteArray return the byte array that back the buffer. Array can be bigger that relevant data in ByteBuffer.
CAMEL-336663c9$$NIOConverter need to call flip() when we put something into the buffer$$When we create a ByteBuffer, we need to make sure it is ready to be read.
CAMEL-ae419224$$Address the SchemaFactory thread safe issue.$$SchemaFactory is not thread safe, we need to do addition work in ValidatorProcessor to avoid the threads issue.
CAMEL-18c23fa8$$ByteBuffer to String conversion uses buffer capacity not limit$$Camel's conversion logic for ByteBuffer's to String's has a bug where camel uses a ByteBuffers capacity() instead of it's limit().  If you allocate a large byte buffer and only partially fill it with data, and use camel to convert this into a string, camel tries to convert all the bytes, even the non-used ones.  This unit test reproduces this bug.  {code}     @Test     public void testByteBufferToStringConversion()     {         String str = "123456789";         ByteBuffer buffer = ByteBuffer.allocate( 16 );         buffer.put( str.getBytes() );          Exchange exchange = new DefaultExchange( context() );         exchange.getIn().setBody( buffer );         assertEquals( str, exchange.getIn().getBody( String.class ) );     } {code}
CAMEL-a5a2f750$$AbstractListGroupedExchangeAggregationStrategy produces failed exchange if first received exchange fails$$If the first exchange received by a (concrete implementation of) AggregationStrategy  contains an exception, then the result of the aggregation will also contain that exception, and so will not continue routing without error. This makes the first received exchange have an effect that subsequent exchanges do not have.  The specific use case multicasts to GroupedExchangeAggregationStrategy. The MulticastProcessor.doDone function uses ExchangeHelper.copyResults to copy the aggregated result to the original exchange. The copyResults method copies the exception as well, thereby propagating the error.   The attached unit test has 3 tests, testAFail, testBFail, and testAllGood. All three of these should pass, but testAFail does not.  What is happening is that AbstractListAggregationStrategy is directly storing its values on and returning the first exchange:     public Exchange aggregate(Exchange oldExchange, Exchange newExchange) {         List<V> list;          if (oldExchange == null) {             list = getList(newExchange);         } else {             list = getList(oldExchange);         }          if (newExchange != null) {             V value = getValue(newExchange);             if (value != null) {                 list.add(value);             }         }          return oldExchange != null ? oldExchange : newExchange;     }  The pre-CAMEL-5579 version of GroupedExchangeAggregationStrategy created a fresh exchange to store and return the aggregated exchanges:     public Exchange aggregate(Exchange oldExchange, Exchange newExchange) {         List<Exchange> list;         Exchange answer = oldExchange;          if (oldExchange == null) {             answer = new DefaultExchange(newExchange);             list = new ArrayList<Exchange>();             answer.setProperty(Exchange.GROUPED_EXCHANGE, list);         } else {             list = oldExchange.getProperty(Exchange.GROUPED_EXCHANGE, List.class);         }          if (newExchange != null) {             list.add(newExchange);         }         return answer;     }
CAMEL-44cad623$$Using doTry .. doCatch with recipient list should not trigger error handler during recipient list work$$When you have a route like this  {code}                 from("direct:start")                     .doTry()                         .recipientList(constant("direct:foo")).end()                     .doCatch(Exception.class)                         .to("mock:catch")                         .transform().constant("doCatch")                     .end()                     .to("mock:result"); {code}  Then if an exception was thrown it should be catch by doCatch  A similar route with to instead works as expected.
CAMEL-fa165d6b$$InterceptSendToEndpoint does not work where uri needs to be normalized$$interceptSendToEndpoint("sftp://hostname:22/testDirectory?privateKeyFile=/user/.ssh.id_rsa") is not intercepted because uri passed to InterceptSendToEndpointDefinition is not normalized.  As a result InterceptSendToEndpointDefinition createProcessor() method fails to match EndpointHelper.matchEndpoint(routeContext.getCamelContext(), uri, getUri()) and InterceptSendToEndpoint is not created.
CAMEL-91228815$$Some endpoints configured using beans may result in NPE under DEBUG mode$$CAMEL-6130 seems to have introduced this issue or more precisely speaking, it has made this issue visible.  DefaultEndpoint's toString() method seems to require its endpoint string value to be set. If it's not set, the toString method throws an exception. A fully built endpoint always has its endpoint string value set, thus there is no issue. However, an endpoint being manually set up may not have its endpoint string value set from the beginning (e.g., when its super class uses the DefaultEndpoint's default constructor to instantiate using a bean based instantiation).  The debug log statement introduced in CAMEL-6130 invokes this toString method during the endpoint setup.  That means, a spring based CXF endpoint may result in the following exception under the debug mode.  SLF4J: Failed toString() invocation on an object of type [org.apache.camel.component.cxf.CxfSpringEndpoint] java.lang.IllegalArgumentException: endpointUri is not specified and org.apache.camel.component.cxf.CxfSpringEndpoint does not implement createEndpointUri() to create a default value at org.apache.camel.impl.DefaultEndpoint.getEndpointUri(DefaultEndpoint.java:154) at org.apache.camel.impl.DefaultEndpoint.toString(DefaultEndpoint.java:139) at org.slf4j.helpers.MessageFormatter.safeObjectAppend(MessageFormatter.java:304) at org.slf4j.helpers.MessageFormatter.deeplyAppendParameter(MessageFormatter.java:276) at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:230) at org.slf4j.impl.Log4jLoggerAdapter.debug(Log4jLoggerAdapter.java:271) at org.apache.camel.util.IntrospectionSupport.setProperty(IntrospectionSupport.java:528) at org.apache.camel.util.IntrospectionSupport.setProperty(IntrospectionSupport.java:570) at org.apache.camel.util.IntrospectionSupport.setProperties(IntrospectionSupport.java:454) at org.apache.camel.util.EndpointHelper.setProperties(EndpointHelper.java:249) at org.apache.camel.component.cxf.CxfEndpoint.setCamelContext(CxfEndpoint.java:840)  I  wonder whether we really need DefaultEndpoint's getEndpointUri() to throw an exception when it's endpoint string value is not set. But if we keep this rule, we must catch the exception in its toString() method so that we won't throw the above exception when the toString() method is called during the endpoint setup  I would propose to add the exception catching in the toString method. If we decide to change the getEndpointUri() method to not throw the exception (that change will likely require the NPE check at the users of this method), we can make that change and remove the exception catch from the toString method  This issue affects camel 2.11.0 and later versions.
CAMEL-9cb09d14$$Simple Language - Additional after text after inbuilt function call is ignored$$The following Simple expression is valid and runs OK - however it may have been appropriate to report an error to the developer.  {code:xml}             <setBody>                 <simple> {bodyAs(java.lang.String) Additional text ignored...}</simple>             </setBody> {code}  The above seems a somewhat contrived example; However this is a more 'realistic' scenario in which the behaviour is not unexpected -  {code:xml}             <setBody>                 <simple> {bodyAs(java.lang.String).toUpperCase()}</simple>             </setBody> {code}  The above simple expression will simply set the body to be of type java.lang.String, however will not invoke the subsequent toUpperCase() call - likewise no error is reported to the developer.  Camel has the same issue when using the function of headerAs and mandatoryBodyAs.
CAMEL-e6fbbf04$$Simple Language - Additional after text after inbuilt function call is ignored$$The following Simple expression is valid and runs OK - however it may have been appropriate to report an error to the developer.  {code:xml}             <setBody>                 <simple> {bodyAs(java.lang.String) Additional text ignored...}</simple>             </setBody> {code}  The above seems a somewhat contrived example; However this is a more 'realistic' scenario in which the behaviour is not unexpected -  {code:xml}             <setBody>                 <simple> {bodyAs(java.lang.String).toUpperCase()}</simple>             </setBody> {code}  The above simple expression will simply set the body to be of type java.lang.String, however will not invoke the subsequent toUpperCase() call - likewise no error is reported to the developer.  Camel has the same issue when using the function of headerAs and mandatoryBodyAs.
CAMEL-7bbb88ba$$JpaMessageIdRepository uses EntityManager non thread-safe$$In our product we have found strange behavior of JpaMessageIdRepository when change version 2.9.2 to 2.12.3. The reason for this was that EntityManager assigned in the constructor org.apache.camel.processor.idempotent.jpa.JpaMessageIdRepository, but EntityManager not required to be thread safe. http://download.oracle.com/otn-pub/jcp/persistence-2.0-fr-oth-JSpec/persistence-2_0-final-spec.pdf page 286. I think need assign the EntityManager in each method separately.
CAMEL-cabee0e9$$org.apache.camel.impl.JndiRegistry.findByTypeWithName$$I guess this line isn't correct: if (type.isInstance(pair.getClass()) || type.getName().equals(pair.getClassName()))  The variable "pair.getClass()" always returns "javax.naming.NameClassPair" or its subclasses and the method "isInstance" works only with Instances, but doesnt Classes.    I think the correct code should be: if (type.isAssignableFrom(Class.forName(pair.getClassName())))   I've tried to test a transacted route, but i couldnt because the error:  Failed to create route route1 at: >>> Transacted[] <<< in route: Route(route1)[[From[direct:start]] -> [Transacted[]]] because of No bean could be found in the registry of type: PlatformTransactionManager
CAMEL-43956f93$$Camel Properties Component concatenation issue$$Hi,  Suppose you have a properties file of this type  {code} #PROPERTIES CONCATENATION prop1=file: prop2=dirname concat.property={{prop1}}{{prop2}}  #PROPERTIES WITHOUT CONCATENATION property.complete=file:dirname {code}  and you want to use the property concat.property. Using Camel 2.10.3 loading this property doesn't create any kind of problem. When I upgrade to Camel 2.12.3 I get an exception, that you can reproduce with the following informations.  In *DefaultPropertiesParser* class of org.apache.camel.component.properties package, I found a strange behaviour relative to that specific kind of property.  When I execute a test like the following, (the first try to use concatenated property and the second try to use property without concatenation):  {code:title=PropertiesComponentConcatenatePropertiesTest.java} import org.apache.camel.CamelContext; import org.apache.camel.ContextTestSupport; import org.apache.camel.builder.RouteBuilder;  public class PropertiesComponentConcatenatePropertiesTest extends ContextTestSupport {          @Override     protected CamelContext createCamelContext() throws Exception {         CamelContext context = super.createCamelContext();         context.addComponent("properties", new PropertiesComponent("classpath:org/apache/camel/component/properties/concatenation.properties"));         return context;     }          @Override     protected void setUp() throws Exception {         System.setProperty("environment", "junit");         super.setUp();     }          @Override     protected void tearDown() throws Exception {         System.clearProperty("environment");         super.tearDown();     }          public void testConcatPropertiesComponentDefault() throws Exception {         context.addRoutes(new RouteBuilder() {             @Override             public void configure() throws Exception {                 from("direct:start").setBody(simple(" {properties:concat.property}"))                 .to("mock:result");             }         });         context.start();          getMockEndpoint("mock:result").expectedBodiesReceived("file:dirname");          template.sendBody("direct:start", "Test");          assertMockEndpointsSatisfied();     }          public void testWithoutConcatPropertiesComponentDefault() throws Exception {         context.addRoutes(new RouteBuilder() {             @Override             public void configure() throws Exception {                 from("direct:start").setBody(simple(" {properties:property.complete}"))                 .to("mock:result");             }         });         context.start();          getMockEndpoint("mock:result").expectedBodiesReceived("file:dirname");          template.sendBody("direct:start", "Test");          assertMockEndpointsSatisfied();     } } {code}  The first test return the following exception: {code} org.apache.camel.CamelExecutionException: Exception occurred during execution on the exchange: Exchange[Message: Test] 	at org.apache.camel.util.ObjectHelper.wrapCamelExecutionException(ObjectHelper.java:1379) 	at org.apache.camel.util.ExchangeHelper.extractResultBody(ExchangeHelper.java:622) 	at org.apache.camel.impl.DefaultProducerTemplate.extractResultBody(DefaultProducerTemplate.java:467) 	at org.apache.camel.impl.DefaultProducerTemplate.extractResultBody(DefaultProducerTemplate.java:463) 	at org.apache.camel.impl.DefaultProducerTemplate.sendBody(DefaultProducerTemplate.java:139) 	at org.apache.camel.impl.DefaultProducerTemplate.sendBody(DefaultProducerTemplate.java:144) 	at org.apache.camel.component.properties.PropertiesComponentConcatenatePropertiesTest.testConcatPropertiesComponentDefault(PropertiesComponentConcatenatePropertiesTest.java:56) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:606) 	at junit.framework.TestCase.runTest(TestCase.java:176) 	at junit.framework.TestCase.runBare(TestCase.java:141) 	at org.apache.camel.TestSupport.runBare(TestSupport.java:58) 	at junit.framework.TestResult 1.protect(TestResult.java:122) 	at junit.framework.TestResult.runProtected(TestResult.java:142) 	at junit.framework.TestResult.run(TestResult.java:125) 	at junit.framework.TestCase.run(TestCase.java:129) 	at junit.framework.TestSuite.runTest(TestSuite.java:255) 	at junit.framework.TestSuite.run(TestSuite.java:250) 	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84) 	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50) 	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197) Caused by: org.apache.camel.RuntimeCamelException: java.lang.IllegalArgumentException: Expecting }} but found end of string from text: prop1}}{{prop2 	at org.apache.camel.util.ObjectHelper.wrapRuntimeCamelException(ObjectHelper.java:1363) 	at org.apache.camel.builder.ExpressionBuilder 78.evaluate(ExpressionBuilder.java:1784) 	at org.apache.camel.support.ExpressionAdapter.evaluate(ExpressionAdapter.java:36) 	at org.apache.camel.builder.SimpleBuilder.evaluate(SimpleBuilder.java:83) 	at org.apache.camel.processor.SetBodyProcessor.process(SetBodyProcessor.java:46) 	at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:398) 	at org.apache.camel.processor.CamelInternalProcessor.process(CamelInternalProcessor.java:191) 	at org.apache.camel.processor.Pipeline.process(Pipeline.java:118) 	at org.apache.camel.processor.Pipeline.process(Pipeline.java:80) 	at org.apache.camel.processor.CamelInternalProcessor.process(CamelInternalProcessor.java:191) 	at org.apache.camel.component.direct.DirectProducer.process(DirectProducer.java:51) 	at org.apache.camel.processor.CamelInternalProcessor.process(CamelInternalProcessor.java:191) 	at org.apache.camel.processor.UnitOfWorkProducer.process(UnitOfWorkProducer.java:73) 	at org.apache.camel.impl.ProducerCache 2.doInProducer(ProducerCache.java:378) 	at org.apache.camel.impl.ProducerCache 2.doInProducer(ProducerCache.java:1) 	at org.apache.camel.impl.ProducerCache.doInProducer(ProducerCache.java:242) 	at org.apache.camel.impl.ProducerCache.sendExchange(ProducerCache.java:346) 	at org.apache.camel.impl.ProducerCache.send(ProducerCache.java:184) 	at org.apache.camel.impl.DefaultProducerTemplate.send(DefaultProducerTemplate.java:124) 	at org.apache.camel.impl.DefaultProducerTemplate.sendBody(DefaultProducerTemplate.java:137) 	... 22 more Caused by: java.lang.IllegalArgumentException: Expecting }} but found end of string from text: prop1}}{{prop2 	at org.apache.camel.component.properties.DefaultPropertiesParser.doParseUri(DefaultPropertiesParser.java:90) 	at org.apache.camel.component.properties.DefaultPropertiesParser.parseUri(DefaultPropertiesParser.java:51) 	at org.apache.camel.component.properties.DefaultPropertiesParser.parseUri(DefaultPropertiesParser.java:38) 	at org.apache.camel.component.properties.DefaultPropertiesParser.createPlaceholderPart(DefaultPropertiesParser.java:189) 	at org.apache.camel.component.properties.DefaultPropertiesParser.doParseUri(DefaultPropertiesParser.java:105) 	at org.apache.camel.component.properties.DefaultPropertiesParser.parseUri(DefaultPropertiesParser.java:51) 	at org.apache.camel.component.properties.PropertiesComponent.parseUri(PropertiesComponent.java:158) 	at org.apache.camel.component.properties.PropertiesComponent.parseUri(PropertiesComponent.java:117) 	at org.apache.camel.builder.ExpressionBuilder 78.evaluate(ExpressionBuilder.java:1781) 	... 40 more {code}  It seems that *DefaultPropertiesParser* doesn't like concatenation of properties. I've forked Camel project on GitHub and I've added the unit test posted above. Here is the link: https://github.com/ancosen/camel  Investigating the history of the particular class I found that the problem should arise from:  *CAMEL-5328 supports resolution of nested properties in PropertiesComponent*  Here is the link of the commit: https://github.com/apache/camel/commit/83f4b0f485521967d05de4e65025c4558a75ff3c  Thanks. Bye
CAMEL-35bde2b2$$throttle EIP - unchanged value$$Throttler Documentation [1] states "If the header is absent, then the Throttler uses the old value. So that allows you to only provide a header if the  value is to be changed".  however if the expression evaluates to null (header missing from message) the Throttler throws an exception (Throttler.java:108).  The workaround is to ensure that all messages carry the value (if the value is the same no changes will take affect). Adding an option to turn this on and off (e.g. allowNullException) would make it much easier to use (as per camel-users thread [2]).  [1] http://camel.apache.org/throttler.html [2] http://camel.465427.n5.nabble.com/throttle-EIP-unchanged-value-td5751300.html
CAMEL-02da984a$$Camel PropertiesComponent ignores custom parser in Blueprint$$I have implemented a custom PropertiesParser which allows me to use system property placeholders in propertyPrefix and propertySuffix.  In my use case the propertyPrefix is defined as " \{container.stage}.", where container.stage is a jvm option defined at container creation. The value is one of dev, test and prod.  This works fine in Java DSL world (SCR bundle), but custom parser is ignored in Blueprint. Here is sample of my blueprint xml: {code}  <cm:property-placeholder id="integration" persistent-id="org.apache.camel.sample.temp" placeholder-prefix="[[" placeholder-suffix="]]">     <cm:default-properties>         <cm:property name="example" value="this value is the default"/>         <cm:property name="dev.example" value="this value is used in development environment"/>         <cm:property name="test.example" value="this value is used in test environment"/>         <cm:property name="prod.example" value="this value is used in production environment"/>     </cm:default-properties> </cm:property-placeholder>  <bean id="parser" class="org.apache.camel.sample.MyCustomPropertiesParser"/>  <!-- Load properties for current container stage --> <bean id="properties" class="org.apache.camel.component.properties.PropertiesComponent">     <property name="propertiesParser" ref="parser"/>     <property name="propertyPrefix" value=" {container.stage}."/>     <property name="fallbackToUnaugmentedProperty" value="true"/>     <property name="location" value="blueprint:integration,classpath:properties/temp.properties"/></bean>  <camelContext id="temp" xmlns="http://camel.apache.org/schema/blueprint">     <route id="exampleRoute">         <from uri="timer:foo?period=5000"/>         <transform>             <simple>{{example}}</simple>         </transform>         <to uri="log:something"/>     </route> </camelContext> {code}  The reason it did not work was because by default, it uses blueprint property resolver (useBlueprintPropertyResolver="true") to bridge PropertiesComponent to blueprint in order to support looking up property placeholders from the Blueprint Property Placeholder Service. Then it always creates a BlueprintPropertiesParser object and set it to PropertiesComponent.   The customer Property Parser I created was only set into the BlueprintPropertiesParser object as a delegate Property Parser. Therefore, it was always the method parseUri() from the BlueprintPropertiesParser object got invoked. The same method from your custom parser was ignored.   For more detail, please take a look at org.apache.camel.blueprint.CamelContextFactoryBean.initPropertyPlaceholder() function.  The only workaround is to add the attribute useBlueprintPropertyResolver="false" to <camelContext> element to disable default blueprint property resolver. However, I will have to change PropertiesComponent's "location" property to remove blueprint "blueprint:integration" from the comma separated value list: {code}  <property name="location" value="classpath:properties/temp.properties"/>  {code} Because once I set it to false, I will no longer be able to lookup from blueprint property service.
CAMEL-57ba1bde$$parseQuery Drops Char When Last Parameter is RAW with value ending in ')'$$org.apache.camel.util.URISupport  When processing RAW parameters as part of parseQuery a look ahead to the next char is needed in order to determine the end of the RAW value.  The logic to prevent a _StringIndexOutOfBoundsException_ drops the last char when evaluating for _next_ char when the current char (_i_) is the second to last char of the string.  This becomes an issue when the RAW value ends in ')'   Consider: uri = "foo=RAW(ba(r))" uri.length() = 14 i = 12 uri.charAt(12) = ')' uri.charAt(13) = ')'  (i < uri.legnth() - 2) = 12 < (14 - 2) = 12 < 12 = false thus next = "\u0000"  The RAW value now ends satisfying the requirements and the char at index 13 is never read.  The resulting parameter is "foo=RAW(ba(r)".  The logic to prevent the index exception should be "(i <*=* uri.legnth() -2)" or "(i < uri.legnth() - *1*)"
CAMEL-69b00a31$$Simple Language - Length of array properties is not correctly evaluated$$If the exchange body is an array, then {{body.length}} returns correctly the length of the array. However, if the array is a property of an object, then not the correct value is returned: {code:title=MyClass.java} public class MyClass {     public Object[] getMyArray() {         return new Object[]{"Hallo", "World", "!"};     } } {code} Accessing the property {{myArray}} with Simple: {code} <setHeader headerName="mySimpleHeader">     <simple>body.myArray.length</simple> </setHeader> <log message="mySimpleHeader =  {header.mySimpleHeader}" /> {code} Java: {code} final ProducerTemplate template = main.getCamelTemplate(); template.sendBody("direct:start", new MyClass()); {code} Log: {noformat} [main] route1 INFO  mySimpleHeader = 1 {noformat} The return value should be {{3}} instead of {{1}}.
CAMEL-85ced066$$Using JPA entities as the argument in Aggregator using POJO$$I have an Aggregator POJO with this method :  public Map<Hoteles, List<EventoPrecio>> agregaEventoPrecio(Map<Hoteles, List<EventoPrecio>> lista, EventoPrecio evento)   With this route :  from("timer://tesipro?fixedRate=true&period=60000"). beanRef("uploadARIService", "getEventosPrecio"). aggregate(constant(true), AggregationStrategies.bean(AgregadorEventos.class, "agregaEventoPrecio")). completionSize(100). log("Ejecucion de Quartz ");  And I get this error :  Error occurred during starting Camel: CamelContext(249-camel-9) due Parameter annotations at index 1 is not supported on method: public java.util.HashMap com.tesipro.conectores.interfaces.tesiproconpush.camel.AgregadorEventos.agregaEventoPrecio(java.util.HashMap,com.tesipro.conectores.domain.EventoPrecio)            It seems the problem is that annotations are not supported in the aggregator arguments nor in the argument class.  https://github.com/apache/camel/blob/3f4f8e9ddcc8de32cca084927a10c5b3bceef7f9/camel-core/src/main/java/org/apache/camel/processor/aggregate/AggregationStrategyBeanInfo.java#L67
CAMEL-689147e9$$camel-test - AdviceWith in CBR may add twice$$See nabble http://camel.465427.n5.nabble.com/Camel-AdviceWith-issues-tp5752786.html  When using advice-with for a CBR it may add to the when clauses 2 times.
CAMEL-b3377b16$$OnComplete does not  work on transactioned route after rollback$$Example: {code:title=Route Sample|borderStyle=solid} this.from("servlet:///test").routeId("CamelTestRoute")     .onCompletion()        .bean(this.logCompletionRoute)     .end()     .onException(Exception.class)        .log(LoggingLevel.ERROR, this.log, "Error on processing message. Sending Rollback command!")        .log(LoggingLevel.ERROR, this.log, " {exception.stacktrace}")        .rollback()       .handled(true)     .end()     .transacted(RouteTransactionConfiguration.PROPAGATION_REQUIRED)     .process(new Processor() {                  @Override                  public void process(Exchange exchange) throws Exception {                      throw new Exception();                  }});  {code}  In this sample, the OnCompletion bean never is executed. But, if I remove the "rollback()" call, it is executed properly.  thanks,
CAMEL-1f92fa42$$NotCompliantMBeanException : Attribute MessageHistory has more than one getter$$Hello, I wasn't able to subscribe on the mailing list, so I'm posting my issue directly here.  In my project I need to use some _ManagedCamelContextMBean_, which I am trying to access through [JMX.newMBeanProxy|http://docs.oracle.com/javase/8/docs/api/javax/management/JMX.html#newMBeanProxy-javax.management.MBeanServerConnection-javax.management.ObjectName-java.lang.Class-]  However, it is not working as I'm getting a *NotCompliantMBeanException* because the attribute _MessageHistory_ is said to have more than one getter.  I checked the source code of newMBeanProxy, then the [JMX 1.4 specification|http://docs.oracle.com/javase/8/docs/technotes/guides/jmx/JMX_1_4_specification.pdf], and then Camel's source code, and it appears that ManagedCamelContextMBean is indeed not respecting the standard MBean.  The problem is that two methods are defined in _ManagedCamelContextMBean_ : isMessageHistory() and getMessageHistory() Since the return type is boolean, isMessageHistory is considered to be a getter, which makes two getter according to the JMX specification and is blocking the newMBeanProxy() method.
CAMEL-e30f1c53$$org.apache.camel.util.KeyValueHolder equals bug$$According to java.lang.Object javadoc (http://docs.oracle.com/javase/7/docs/api/java/lang/Object.html), "equal objects must have equal hash codes".   Current implementation of the "equals" and "hashCode" method of the org.apache.camel.util.KeyValueHolder does not seem to follow that rule: hashCode is calculated from the key and value attributes while the equals compares only the key attribute.   Could generate unexpected behaviour in certain circumstances.
CAMEL-faa20255$$advice-with - No outputs found matching id when upgrading from 2.13 to 2.14$$I have the following route defined with the Java DSL:   from("direct:localMemberLookup").routeId("localMemberLookup")          .process(new MemberLookupToSqlParametersProcessor()).id("sqlParams")          .recipientList(simple("sql:{{sql.memberLookup}}")).delimiter("false")          .to("log:output")          .process(new MemberLookupProcessor())          // do more processing          .to("log:output");   I'm testing it with a test that looks as follows:   @EndpointInject(uri = "mock:lookupHeaders")  MockEndpoint lookupHeaders;   @EndpointInject(uri = "mock:searchResult")  MockEndpoint searchResult;   @EndpointInject(uri = "mock:lookupResult")  MockEndpoint lookupResult;   @Autowired  CamelContext camelContext;   @Before  public void before() throws Exception {          ModelCamelContext context = (ModelCamelContext) camelContext;          context.setTracing(true);          RouteDefinition searchRoute = context.getRouteDefinition("memberSearchRequest");          searchRoute.to(searchResult);           RouteDefinition lookupRoute = context.getRouteDefinition("localMemberLookup");          lookupRoute.adviceWith(context, new AdviceWithRouteBuilder() {                  @Override                  public void configure() throws Exception {                          weaveById("sqlParams").after().to(lookupHeaders);                  }          });          lookupRoute.to(lookupResult);          context.start();  }   With Camel 2.13.1, this works fine. However, with 2.14-SNAPSHOT, I get the following error:   java.lang.IllegalArgumentException: There are no outputs which matches: sqlParams in the route   Mailing list thread: http://camel.465427.n5.nabble.com/weaveById-works-with-2-13-1-not-with-2-14-SNAPSHOT-td5753809.html
CAMEL-7ad36e3d$$Failure to create producer during routing slip or similar eip causes exchange causes error handler not to react properly$$If an endpoint.createProducer throws an exception from a dynamic eip, then the exchange is kept marked as inflight, and the error handler does not react asap and as expected.  This was working in Camel 2.10.x etc.
CAMEL-eab06182$$Mock - Defining assertion on message doest work if using convertTo$$See http://www.manning-sandbox.com/thread.jspa?threadID=41025&tstart=0   The reason is when you use a method in the fluent builder that returns a ValueBuilder then that didn't detect the predicate.
CAMEL-19b2aa31$$Regression: MDC may lose values after when Async Routing Engine is used$$CAMEL-6377 introduced some optimisations in the MDC Logging mechanism which make it lose MDC values when the async routing engine is used.  If we are using an async component such as CXF, the response or done() callback will be issued from a thread NOT managed by Camel. Therefore, we need the MDCCallback to reset *ALL* MDC values, not just the routeId (as was intended by the commits that caused the regression).  The situation may be salvaged by the fact that underlying MDC implementations use an InheritableThreadLocal, so the first requests after system initialisation may see correct behaviour, because the MDC values from the requesting thread is propagated to the newly initialised threads in the underlying stack's ThreadPool, as the coreThreads are being initialised within the context of the original threads which act like parent threads.  But after those first attempts, odd behaviour is seen and all responses from the async endpoint come back without an MDC.
CAMEL-d57f402b$$XSD decoding bad guess in Validator$$Validator component does not take imported XSD encoding into account when validating XML. That may lead to validation errors if an imported XSD is ISO-8859-1 encoded and containing non ASCII caracters, even though that XSD declares its encoding correctly in its XML prolog.
CAMEL-799b45df$$CircuitBreakerLoadBalancer fails on async processors$$The CircuitBreakerLoadBalancer works fine on direct synchronous processor, but it seems to not behave as expected in case of async processor.  To reproduce the error, it's enough to add a .threads(1) before the mock processor in the CircuitBreakerLoadBalancerTest routeBuilder configuration.  This misbehaviour seems to be related to the use of the AsyncProcessorConverterHelper to force any processor to behave like asynchronous.   I'm going to propose a patch with the failing test and a proposal of solution.  EDIT:  the patch contains the fix also to other unexpected behaviour of the CircuitBreaker.  The second problem addressed is that, after the opening of the circuit, the RejectedExecutionException raised by the circuit breaker is set in the Exchange, but it doesn't return. This cause the processor will receive the Exchange even if the circuit is open. In this case also, if the CircuitBreaker is instructed to react only to specific Exception, it will close the circuit after the following request, because the raised exception would be a RejectedExecutionException instead of the one specified in the configuration.
CAMEL-d581c4a4$$IdempotentConsumer - If no messageId should allow Camel error handler to react$$See SO http://stackoverflow.com/questions/26453348/camel-onexception-doesnt-catch-nomessageidexception-of-idempotentconsumer  The idempotent consumer should set the exchange on the exchange and invoke the callback, that is an internal routing engine bug in the implementation of that eip.
CAMEL-cac72b14$$Memory leak when adding/removing a lot of routes$$Dynamically adding/removing routes to camel causes registrations in org.apache.camel.builder.ErrorHandlerBuilderRef.handlers (Map<RouteContext, ErrorHandlerBuilder>) for RouteContext instances. Those never get removed and can cause leaks if memory consuming objects are attached in the RouteContext for example constant definitions.
CAMEL-2e985f9b$$Multicast Aggregator should keep processing other exchange which is not timeout$$It makes sense the multicast aggregator keep processing the exchange even some exchange are timeout.  Here is [a thread|http://camel.465427.n5.nabble.com/Multicast-with-multiple-timeouts-tp5759576p5759646.html] in the camel user mailing list talks about it.
CAMEL-39ccf5d6$$XML parsing error is ignored by xtoknize XML tokenizer$$XML parsing exceptions are ignored by xtokenize XML tokenizer and this is leading to the same token extracted repeated times.
CAMEL-36e7b668$$PropertyInject gives NullPointerException$$Using the annotation @PropertyInject on a field of the RouteBuilder class gives a NullPointerException  public class RouteBuilder extends SpringRouteBuilder { 	 	@PropertyInject("foo.bar") 	private String fooBar;         ... }  Using the {{ }} notation in endpoint URIs is working though.
CAMEL-53b4e90c$$Simple language does not resolve overloaded method calls$$I am having an issue with the Simple language. I have a property named {{myFile}} with a value of a {{java.nio.file.Path}} object. When I try to use the following expression {noformat}  {property.file.getFileName} {noformat} in order to invoke the getFileName() method I get an exception saying: {noformat} Ambiguous method invocations possible: [public sun.nio.fs.UnixPath.getFileName(), public abstract java.nio.file.Path java.nio.file.Path.getFileName()] {noformat}  I am able to use SpEL if I do {noformat} #{properties[myFile].getFileName()} {noformat}  It would be nice if Simple supported this as well so I wouldn't have to go through hoops in order to use SpEL since I can't use SpEL to specify parameters in a uri.
CAMEL-17475d80$$Starting and stopping routes leak threads$$Seems to be identical consequence as with previous issue CAMEL-5677, but perhaps due to a different cause.  Having a file or SFTP based route, trying something like: {code} for (int i = 0; i < 50; i++) {     camelContext.startRoute(routeId);     camelContext.stopRoute(routeId); } {code} results in 50 orphan threads of this type:  {code} "Camel (camel) thread #231 - sftp://user@host/path" #10170 daemon prio=5 os_prio=0 tid=0x00007fa4b46a5800 nid=0x10fc waiting on condition [0x00007fa452934000]    java.lang.Thread.State: TIMED_WAITING (parking)       at sun.misc.Unsafe.park(Native Method)       - parking to wait for  <0x00000000b83dc900> (a java.util.concurrent.locks.AbstractQueuedSynchronizer ConditionObject)       at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)       at java.util.concurrent.locks.AbstractQueuedSynchronizer ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)       at java.util.concurrent.ScheduledThreadPoolExecutor DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)       at java.util.concurrent.ScheduledThreadPoolExecutor DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)       at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)       at java.util.concurrent.ThreadPoolExecutor Worker.run(ThreadPoolExecutor.java:617)       at java.lang.Thread.run(Thread.java:745) {code}  Switching to suspend/resume solves the problem, however I guess the start/stop issue should be addressed.
CAMEL-54d7fc59$$Using exchangePattern=InOnly in to uris are not used$$Related to CAMEL-5301  Which was implemented for recipient list. But the same thing should be fixed/implemented for send processor as well.  See nabble http://camel.465427.n5.nabble.com/Rest-DSL-org-apache-camel-ExchangeTimedOutException-The-OUT-message-was-not-received-within-20000-mis-tp5761530.html
CAMEL-dd0f74c0$$Circuit breaker does not honour halfOpenAfter period$$The CircuitBreakerLoadBalancer will always switch to a half-open state immediately after the first rejected message instead of honouring the halfOpenAfter period.  It's due to the failed message count getting reset in the rejectExchange method: https://github.com/apache/camel/blob/master/camel-core/src/main/java/org/apache/camel/processor/loadbalancer/CircuitBreakerLoadBalancer.java#L207
CAMEL-57f72cd9$$NPE in AbstractListAggregationStrategy if empty list$$See nabble http://camel.465427.n5.nabble.com/NullPointerException-on-empty-List-in-AbstractListAggregationStrategy-tp5764965.html
CAMEL-597883fa$$Bean component - Potential NPE in BeanInfo$$See nabble http://camel.465427.n5.nabble.com/transformers-not-working-after-update-to-2-15-1-tp5765600.html
CAMEL-d063f471$$Leaking exchangesInFlightKeys in ManagedRoute$$Having a camel context with a single route: {code}         onException(Throwable.class)                 .handled(true)                 .process(handleException()); // essentially  doing exchange.setException(someConvertedException);          from("direct:generalFlow")                 .routingSlip(property(GeneralFlowRoute.class.getName())); {code}  started from Spring: {code}     <camelContext id="flows" xmlns="http://camel.apache.org/schema/spring">         <template id="template" defaultEndpoint="direct:generalFlow"/>         <routeBuilder ref="generalFlow"/>     </camelContext>      <bean id="generalFlow" class="com.blabla.GeneralFlowRoute"/> {code}  During performance test both exchangesInFlightKeys  and exchangesInFlightStartTimestamps are accumulating over time.  But if the test is run in one thread with debug - nothing is accumulated.  Issue found after migration from 2.14.1 to 2.15.1
CAMEL-7b1253db$$Lock information is not handovered together with Exchange on-completion synchronizations$$This applies to the file components when using common read-lock strategies:  - *markerFile* - org.apache.camel.component.file.strategy.MarkerFileExclusiveReadLockStrategy - *fileLock* - org.apache.camel.component.file.strategy.FileLockExclusiveReadLockStrategy  This strategies stores lock information in the Exchange properties:  - *Exchange.FILE_LOCK_FILE_ACQUIRED* == "CamelFileLockFileAcquired" - *Exchange.FILE_LOCK_FILE_NAME* == "CamelFileLockFileName" - *Exchange.FILE_LOCK_EXCLUSIVE_LOCK* == "CamelFileLockExclusiveLock" - *Exchange.FILE_LOCK_RANDOM_ACCESS_FILE* == "CamelFileLockRandomAccessFile"  Lock information is stored as scalar values and can hold information about _only one single lock_.  When there are two Exchanges participates in the route, share UoW, and synchronizations are handovered from one Exchange to another, information about both locks can't be stored in the Exchange properties and lost. Consequently when on-completion synchronizations are performed, read-lock strategies can't access information about all the locks and they are not released.  For example, after completing this route lock for file1.dat is not released: {code:java} from("file:data/input-a?fileName=file1.dat&readLock=markerFile")     .pollEnrich("file:data/input-b?fileName=file2.dat&readLock=markerFile")     .to("mock:result"); {code}
CAMEL-ea8ee025$$CamelContext - API for control routes may cause Route not to update it state$$See CAMEL-8963  Its the Route instance that do not update it state as well. But the RouteService has the correct state. So one can be Started and the other Suspended.
CAMEL-108d94f7$$Bean component - Should filter out abstract methods$$If you call a method on a bean then the introspector should filter out abstract methods if there is class inheritance with abstract defined methods.  See SO http://stackoverflow.com/questions/31671894/camel-ambiguousmethodcallexception-abstract-classes
CAMEL-9da2c05a$$RedeliveryPattern should support property placeholders$$See nabble http://camel.465427.n5.nabble.com/Can-t-configure-delayPattern-with-property-placeholders-tp5771356.html
CAMEL-08077733$$Producers that implement the ServicePoolAware interface cause memory leak due to JMX references$$h4. Description  Producer instances that implement the ServicePoolAware interface will leak memory if their route is stopped, with new producers being leaked every time the route is started/stopped.  Known implementations that are affected are RemoteFileProducer (ftp, sftp) and Mina2Producer.  This is due to the behaviour that the SendProcessor which when the route is stopped it shuts down it's `producerCache` instance.  {code}     protected void doStop() throws Exception {         ServiceHelper.stopServices(producerCache, producer);     } {code}  this in turn calls `stopAndShutdownService(pool)` which will call stop on the SharedProducerServicePool instance which is a NOOP however it also calls shutdown which effects a stop of the global pool (this stops all the registered services and then clears the pool.  {code}     protected void doStop() throws Exception {         // when stopping we intend to shutdown         ServiceHelper.stopAndShutdownService(pool);         try {             ServiceHelper.stopAndShutdownServices(producers.values());         } finally {             // ensure producers are removed, and also from JMX             for (Producer producer : producers.values()) {                 getCamelContext().removeService(producer);             }         }         producers.clear();     } {code}  However no call to `context.removeService(Producer) is called for the entries from the pool only those singleton instances that were in the `producers` map hence the JMX `ManagedProducer` that is created when `doGetProducer` invokes {code}                getCamelContext().addService(answer, false); {code} is never removed.   Since the global pool is empty when the next request to get a producer is called a new producer is created, jmx wrapper and all, whilst the old instance remains orphaned retaining any objects that pertain to that instance.  One workaround is for the producer to call {code}getEndpoint().getCamelContext().removeService(this){code} in it's stop method, however this is fairly obscure and it would probably be better to invoke removal of the producer when it is removed from the shared pool.  Another issue of note is that when a route is shutdown that contains a SendProcessor due to the shutdown invocation on the SharedProcessorServicePool the global pool is cleared of `everything` and remains in `Stopped` state until another route starts it (although it is still accessed and used whilst in the `Stopped` state).  h4. Impact  For general use where there is no dynamic creation or passivation of routes this issue should be minimal, however in our use case where the routes are not static, there is a certain amount of recreation of routes as customer endpoints change and there is a need to passivate idle routes this causes a considerable memory leak (via SFTP in particular).  h4. Test Case {code} package org.apache.camel.component;  import com.google.common.util.concurrent.AtomicLongMap;  import org.apache.camel.CamelContext; import org.apache.camel.Consumer; import org.apache.camel.Endpoint; import org.apache.camel.Exchange; import org.apache.camel.Processor; import org.apache.camel.Producer; import org.apache.camel.Route; import org.apache.camel.Service; import org.apache.camel.ServicePoolAware; import org.apache.camel.ServiceStatus; import org.apache.camel.builder.RouteBuilder; import org.apache.camel.impl.DefaultComponent; import org.apache.camel.impl.DefaultEndpoint; import org.apache.camel.impl.DefaultProducer; import org.apache.camel.support.LifecycleStrategySupport; import org.apache.camel.support.ServiceSupport; import org.apache.camel.test.junit4.CamelTestSupport; import org.junit.Test;  import java.util.Map;  import static com.google.common.base.Preconditions.checkNotNull;  /**  * Test memory behaviour of producers using {@link ServicePoolAware} when using JMX.  */ public class ServicePoolAwareLeakyTest extends CamelTestSupport {    private static final String LEAKY_SIEVE_STABLE = "leaky://sieve-stable?plugged=true";   private static final String LEAKY_SIEVE_TRANSIENT = "leaky://sieve-transient?plugged=true";     private static boolean isPatchApplied() {     return Boolean.parseBoolean(System.getProperty("patchApplied", "false"));   }    /**    * Component that provides leaks producers.    */   private static class LeakySieveComponent extends DefaultComponent {     @Override     protected Endpoint createEndpoint(String uri, String remaining, Map<String, Object> parameters) throws Exception {       boolean plugged = "true".equalsIgnoreCase((String) parameters.remove("plugged"));       return new LeakySieveEndpoint(uri, isPatchApplied() && plugged);     }   }    /**    * Endpoint that provides leaky producers.    */   private static class LeakySieveEndpoint extends DefaultEndpoint {      private final String uri;     private final boolean plugged;      public LeakySieveEndpoint(String uri, boolean plugged) {       this.uri = checkNotNull(uri, "uri must not be null");       this.plugged = plugged;     }      @Override     public Producer createProducer() throws Exception {       return new LeakySieveProducer(this, plugged);     }      @Override     public Consumer createConsumer(Processor processor) throws Exception {       throw new UnsupportedOperationException();     }      @Override     public boolean isSingleton() {       return true;     }      @Override     protected String createEndpointUri() {       return uri;     }   }    /**    * Leaky producer - implements {@link ServicePoolAware}.    */   private static class LeakySieveProducer extends DefaultProducer implements ServicePoolAware {      private final boolean plugged;      public LeakySieveProducer(Endpoint endpoint, boolean plugged) {       super(endpoint);       this.plugged = plugged;     }      @Override     public void process(Exchange exchange) throws Exception {       // do nothing     }      @Override     protected void doStop() throws Exception {       super.doStop();        //noinspection ConstantConditions       if (plugged) {         // need to remove self from services since we are ServicePoolAware this will not be handled for us otherwise we         // leak memory         getEndpoint().getCamelContext().removeService(this);       }     }   }    @Override   protected boolean useJmx() {     // only occurs when using JMX as the GC root for the producer is through a ManagedProducer created by the     // context.addService() invocation     return true;   }    /**    * Returns true if verification of state should be performed during the test as opposed to at the end.    */   public boolean isFailFast() {     return false;   }    /**    * Returns true if during fast failure we should verify that the service pool remains in the started state.    */   public boolean isVerifyProducerServicePoolRemainsStarted() {     return false;   }    @Override   public boolean isUseAdviceWith() {     return true;   }    @Test   public void testForMemoryLeak() throws Exception {     registerLeakyComponent();      final AtomicLongMap<String> references = AtomicLongMap.create();      // track LeakySieveProducer lifecycle     context.addLifecycleStrategy(new LifecycleStrategySupport() {       @Override       public void onServiceAdd(CamelContext context, Service service, Route route) {         if (service instanceof LeakySieveProducer) {           references.incrementAndGet(((LeakySieveProducer) service).getEndpoint().getEndpointKey());         }       }        @Override       public void onServiceRemove(CamelContext context, Service service, Route route) {         if (service instanceof LeakySieveProducer) {           references.decrementAndGet(((LeakySieveProducer) service).getEndpoint().getEndpointKey());         }       }     });      context.addRoutes(new RouteBuilder() {       @Override       public void configure() throws Exception {         from("direct:sieve-transient")             .id("sieve-transient")             .to(LEAKY_SIEVE_TRANSIENT);          from("direct:sieve-stable")             .id("sieve-stable")             .to(LEAKY_SIEVE_STABLE);       }     });      context.start();      for (int i = 0; i < 1000; i++) {       ServiceSupport service = (ServiceSupport) context.getProducerServicePool();       assertEquals(ServiceStatus.Started, service.getStatus());       if (isFailFast()) {         assertEquals(2, context.getProducerServicePool().size());         assertEquals(1, references.get(LEAKY_SIEVE_TRANSIENT));         assertEquals(1, references.get(LEAKY_SIEVE_STABLE));       }        context.stopRoute("sieve-transient");        if (isFailFast()) {         assertEquals("Expected no service references to remain", 0, references.get(LEAKY_SIEVE_TRANSIENT));       }        if (isFailFast()) {         // looks like we cleared more than just our route, we've stopped and cleared the global ProducerServicePool         // since SendProcessor.stop() invokes ServiceHelper.stopServices(producerCache, producer); which in turn invokes         // ServiceHelper.stopAndShutdownService(pool);.         //         // Whilst stop on the SharedProducerServicePool is a NOOP shutdown is not and effects a stop of the pool.          if (isVerifyProducerServicePoolRemainsStarted()) {          assertEquals(ServiceStatus.Started, service.getStatus());         }         assertEquals("Expected one stable producer to remain pooled", 1, context.getProducerServicePool().size());         assertEquals("Expected one stable producer to remain as service", 1, references.get(LEAKY_SIEVE_STABLE));       }        // Send a body to verify behaviour of send producer after another route has been stopped       sendBody("direct:sieve-stable", "");        if (isFailFast()) {         // shared pool is used despite being 'Stopped'         if (isVerifyProducerServicePoolRemainsStarted()) {           assertEquals(ServiceStatus.Started, service.getStatus());         }          assertEquals("Expected only stable producer in pool", 1, context.getProducerServicePool().size());         assertEquals("Expected no references to transient producer", 0, references.get(LEAKY_SIEVE_TRANSIENT));         assertEquals("Expected reference to stable producer", 1, references.get(LEAKY_SIEVE_STABLE));       }        context.startRoute("sieve-transient");        // ok, back to normal       assertEquals(ServiceStatus.Started, service.getStatus());       if (isFailFast()) {         assertEquals("Expected both producers in pool", 2, context.getProducerServicePool().size());         assertEquals("Expected one transient producer as service", 1, references.get(LEAKY_SIEVE_TRANSIENT));         assertEquals("Expected one stable producer as service", 1, references.get(LEAKY_SIEVE_STABLE));       }     }      if (!isFailFast()) {       assertEquals("Expected both producers in pool", 2, context.getProducerServicePool().size());        // if not fixed these will equal the number of iterations in the loop + 1       assertEquals("Expected one transient producer as service", 1, references.get(LEAKY_SIEVE_TRANSIENT));       assertEquals("Expected one stable producer as service", 1, references.get(LEAKY_SIEVE_STABLE));     }   }    private void registerLeakyComponent() {     // register leaky component     context.addComponent("leaky", new LeakySieveComponent());   } } {code}
CAMEL-e7ac45b6$$URI validation verifies usage of & char incorrectly$$Hello Camel team,  I have faced a URI validation issue that does not allow me to use a correct file producer URI if it does not contain parameters. My file endpoint URI looks like this: {code} raw form:             file:D:\camel_test\test&run in encoded format:  file:D%3A%5Ccamel_test%5Ctest%26run {code}  As you can see this is a simple file endpoint URI which does not contain any parameters, please note that the target folder name contains '&' char.  When I try to start a route for this endpoint I get the following error: {code} Caused by: org.apache.camel.ResolveEndpointFailedException: Failed to resolve endpoint: file://D:%5Ccamel_test%5Ctest&run due to: Failed to resolve endpoint: file://D:%5Ccamel_test%5Ctest&run due to: Invalid uri syntax: no ? marker however the uri has & parameter separators. Check the uri if its missing a ? marker. at org.apache.camel.impl.DefaultCamelContext.getEndpoint(DefaultCamelContext.java:547) at org.apache.camel.util.CamelContextHelper.getMandatoryEndpoint(CamelContextHelper.java:72) at org.apache.camel.model.RouteDefinition.resolveEndpoint(RouteDefinition.java:202) at org.apache.camel.impl.DefaultRouteContext.resolveEndpoint(DefaultRouteContext.java:107) at org.apache.camel.impl.DefaultRouteContext.resolveEndpoint(DefaultRouteContext.java:113) at org.apache.camel.model.SendDefinition.resolveEndpoint(SendDefinition.java:61) at org.apache.camel.model.SendDefinition.createProcessor(SendDefinition.java:55) at org.apache.camel.model.ProcessorDefinition.makeProcessor(ProcessorDefinition.java:500) at org.apache.camel.model.ProcessorDefinition.addRoutes(ProcessorDefinition.java:213) at org.apache.camel.model.RouteDefinition.addRoutes(RouteDefinition.java:942) Caused by: org.apache.camel.ResolveEndpointFailedException: Failed to resolve endpoint: file://D:%5Ccamel_test%5Ctest&run due to: Invalid uri syntax: no ? marker however the uri has & parameter separators. Check the uri if its missing a ? marker. at org.apache.camel.impl.DefaultComponent.validateURI(DefaultComponent.java:210) at org.apache.camel.impl.DefaultComponent.createEndpoint(DefaultComponent.java:115) at org.apache.camel.impl.DefaultCamelContext.getEndpoint(DefaultCamelContext.java:527) {code}  The issue is that I have an '&' char in my folder path and do not have URI parameters. As a result the DefaultComponent.validateURI() function throws the exception at this point: {code} if (uri.contains("&") && !uri.contains("?")) {             throw new ResolveEndpointFailedException(uri, "Invalid uri syntax: no ? marker however the uri "                 + "has & parameter separators. Check the uri if its missing a ? marker.");         } {code}  I think the issue is that for some reason & char at this point is in decoded form which is incorrect because it should be URI encoded here. I know that Camel has issues with + and & chars and in case of URI parameters we can use RAW() wrapper as a workaround. But as far as I can see we cannot do the same in base endpoint URI.  Please note that if I add an URI parameter (any parameter works) the issue disappears and the event target works fine. For example: {code} raw form:             file:D:\camel_test\test&run?forceWrites=true in encoded format:  file:D%3A%5Ccamel_test%5Ctest%26run?forceWrites=true {code}  I also found that Camel cannot normally handle URIs with ? char in base URI string. For example in case of File endpoint it tries to handle the second part of the base URI (which follows the ? char) as a parameter which is incorrect because ? char was correctly encoded and should not be used as the point where parameters string starts. If I try to use ? char in bucket name for S3 endpoint the part after ? is simply ignored. For example the following URI produces an error: {code} raw form:             file:D:\camel_test\test?run?forceWrites=true in encoded format:  file:D%3A%5Ccamel_test%5Ctest%3Frun?forceWrites=true {code} Error: {code} 2015-10-13 13:26:38,285 INFO [com.informatica.saas.infaagentv3.agentcore.TomcatManager] - Caused by: org.apache.camel.ResolveEndpointFailedException: Failed to resolve endpoint: file://D:%5Ccamel_test%5Ctest?run%3FforceWrites=true due to: There are 1 parameters that couldn't be set on the endpoint. Check the uri if the parameters are spelt correctly and that they are properties of the endpoint. Unknown parameters=[{run?forceWrites=true}] at org.apache.camel.impl.DefaultComponent.validateParameters(DefaultComponent.java:192) at org.apache.camel.impl.DefaultComponent.createEndpoint(DefaultComponent.java:137) at org.apache.camel.impl.DefaultCamelContext.getEndpoint(DefaultCamelContext.java:527) {code}  And yes I know that ? char in a file path is not the best idea, I used the above example to illustrate a global camel issue.
CAMEL-169b981e$$NPE while GenericFile.changeFileName$$If a relative file path is specified for the {{move}} or {{moveFailed}} Attribute of the file2 component, a NullPointerException is thrown while processing the onCompletion commit resp. rollback strategy.  And because the processed file cannot be moved away, the processing is restarted again and so on...  Wrong code line (GenericFile.java:203 in camel-core V2.15.3): {code:java} ObjectHelper.after(newFileName, newEndpointPath + File.separatorChar); {code} when {{newFileName}} and {{newEndpointPath}} are both relative paths.   Stacktrace: {code:java} java.lang.NullPointerException 	at java.io.File.<init>(File.java:277) ~[?:1.8.0_60] 	at org.apache.camel.component.file.GenericFile.changeFileName(GenericFile.java:207) ~[camel-core-2.15.3.jar:2.15.3] 	at org.apache.camel.component.file.strategy.GenericFileExpressionRenamer.renameFile(GenericFileExpressionRenamer.java:41) ~[camel-core-2.15.3.jar:2.15.3] 	at org.apache.camel.component.file.strategy.GenericFileRenameProcessStrategy.commit(GenericFileRenameProcessStrategy.java:87) ~[camel-core-2.15.3.jar:2.15.3] 	at org.apache.camel.component.file.GenericFileOnCompletion.processStrategyCommit(GenericFileOnCompletion.java:124) [camel-core-2.15.3.jar:2.15.3] 	at org.apache.camel.component.file.GenericFileOnCompletion.onCompletion(GenericFileOnCompletion.java:80) [camel-core-2.15.3.jar:2.15.3] 	at org.apache.camel.component.file.GenericFileOnCompletion.onComplete(GenericFileOnCompletion.java:54) [camel-core-2.15.3.jar:2.15.3] 	at org.apache.camel.util.UnitOfWorkHelper.doneSynchronizations(UnitOfWorkHelper.java:104) [camel-core-2.15.3.jar:2.15.3] 	at org.apache.camel.impl.DefaultUnitOfWork.done(DefaultUnitOfWork.java:229) [camel-core-2.15.3.jar:2.15.3] 	at org.apache.camel.util.UnitOfWorkHelper.doneUow(UnitOfWorkHelper.java:65) [camel-core-2.15.3.jar:2.15.3] 	at org.apache.camel.processor.CamelInternalProcessor UnitOfWorkProcessorAdvice.after(CamelInternalProcessor.java:637) [camel-core-2.15.3.jar:2.15.3] 	at org.apache.camel.processor.CamelInternalProcessor UnitOfWorkProcessorAdvice.after(CamelInternalProcessor.java:605) [camel-core-2.15.3.jar:2.15.3] 	at org.apache.camel.processor.CamelInternalProcessor InternalCallback.done(CamelInternalProcessor.java:239) [camel-core-2.15.3.jar:2.15.3] 	at org.apache.camel.processor.Pipeline.process(Pipeline.java:106) [camel-core-2.15.3.jar:2.15.3] 	at org.apache.camel.processor.CamelInternalProcessor.process(CamelInternalProcessor.java:190) [camel-core-2.15.3.jar:2.15.3] 	at org.apache.camel.component.file.GenericFileConsumer.processExchange(GenericFileConsumer.java:439) [camel-core-2.15.3.jar:2.15.3] 	at org.apache.camel.component.file.GenericFileConsumer.processBatch(GenericFileConsumer.java:211) [camel-core-2.15.3.jar:2.15.3] 	at org.apache.camel.component.file.GenericFileConsumer.poll(GenericFileConsumer.java:175) [camel-core-2.15.3.jar:2.15.3] 	at org.apache.camel.impl.ScheduledPollConsumer.doRun(ScheduledPollConsumer.java:174) [camel-core-2.15.3.jar:2.15.3] 	at org.apache.camel.impl.ScheduledPollConsumer.run(ScheduledPollConsumer.java:101) [camel-core-2.15.3.jar:2.15.3] 	at java.util.concurrent.Executors RunnableAdapter.call(Executors.java:511) [?:1.8.0_60] 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [?:1.8.0_60] 	at java.util.concurrent.ScheduledThreadPoolExecutor ScheduledFutureTask.access 301(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_60] 	at java.util.concurrent.ScheduledThreadPoolExecutor ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [?:1.8.0_60] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_60] 	at java.util.concurrent.ThreadPoolExecutor Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_60] {code}
CAMEL-1957a828$$Invocation of Bean fails when Bean extends and abstract which implements the actual method$$The issue described here does NOT exist in 2.15.2 and only manifests in 2.15.3.  With the following definition of a Bean:  {code}     public interface MyBaseInterface {         @Handler         String hello(@Body String hi);     }      public abstract static class MyAbstractBean implements MyBaseInterface {         public String hello(@Body String hi) {             return "Hello " + hi;         }         public String doCompute(String input) {             fail("Should not invoke me");             return null;         }     }      public static class MyConcreteBean extends MyAbstractBean {     }  {code}  The following test case will fail to invoke the proper method:  {code} public class BeanHandlerMethodTest extends ContextTestSupport {      public void testInterfaceBeanMethod() throws Exception {         BeanInfo info = new BeanInfo(context, MyConcreteBean.class);          Exchange exchange = new DefaultExchange(context);         MyConcreteBean pojo = new MyConcreteBean();         MethodInvocation mi = info.createInvocation(pojo, exchange);         assertNotNull(mi);         assertEquals("hello", mi.getMethod().getName());     } {code}  The issue is how BeanInfo.introspect determines which methods are available to be invoked.  At line 344, if the class is public, the interface methods are added to the list:  {code}         if (Modifier.isPublic(clazz.getModifiers())) {             // add additional interface methods             List<Method> extraMethods = getInterfaceMethods(clazz);             for (Method target : extraMethods) {                 for (Method source : methods) {                     if (ObjectHelper.isOverridingMethod(source, target, false)) {                         overrides.add(target);                     }                 }             }             // remove all the overrides methods             extraMethods.removeAll(overrides);             methods.addAll(extraMethods);         } {code}  However, all the methods from the interface are "abstract".  Later, when the real implementation is encountered as the code crawls up the tree, the abstract method is not replaced:  Line 390:  {code}         MethodInfo existingMethodInfo = overridesExistingMethod(methodInfo);         if (existingMethodInfo != null) {             LOG.trace("This method is already overridden in a subclass, so the method from the sub class is preferred: {}", existingMethodInfo);             return existingMethodInfo;         } {code}  Finally, during the invocation, the following was added as part of 2.15.3 release:  Line 561:  {code}         removeAllAbstractMethods(localOperationsWithBody);         removeAllAbstractMethods(localOperationsWithNoBody);         removeAllAbstractMethods(localOperationsWithCustomAnnotation);         removeAllAbstractMethods(localOperationsWithHandlerAnnotation); {code}  As a result, the abstract method is removed and not invoked.  I think the fix should be to see if the existingMethodInfo references an "abstract' method and if it does and methodInfo does not, replace the existingMethodInfo with methodInfo in the collection.  This would preserve the preferences implied with the rest of the code while properly replacing the abstract method with their proper implementations.
CAMEL-62b2042b$$NotifyBuilder.fromRoute() does not work for some endpoint types$${{NotifyBuilder.fromRoute()}} does not work if the endpoint uri in the {{from()}} clause for a route does not match the actual endpoint uri the exchange was sent to. Because we also have the route id itself available in the exchange, we can use that as a fallback when the match on from endpoint uri doesn't work.
CAMEL-1cab39f6$$FileIdempotentRepository fails to create fileStore when no path is specified$$I create a FileIdempotentRepository like this:  {code} .idempotentConsumer(fileIdempotentRepository(new File('ids'))) {     it.in.body.id } {code}  I get an error, and I traced it to: {noformat} Caused by: java.lang.NullPointerException: null 	at org.apache.camel.processor.idempotent.mpotentRepository.loadStore(FileIdempotentRepository.java:293) ~[camel-core-2.16.0.jar:2.16.0] 	at org.apache.camel.processor.idempotent.FileIdempotentRepository.doStart(FileIdempotentRepository.java:328) ~[camel-core-2.16.0.jar:2.16.0] {noformat}  The FileIdempotentRepository is trying to create the parent directory of the file that was specified for the file store. If a path to the file is not specified, then getParentFile() returns null. Calling .mkdirs() on that bombs.  This route works the second time it runs because then the file exists. It also works if I specify my file name as "./ids" instead of "ids".
CAMEL-baece126$$Incorrect exceptions handling from Splitter$$Steps to reproduce: 1. Create global onException handler {code} <onException>     <exception>java.lang.Exception</exception>     <handled>         <constant>false</constant>     </handled>     <log message="SOME MESSAGE"/> </onException> {code}  2. Create 2 routes with Splitter (set shareUnitOfWork to TRUE, important) {code} <route>     <from uri="timer://foo?repeatCount=1"/>      <!-- Add some value list to body here -->      <split shareUnitOfWork="true" stopOnException="true">         <simple> {body}</simple>         <to uri="direct:handleSplit"/>     </split> </route>  <route>     <from uri="direct:handleSplit"/>     <throwException ref="myException"/> </route> {code}  Expected: string "SOME MESSAGE" is logged Actual:  <log message="SOME MESSAGE"/> is not executed at all
CAMEL-0ead2cac$$IdempotentConsumer - If exception from repo it should be able to handle by onException$$See nabble http://camel.465427.n5.nabble.com/Exception-from-idempotentConsumer-not-propagating-to-onException-tp5775779.html
CAMEL-9a6e6d8a$$Simple backwards parser bug if using file$$See nabble http://camel.465427.n5.nabble.com/Unknown-File-Language-Syntax-tp5778208.html
CAMEL-da035952$$Safe copy of DefaultExchange does not propagate 'fault' property$${{fault}} property should be copied in the following places:  https://github.com/apache/camel/blob/master/camel-core/src/main/java/org/apache/camel/impl/DefaultExchange.java#L100 https://github.com/apache/camel/blob/master/camel-core/src/main/java/org/apache/camel/impl/DefaultExchange.java#L107  Consequences: {{DefaultExchange#isFault()}} does not work if {{exception}} property is not set.
CAMEL-84922699$$ClassCastException with interceptFrom$$If a statement like  {code:java} interceptFrom().when(simple(" {header.foo} == 'bar'")).to("mock:intercepted"); {code}  is available in a route builder with JMX enabled the startup will fail in Camel 2.16.2 (and the current 2.17-SNAPSHOT) with a ClassCastException in line 310 of DefaultManagementObjectStrategy.  The generated processor is a FilterProcessor, but the resulting definition is a WhenDefinition not a FilterDefinition.  The reason is that CAMEL-8992 introduced a too precise class check for this.  The attached patch relexes the class constraint on the definition.
CAMEL-7944093f$$doTry .. doFinally should run the finally block for fault messages also$$If a message has fault flag, then a doFinally block is only executed the first processor. We should ensure the entire block is processed like we do if an exception was thrown. The same kind of logic should apply for fault.
CAMEL-4d03e9de$$seda - discardIfNoConsumers=true do not call on completions$$See SO http://stackoverflow.com/questions/35938139/how-to-release-file-lock-with-camel-when-not-consuming-from-seda-queue/35940850#35940850
LOG4J2-0c20bfd8$$org.apache.logging.log4j.core.config.plugins.util.ResolverUtil.extractPath(URL) incorrectly converts '+' characters to spaces$$org.apache.logging.log4j.core.config.plugins.util.ResolverUtil.extractPath(URL) incorrectly converts '+' characters to spaces.
LOG4J2-c79a743b$$Attribute "format" for SyslogAppender is mandatory$$In the SyslogAppender the configuration attribute "Format" must be present, otherwise a NullPointerException is thrown in method "createAppender" when the following statement is executed:  if (format.equalsIgnoreCase(RFC5424))  The fix is simply to add a check so the "equalsIgnoreCase" is called only if the format variable isn't Null.
LOG4J2-a96b455c$$Custom java.util.logging.Level gives null Log4j Level and causes NPE$$I use a 3rd party library which uses custom non-standard java.util.logging.Level.  The Log4j JUL adapter will emit log event with level set to null in that case, which causes NullPointerException in a Log4j filter further on.  This is not acceptable. When encountering an unrecognised JUL Level, the JUL adapter should either:  - emit some default Log4j Level - throw an Exception with a clear error message immediately - silently discard the log event - discard the log event and log a warning to the StatusLogger  {code}  java.lang.NullPointerException         at org.apache.logging.log4j.Level.isMoreSpecificThan(Level.java:163)         at org.apache.logging.log4j.core.filter.BurstFilter.filter(BurstFilter.java:129)         at org.apache.logging.log4j.core.filter.BurstFilter.filter(BurstFilter.java:101)         at org.apache.logging.log4j.core.Logger PrivateConfig.filter(Logger.java:295)         at org.apache.logging.log4j.core.Logger.isEnabled(Logger.java:122)         at org.apache.logging.log4j.spi.ExtendedLoggerWrapper.isEnabled(ExtendedLoggerWrapper.java:87)         at org.apache.logging.log4j.spi.AbstractLogger.logIfEnabled(AbstractLogger.java:699)         at org.apache.logging.log4j.jul.WrappedLogger.log(WrappedLogger.java:50)         at org.apache.logging.log4j.jul.ApiLogger.log(ApiLogger.java:106) {code}
LOG4J2-7f391872$$Bad priority in Syslog messages$$In class org.apache.logging.log4j.core.net.Priority the method getPriority returns a bad priority when used for Syslog messages (the only usage at the moment). The bug is in the statement:  facility.getCode() << 3 + Severity.getSeverity(level).getCode()  That's because the operator "+" takes precedence over "<<", and so the facility code isn't shifted by 3 but by "3 + Severity.getSeverity(level).getCode()".
LOG4J2-11960820$$Circular Exception cause throws StackOverflowError$$If an exception with a circular-referenced exception (suppressed, or otherwise) is logged, log4j will throw a StackOverflowError: {code:java}         Exception e1 = new Exception();         Exception e2 = new Exception(e1);         e1.initCause(e2);         LogManager.getLogger().error("Error", e1); {code}  Will throw the following: {code:java} java.lang.StackOverflowError 	at java.util.Vector.elementData(Vector.java:730) 	at java.util.Vector.elementAt(Vector.java:473) 	at java.util.Stack.peek(Stack.java:103) 	at org.apache.logging.log4j.core.impl.ThrowableProxy.toExtendedStackTrace(ThrowableProxy.java:555) 	at org.apache.logging.log4j.core.impl.ThrowableProxy.<init>(ThrowableProxy.java:147) 	at org.apache.logging.log4j.core.impl.ThrowableProxy.<init>(ThrowableProxy.java:148) 	at org.apache.logging.log4j.core.impl.ThrowableProxy.<init>(ThrowableProxy.java:148) 	at org.apache.logging.log4j.core.impl.ThrowableProxy.<init>(ThrowableProxy.java:148) 	at org.apache.logging.log4j.core.impl.ThrowableProxy.<init>(ThrowableProxy.java:148) 	at org.apache.logging.log4j.core.impl.ThrowableProxy.<init>(ThrowableProxy.java:148) 	at org.apache.logging.log4j.core.impl.ThrowableProxy.<init>(ThrowableProxy.java:148)         ... {code}
LOG4J2-3b12e13d$$LogManager initialization failed when running from Jdeveloper.$$This issue was incorrectly opened in bugzilla as https://issues.apache.org/bugzilla/show_bug.cgi?id=54053 by Evgeny.   Steps to Reproduce: //config file presents or not - does not meter.  Run / Debug simple application:  import org.apache.logging.log4j.LogManager; import org.apache.logging.log4j.Logger;  public class test_log {     public test_log() {         super();     }          static Logger logger = LogManager.getLogger(test_log.class.getName());      public static void main(String[] args) {         test_log test_log = new test_log();                logger.entry();          logger.debug("test");          logger.error("test Err");          logger.exit();           } ... }  Actual Results: Failed with error java.lang.ExceptionInInitializerError 	at view.test_log.<clinit>(test_log.java:13) Caused by: java.lang.ClassCastException: oracle.xml.parser.v2.DTD cannot be cast to org.w3c.dom.Element 	at java.util.XMLUtils.load(XMLUtils.java:61) 	at java.util.Properties.loadFromXML(Properties.java:852) 	at org.apache.logging.log4j.LogManager.<clinit>(LogManager.java:77)  Additional Info: When xmlparserv2.jar is deleted - application run fine. But - it have to be presents - when deleted, JDeveloper failed to start.
LOG4J2-c8fd3c53$$Log4jMarker#contains(String) does not respect org.slf4j.Marker contract$$Expected behavior ============== 'org.apache.logging.slf4j.Log4jMarker' implements 'org.slf4j.Marker'.  'org.slf4j.Marker#contains(String name)' contract states that: "If 'name' is null the returned value is always false." http://www.slf4j.org/apidocs/org/slf4j/Marker.html#contains%28java.lang.String%29  Actual behavior ============= 'org.apache.logging.slf4j.Log4jMarker#contains(final String name)'  throws 'IllegalArgumentException' if 'name' is null
LOG4J2-86d8944f$$Log4jMarker#remove(Marker) does not respect org.slf4j.Marker contract$$Passing {{null}} to {{Log4jMarker#remove(Marker)}} throws a {{NullPointerException}} instead of return {{false}}.
LOG4J2-4cf831b6$$Log4jMarker#add(Marker) does not respect org.slf4j.Marker contract$$Passing {{null}} to {{Log4jMarker#add(Marker)}} throws a {{NullPointerException}} instead of an {{IllegalArgumentException}}.
LOG4J2-4786a739$$ThrowableProxy getExtendedStackTraceAsString throws NPE on deserialized nested exceptions$$In a similar vein to LOG4J2-914, I also am attempting to use log4j as a daemon log server.  The fix for LOG4J2-914 only solved the NPE problem for one dimensional exceptions. Nested exceptions also cause an NPE in the current implementation.  Here is a test/patch diff for the bug:  {code} ---  .../org/apache/logging/log4j/core/impl/ThrowableProxy.java     |  2 +-  .../org/apache/logging/log4j/core/impl/ThrowableProxyTest.java | 10 ++++++++++  2 files changed, 11 insertions(+), 1 deletion(-)  diff --git a/log4j-core/src/main/java/org/apache/logging/log4j/core/impl/ThrowableProxy.java b/log4j-core/src/main/java/org/apache/logging/log4j/core/impl/ThrowableProxy.java index 67d55ec..307de58 100644 --- a/log4j-core/src/main/java/org/apache/logging/log4j/core/impl/ThrowableProxy.java +++ b/log4j-core/src/main/java/org/apache/logging/log4j/core/impl/ThrowableProxy.java @@ -207,7 +207,7 @@ public class ThrowableProxy implements Serializable {              return;          }          sb.append("Caused by: ").append(cause).append(EOL); -        this.formatElements(sb, cause.commonElementCount, cause.getThrowable().getStackTrace(), +        this.formatElements(sb, cause.commonElementCount, cause.getStackTrace(),                  cause.extendedStackTrace, ignorePackages);          this.formatCause(sb, cause.causeProxy, ignorePackages);      } diff --git a/log4j-core/src/test/java/org/apache/logging/log4j/core/impl/ThrowableProxyTest.java b/log4j-core/src/test/java/org/apache/logging/log4j/core/impl/ThrowableProxyTest.java index 7019aa2..6eb5dbc 100644 --- a/log4j-core/src/test/java/org/apache/logging/log4j/core/impl/ThrowableProxyTest.java +++ b/log4j-core/src/test/java/org/apache/logging/log4j/core/impl/ThrowableProxyTest.java @@ -146,6 +146,16 @@ public class ThrowableProxyTest {            assertEquals(proxy.getExtendedStackTraceAsString(), proxy2.getExtendedStackTraceAsString());      } +     +    @Test +    public void testSerialization_getExtendedStackTraceAsStringWithNestedThrowable() throws Exception { +        final Throwable throwable = new RuntimeException(new IllegalArgumentException("This is a test")); +        final ThrowableProxy proxy = new ThrowableProxy(throwable); +        final byte[] binary = serialize(proxy); +        final ThrowableProxy proxy2 = deserialize(binary); + +        assertEquals(proxy.getExtendedStackTraceAsString(), proxy2.getExtendedStackTraceAsString()); +    }        @Test      public void testSerializationWithUnknownThrowable() throws Exception { --  {code}
LOG4J2-e7bbeceb$$Exceptions not logged when using TcpSocketServer + SerializedLayout$$This issue was reported in BugZilla bug 57036: https://bz.apache.org/bugzilla/show_bug.cgi?id=57036.   The description there covers the problem well:  "... in the Method format(final LogEvent event, final StringBuilder toAppendTo) in ExtendedThrowablePatternConverter writing the Stacktrace in the logfile on condition that the Throwable throwable from Log4jLogEvent is not null, but on the Socketserver the Throwable throwable is always null, because it's defined as transient."  I couldn't find the bug here in Jira, so I'm reporting again in case it has been lost in the migration.  It's a major problem with a simple fix, so seems like it should be a high priority.  I've worked around it for now by plugging in my own ExtendedThrowablePatternConverter.  My fix is to change this line:  if (throwable != null && options.anyLines() {  to this:  if ((throwable != null || proxy != null) && options.anyLines()) {
LOG4J2-e9b628ec$$Improper handling of JSON escape chars when deserializing JSON log events$$There is an error in the handling of JSON escape characters while determining the log event boundaries in a JSON stream.  This error is causing log events with JSON escaped characters in the message string to be skipped.  The existing tests do not appear to cover this case, and other serialization types are not affected.  Here is a test/fix patch:   {code} diff --git a/log4j-core/src/main/java/org/apache/logging/log4j/core/net/server/JsonInputStreamLogEventBridge.java b/log4j-core/src/main/java/org/apache/logging/log4j/core/net/server/JsonInputStreamLogEventBridge.java index 1b81644..8ed2732 100644 --- a/log4j-core/src/main/java/org/apache/logging/log4j/core/net/server/JsonInputStreamLogEventBridge.java +++ b/log4j-core/src/main/java/org/apache/logging/log4j/core/net/server/JsonInputStreamLogEventBridge.java @@ -55,8 +55,10 @@ public class JsonInputStreamLogEventBridge extends InputStreamLogEventBridge {          boolean inEsc = false;          for (int i = start; i < charArray.length; i++) {              final char c = charArray[i]; -            if (!inEsc) { -                inEsc = false; +            if (inEsc) { +            	// Skip this char and continue +            	inEsc = false; +            } else {                   switch (c) {                  case EVENT_START_MARKER:                      if (!inStr) { diff --git a/log4j-core/src/test/java/org/apache/logging/log4j/core/net/server/AbstractSocketServerTest.java b/log4j-core/src/test/java/org/apache/logging/log4j/core/net/server/AbstractSocketServerTest.java index 891e278..2bdb3c3 100644 --- a/log4j-core/src/test/java/org/apache/logging/log4j/core/net/server/AbstractSocketServerTest.java +++ b/log4j-core/src/test/java/org/apache/logging/log4j/core/net/server/AbstractSocketServerTest.java @@ -69,7 +69,9 @@ public abstract class AbstractSocketServerTest {      private static final String MESSAGE = "This is test message";        private static final String MESSAGE_2 = "This is test message 2"; - +     +    private static final String MESSAGE_WITH_SPECIAL_CHARS = "{This}\n[is]\"n\"a\"\r\ntrue:\n\ttest,\nmessage"; +          static final int PORT_NUM = AvailablePortFinder.getNextAvailable();        static final String PORT = String.valueOf(PORT_NUM); @@ -158,6 +160,13 @@ public abstract class AbstractSocketServerTest {              testServer(m1, m2);          }      } +     +     +    @Test +    public void testMessagesWithSpecialChars() throws Exception { +        testServer(MESSAGE_WITH_SPECIAL_CHARS); +    } +            private void testServer(final int size) throws Exception {          final String[] messages = new String[size]; {code}  The test provided is simplistic and does not attempt to cover all possible special characters as the bug has to do with escaped characters in general.  XML and java serialization handle the special chars in my test string without issue - I did not attempt to locate similar cases in the other serialization types.
LOG4J2-88641f49$$Nesting pattern layout options is broken$$This pattern:  {code:xml} <PatternLayout pattern="%highlight{%d{dd MMM yyyy HH:mm:ss,SSS}{GMT+0} [%t] %-5level: %msg%n%throwable}" /> {code}  Incorrectly outputs:  {noformat} 02 Nov 2012 16:51:14,907{GMT+0 [main] FATAL: Fatal message. }02 Nov 2012 16:51:14,910{GMT+0 [main] ERROR: Error message. }02 Nov 2012 16:51:14,914{GMT+0 [main] WARN : Warning message. }02 Nov 2012 16:51:14,917{GMT+0 [main] INFO : Information message. }02 Nov 2012 16:51:14,920{GMT+0 [main] DEBUG: Debug message. }02 Nov 2012 16:51:14,924{GMT+0 [main] TRACE: Trace message. }02 Nov 2012 16:51:14,929{GMT+0 [main] ERROR: Error message. {noformat}
LOG4J2-3f41ff48$$AbstractStringLayout implements Serializable, but is not Serializable$${{org.apache.logging.log4j.core.layout.AbstractLayout}} line 34 : {code}     // TODO: Charset is not serializable. Implement read/writeObject() ?     private final Charset charset; {code}  The developer has recognised that this class claims to be serializable, but is not actually serializable.  This actually has wide impact due to the fact that the Logger is holding onto the Layout via the {{org.apache.logging.log4j.core.Logger.PrivateConfig#config}} (XML in my case). Many projects, including Spring, do not use static Loggers and prefer getClass type approaches off of their abstract classes, i.e.: {code} protected final Log logger = LogFactory.getLog(getClass()); {code}  This actually can lead to use of spring session beans, which are serialized with the session, trying to serialize the logger also and failing due to this bug.
LOG4J2-fc3e9d2d$$StructuredDataFilter defines "pairs" as attribute instead of element$$org.apache.logging.log4j.core.filter.StructuredDataFilter method createFilter defines parameter "pairs" as follows:  @PluginAttr("pairs") KeyValuePair[] pairs  It should have been:  @PluginElement("pairs") KeyValuePair[] pairs
LOG4J2-afcf92eb$$StructuredDataMessage is incorrectly validating value length instead of key length$$During execution of method SLF4JLogger.log, the following exception is thrown during creation of the StructuredMessage with a key longer than 32 characters.  java.lang.IllegalArgumentException: Structured data values are limited to 32 characters. key: memo value: This is a very long test memo to demonstrate the issue  The validation should be on key length and not the value length.
LOG4J2-8acedb4e$$Unable to define only rootLogger in a properties file.$$I've changed the version of log4j2 from *2.3* to *2.4* in order to load the  configuration via properties file. So i have converted the xml file, that defines only {{<Root>}} in {{<Loggers>}} element, into a properties file.  This is a preview of the xml file : {code:xml}   <Loggers>       <Root level="info">          <AppenderRef ref="ConsoleAppender"/>      </Root>   </Loggers> {code} And this is a preview of the properties file :  {code} rootLogger.level = info rootLogger.appenderRefs = console rootLogger.appenderRef.console.ref = ConsoleAppender {code}  This configuration throw a null pointer exception : {noformat}Exception in thread "main" java.lang.ExceptionInInitializerError Caused by: java.lang.NullPointerException 	at org.apache.logging.log4j.core.config.properties.PropertiesConfigurationFactory.getConfiguration(PropertiesConfigurationFactory.java:132) 	at org.apache.logging.log4j.core.config.properties.PropertiesConfigurationFactory.getConfiguration(PropertiesConfigurationFactory.java:44) 	at org.apache.logging.log4j.core.config.ConfigurationFactory Factory.getConfiguration(ConfigurationFactory.java:491) 	at org.apache.logging.log4j.core.config.ConfigurationFactory Factory.getConfiguration(ConfigurationFactory.java:461) 	at org.apache.logging.log4j.core.config.ConfigurationFactory.getConfiguration(ConfigurationFactory.java:257) 	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:493) 	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:510) 	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:199) 	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:146) 	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41) 	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:264) 	at org.apache.log4j.Logger PrivateManager.getContext(Logger.java:59) 	at org.apache.log4j.Logger.getLogger(Logger.java:41) {noformat}  In order to make this configuration work, i had to add the {{loggers}} component and fill the identifiers. My question is why in xml file we can define only a root logger and it works fine, and in a properties file it does not work ?
LOG4J2-9f924f10$$Unable to define only rootLogger in a properties file.$$I've changed the version of log4j2 from *2.3* to *2.4* in order to load the  configuration via properties file. So i have converted the xml file, that defines only {{<Root>}} in {{<Loggers>}} element, into a properties file.  This is a preview of the xml file : {code:xml}   <Loggers>       <Root level="info">          <AppenderRef ref="ConsoleAppender"/>      </Root>   </Loggers> {code} And this is a preview of the properties file :  {code} rootLogger.level = info rootLogger.appenderRefs = console rootLogger.appenderRef.console.ref = ConsoleAppender {code}  This configuration throw a null pointer exception : {noformat}Exception in thread "main" java.lang.ExceptionInInitializerError Caused by: java.lang.NullPointerException 	at org.apache.logging.log4j.core.config.properties.PropertiesConfigurationFactory.getConfiguration(PropertiesConfigurationFactory.java:132) 	at org.apache.logging.log4j.core.config.properties.PropertiesConfigurationFactory.getConfiguration(PropertiesConfigurationFactory.java:44) 	at org.apache.logging.log4j.core.config.ConfigurationFactory Factory.getConfiguration(ConfigurationFactory.java:491) 	at org.apache.logging.log4j.core.config.ConfigurationFactory Factory.getConfiguration(ConfigurationFactory.java:461) 	at org.apache.logging.log4j.core.config.ConfigurationFactory.getConfiguration(ConfigurationFactory.java:257) 	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:493) 	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:510) 	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:199) 	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:146) 	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41) 	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:264) 	at org.apache.log4j.Logger PrivateManager.getContext(Logger.java:59) 	at org.apache.log4j.Logger.getLogger(Logger.java:41) {noformat}  In order to make this configuration work, i had to add the {{loggers}} component and fill the identifiers. My question is why in xml file we can define only a root logger and it works fine, and in a properties file it does not work ?
LOG4J2-424068f7$$JUL bridge broken$$org.apache.logging.log4j.jul.ApiLogger doesnt behave the same depending where we come from (logger.info() vs logger.log() typically)  The main difference is the message factory used.  for this statement:  {code} logger.info("{foo}"); {code}  a SimpleMessage will be emitted but for  {code} logger.log(recordWithSameContent); {code}  a MessageFormatMessage will be emitted making the log statement failling.  org.apache.logging.log4j.jul.ApiLogger#log(java.util.logging.LogRecord) should be reworked to handle such a case.  Here how to reproduce it:  {code} Logger.getLogger("foo").info("{test}"); Logger.getLogger("foo").log(new LogRecord(Level.INFO, "{test}")); {code}  The fix is as simple as testing org.apache.logging.log4j.jul.ApiLogger#log(java.util.logging.LogRecord) and if null don't call logger.getMessageFactory().newMessage(record.getMessage(), record.getParameters()) but logger.getMessageFactory().newMessage(record.getMessage())
LOG4J2-029e79da$$Methods info, warn, error, fatal with marker and message do not pass the marker$$The follwing methods do not log the message, because the marker is not passed to isXxxEnabled:  AbstractLogger.info(Marker, Message) AbstractLogger.warn(Marker, Message) AbstractLogger.error(Marker, Message) AbstractLogger.fatal(Marker, Message)
LOG4J2-c6318b63$$JndiLookup mindlessly casts to String and should use String.valueOf()$$The value returned from Context.lookup() is cast to String which can cause problems if the value is, well, anything else.
LOG4J2-1d12bf0e$$XMLLayout indents, but not the first child tag (<Event>)$$I am using log4j 2.5 to print the logs via XMLLayout. I have set compact="true", hence the new line and indents of sub tags work correctly. However I have noticed that the first child tag is not indented correctly.   Following is such a sample where <Events> and <Event> are at the same indent level (0 indent).   {code:xml} <?xml version="1.0" encoding="UTF-8"?> <Events xmlns="http://logging.apache.org/log4j/2.0/events"> <Event xmlns="http://logging.apache.org/log4j/2.0/events" timeMillis="1460974404123" thread="main" level="INFO" loggerName="com.foo.Bar" endOfBatch="true" loggerFqcn="org.apache.logging.log4j.spi.AbstractLogger" threadId="11" threadPriority="5">   <Message>First Msg tag must be in level 2 after correct indentation</Message> </Event>  <Event xmlns="http://logging.apache.org/log4j/2.0/events" timeMillis="1460974404133" thread="main" level="INFO" loggerName="com.foo.Bar" endOfBatch="true" loggerFqcn="org.apache.logging.log4j.spi.AbstractLogger" threadId="11" threadPriority="5">   <Message>Second Msg tag must also be in level 2 after correct indentation</Message> </Event>  </Events> {code}
LOG4J2-ffedf33f$$XMLLayout indents, but not the first child tag (<Event>)$$I am using log4j 2.5 to print the logs via XMLLayout. I have set compact="true", hence the new line and indents of sub tags work correctly. However I have noticed that the first child tag is not indented correctly.   Following is such a sample where <Events> and <Event> are at the same indent level (0 indent).   {code:xml} <?xml version="1.0" encoding="UTF-8"?> <Events xmlns="http://logging.apache.org/log4j/2.0/events"> <Event xmlns="http://logging.apache.org/log4j/2.0/events" timeMillis="1460974404123" thread="main" level="INFO" loggerName="com.foo.Bar" endOfBatch="true" loggerFqcn="org.apache.logging.log4j.spi.AbstractLogger" threadId="11" threadPriority="5">   <Message>First Msg tag must be in level 2 after correct indentation</Message> </Event>  <Event xmlns="http://logging.apache.org/log4j/2.0/events" timeMillis="1460974404133" thread="main" level="INFO" loggerName="com.foo.Bar" endOfBatch="true" loggerFqcn="org.apache.logging.log4j.spi.AbstractLogger" threadId="11" threadPriority="5">   <Message>Second Msg tag must also be in level 2 after correct indentation</Message> </Event>  </Events> {code}
LOG4J2-50e19247$$NPE while using SocketAppender$$I try to use the SocketAppender and get the following ERROR/NPE:  2013-01-06 00:54:14,024 DEBUG Generated plugins in 0.000032000 seconds 2013-01-06 00:54:14,044 DEBUG Calling createLayout on class org.apache.logging.log4j.core.layout.PatternLayout for element PatternLayout with params(pattern="%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} --- %msg%n", Configuration(/Users/jhuxhorn/Documents/Projects/huxi/lilith/sandbox/log4j2-sandbox/build/resources/main/log4j2.xml), null, charset="null") 2013-01-06 00:54:14,045 DEBUG Generated plugins in 0.000029000 seconds 2013-01-06 00:54:14,049 DEBUG Calling createAppender on class org.apache.logging.log4j.core.appender.ConsoleAppender for element Console with params(PatternLayout(%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} --- %msg%n), null, target="SYSTEM_OUT", name="Console", suppressExceptions="null") 2013-01-06 00:54:14,049 DEBUG Calling createLayout on class org.apache.logging.log4j.core.layout.SerializedLayout for element SerializedLayout 2013-01-06 00:54:14,052 DEBUG Calling createAppender on class org.apache.logging.log4j.core.appender.SocketAppender for element Socket with params(host="localhost", port="4560", protocol="null", reconnectionDelay="null", name="Socket", immediateFlush="null", suppressExceptions="null", SerializedLayout(org.apache.logging.log4j.core.layout.SerializedLayout@5cc4211b), null) 2013-01-06 00:54:14,054 ERROR Unable to invoke method createAppender in class org.apache.logging.log4j.core.appender.SocketAppender for element Socket java.lang.reflect.InvocationTargetException 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:601) 	at org.apache.logging.log4j.core.config.BaseConfiguration.createPluginObject(BaseConfiguration.java:711) 	at org.apache.logging.log4j.core.config.BaseConfiguration.createConfiguration(BaseConfiguration.java:477) 	at org.apache.logging.log4j.core.config.BaseConfiguration.createConfiguration(BaseConfiguration.java:469) 	at org.apache.logging.log4j.core.config.BaseConfiguration.doConfigure(BaseConfiguration.java:156) 	at org.apache.logging.log4j.core.config.BaseConfiguration.start(BaseConfiguration.java:114) 	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:251) 	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:267) 	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:134) 	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:75) 	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:30) 	at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:165) 	at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:174) 	at de.huxhorn.lilith.sandbox.Log4j2Sandbox.main(Log4j2Sandbox.java:43) Caused by: java.lang.NullPointerException 	at org.apache.logging.log4j.core.appender.SocketAppender.createSocketManager(SocketAppender.java:95) 	at org.apache.logging.log4j.core.appender.SocketAppender.createAppender(SocketAppender.java:86) 	... 17 more  2013-01-06 00:54:14,056 ERROR Null object returned for Socket in appenders   My configuration looks like this: <?xml version="1.0" encoding="UTF-8"?> <configuration status="debug"> 	<appenders> 		<Console name="Console" target="SYSTEM_OUT"> 			<PatternLayout pattern="%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} --- %msg%n"/> 		</Console> 		<Socket name="Socket" host="localhost" port="4560"> 			<SerializedLayout /> 		</Socket> 	</appenders> 	<loggers> 		<root level="all"> 			<appender-ref ref="Console"/> 			<appender-ref ref="Socket"/> 		</root> 	</loggers> </configuration>
LOG4J2-7792679c$$Exception when using log4j2.properties and logger with dot$$After upgrading log4j2 from 2.5 to 2.6 I get the following exception:  {quote} Exception in thread "main" org.apache.logging.log4j.core.config.ConfigurationException: No name attribute provided for Logger org 	at org.apache.logging.log4j.core.config.properties.PropertiesConfigurationBuilder.createLogger(PropertiesConfigurationBuilder.java:215) 	at org.apache.logging.log4j.core.config.properties.PropertiesConfigurationBuilder.build(PropertiesConfigurationBuilder.java:140) 	at org.apache.logging.log4j.core.config.properties.PropertiesConfigurationFactory.getConfiguration(PropertiesConfigurationFactory.java:52) 	at org.apache.logging.log4j.core.config.properties.PropertiesConfigurationFactory.getConfiguration(PropertiesConfigurationFactory.java:34) 	at org.apache.logging.log4j.core.config.ConfigurationFactory Factory.getConfiguration(ConfigurationFactory.java:510) 	at org.apache.logging.log4j.core.config.ConfigurationFactory Factory.getConfiguration(ConfigurationFactory.java:450) 	at org.apache.logging.log4j.core.config.ConfigurationFactory.getConfiguration(ConfigurationFactory.java:257) 	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:560) 	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:577) 	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:212) 	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:152) 	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:45) 	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:194) 	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:103) 	at org.apache.logging.log4j.jcl.LogAdapter.getContext(LogAdapter.java:39) 	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:42) 	at org.apache.logging.log4j.jcl.LogFactoryImpl.getInstance(LogFactoryImpl.java:40) 	at org.apache.logging.log4j.jcl.LogFactoryImpl.getInstance(LogFactoryImpl.java:55) 	at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:655) 	at org.springframework.context.support.AbstractApplicationContext.<init>(AbstractApplicationContext.java:159) 	at org.springframework.context.support.AbstractApplicationContext.<init>(AbstractApplicationContext.java:223) 	at org.springframework.context.support.AbstractRefreshableApplicationContext.<init>(AbstractRefreshableApplicationContext.java:88) 	at org.springframework.context.support.AbstractRefreshableConfigApplicationContext.<init>(AbstractRefreshableConfigApplicationContext.java:58) 	at org.springframework.context.support.AbstractXmlApplicationContext.<init>(AbstractXmlApplicationContext.java:61) 	at org.springframework.context.support.ClassPathXmlApplicationContext.<init>(ClassPathXmlApplicationContext.java:136) 	at org.springframework.context.support.ClassPathXmlApplicationContext.<init>(ClassPathXmlApplicationContext.java:83) x.y.z.Start.main(Start.java:12) {quote}  The parameter "key" has the value "org" and the parameter properties has the value {code}{activiti.engine.impl.level=info, activiti.engine.impl.name=org.activiti.engine.impl}{code}  The log4j2.properties in use:  {code} # Root logger option rootLogger.level = info rootLogger.appenderRefs = stdout rootLogger.appenderRef.stdout.ref = STDOUT  # Redirect log messages to console appenders = stdout appender.stdout.type = Console appender.stdout.name = STDOUT appender.stdout.layout.type = PatternLayout appender.stdout.layout.pattern = %d %-5p [%t] %c - %m%n {code}  Sadly I have not been able to reproduce the issue in a simple standalone application.
LOG4J2-a523dcd5$$2.6 is re-logging prior throwable instead of logging the throwable that is currently passed in by application code$$Hi,  I just want to make sure you saw the issue I submitted PR for a couple days ago.  It seems a very serious issue in 2.6:  https://github.com/apache/logging-log4j2/pull/31  Thanks, Trask
LOG4J2-1461f1f6$$MessagePatternConverter throws a NullPointerException if the log message is null$$If the application does  logger.debug(msg)  where the value of msg is null MessagePatternConverter will get a NullPointerException.
LOG4J2-17296089$$ThreadContextMapFilter doesn't match properly when a single keyvalue is provided$$I was testing out a global ThreadContextMapFilter and noticed it wasn't matching properly.  I took a closer look at the code and found because it wasn't matching the value to the value on the context but rather the key.  I changed it to use the value as the argument to equals and this fixed it.  Here is the diff of what I am running with.   diff --git a/core/src/main/java/org/apache/logging/log4j/core/filter/ThreadContextMapFilter.java b/core/src/main/java/org/apache/logging/log4j/core/filter/ThreadContextMapFilter.java index 9ad6cab..b3f3838 100644 --- a/core/src/main/java/org/apache/logging/log4j/core/filter/ThreadContextMapFi +++ b/core/src/main/java/org/apache/logging/log4j/core/filter/ThreadContextMapFilter.java @@ -96,7 +96,7 @@ public class ThreadContextMapFilter extends MapFilter {                  }              }          } else { -            match = key.equals(ThreadContext.get(key)); +            match = value.equals(ThreadContext.get(key));          }          return match ? onMatch : onMismatch;      }
LOG4J2-f91ce934$$ERROR StatusLogger An exception occurred processing Appender udpsocket java.lang.NullPointerException$$This seems to be a race condition of some kind. Two threads are exiting at almost exactly the same time, and both print a message as they exit. This exception happens every 3 or 4 times the code is run, so it's easily reproducible.  ERROR StatusLogger An exception occurred processing Appender udpsocket java.lang.NullPointerException 	at org.apache.logging.log4j.core.net.DatagramOutputStream.flush(DatagramOutputStream.java:93) 	at org.apache.logging.log4j.core.appender.OutputStreamManager.flush(OutputStreamManager.java:146) 	at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.append(AbstractOutputStreamAppender.java:117) 	at org.apache.logging.log4j.core.config.AppenderControl.callAppender(AppenderControl.java:102) 	at org.apache.logging.log4j.core.config.LoggerConfig.callAppenders(LoggerConfig.java:335) 	at org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:316) 	at org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:281) 	at org.apache.logging.log4j.core.Logger.log(Logger.java:108) 	at org.apache.logging.log4j.spi.AbstractLogger.trace(AbstractLogger.java:250) 	at com.galmont.automation.framework.gui.handlers.RunCycleHandler 2.run(RunCycleHandler.java:128) 	at java.lang.Thread.run(Unknown Source)  My configuration file:  <?xml version="1.0" encoding="UTF-8"?> <configuration status="OFF"> 	 	<appenders> 		<Console name="console" target="SYSTEM_OUT"> 			<PatternLayout pattern="%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n" /> 		</Console> 		<Socket name="udpsocket" host="localhost" port="90" protocol="UDP"> 			<PatternLayout pattern="%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n" /> 		</Socket> 		<RollingFile name="RollingFile" fileName="logs/app.log" filePattern="logs/  {date:yyyy-MM}/app-%d{MM-dd-yyyy}-%i.log.gz"> 			<PatternLayout pattern="%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n" /> 			<Policies> 				<SizeBasedTriggeringPolicy size="20 MB" /> 			</Policies> 		</RollingFile> 	</appenders> 	 	<loggers> 		<root level="trace"> 			<appender-ref ref="console" /> 			<appender-ref ref="udpsocket" /> 			<appender-ref ref="RollingFile" /> 		</root> 	</loggers> 	 </configuration>
LOG4J2-aeb6fc9d$$MapMessage does not enclose key in quotes when generating XML$$MapMessage does not enclose key in quotes when generating XML
LOG4J2-ed951c76$$Named logger without root logger ends up with empty Appenders map - does not log anything$$On the log4j-user mailing list, Peter DePasquale gave this test case that demonstrates the problem:  Note that the configuration has no root logger, but only contains a named logger.  In a debugger I found that the LoggerConfig for "logtest.LogTest" ended up with an empty "appenders" Map<String, AppenderControl<?>>. The appenderRefs list did contain an AppenderRef object but in #callAppenders there are no AppenderControl objects to call...    (Sorry, I have been unable to find out the underlying cause yet.)  <?xml version="1.0" encoding="UTF-8"?> <configuration status="warn"> 	<appenders> 		<File name="tracelog" fileName="trace-log.txt"  				immediateFlush="true" append="false"> 			<PatternLayout pattern="%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n"/> 		</File> 	</appenders> 	 	<loggers> 		<logger name="logtest.LogTest" level="trace"> 			<appender-ref ref="tracelog"/> 		</logger> 	</loggers> </configuration>    package logtest; import org.apache.logging.log4j.LogManager; import org.apache.logging.log4j.Logger; import org.apache.logging.log4j.core.config.XMLConfigurationFactory;  public class LogTest { 	public static void main(String[] args) { 		System.setProperty(XMLConfigurationFactory.CONFIGURATION_FILE_PROPERTY, 				"log4j2-roottest.xml");  		Logger logger = LogManager.getLogger(LogTest.class); 		logger.trace("This is a trace message"); 		logger.info("This is an info message"); 		logger.warn("This is a warning message"); 	} }
LOG4J2-2d7d6311$$RegexFilter crashes as context-wide filter$$If a RegexFilter is used as a context-wide filter, then a call like   logger.isDebugEnabled() leads to a Null-Pointer-Exception, because the RegexFilter is called with the message "null". The stack-trace (2.0-beta5) is: 	at org.apache.logging.log4j.core.filter.RegexFilter.filter(RegexFilter.java:60) 	at org.apache.logging.log4j.core.filter.CompositeFilter.filter(CompositeFilter.java:176) 	at org.apache.logging.log4j.core.Logger PrivateConfig.filter(Logger.java:317) 	at org.apache.logging.log4j.core.Logger.isEnabled(Logger.java:128) 	at org.apache.logging.log4j.spi.AbstractLogger.isTraceEnabled(AbstractLogger.java:1129)   In the MarkerFilter is the code   return marker != null && ... i.e. it is only necessary to change line 60 to   return msg != null && filter(msg.toString) in RegexFilter (I do not know how to do this correctly...)  In line 77, this check is done; in line 66 and 72 the same problem may arise...  Greetings, Gerald Kroisandt
LOG4J2-7b38965d$$HTML layout does not output meta element for charset.$$HTML layout does not output meta element for charset. HEAD element should include a child element, like: {code:xml} <meta charset="UTF-8"/> {code}
LOG4J2-09175c8b$$HTML layout does not specify charset in content type$$HTML layout does not specify charset in content type
LOG4J2-9d817953$$XML layout does not specify charset in content type$$XML layout does not specify charset in content type
LOG4J2-8faf7f77$$Berkeley (persistent) agent for FlumeAppender only works with MapMessages (and thus not slf4j)$$If I try and use the persistent FlumeAppender with slf4j then I get a NullPointerException in FlumePersistentManager.send because there is no GUID header.  (My repro here was using a copy of Flume modified to use log4j2 - while this particular repro is exotic I'm confident that the general case detailed above will be very common).  There is no GUID header because the FlumeEvent constructor only creates one if the message is a MapMessage.  If the user is using slf4j then all messages are PersistentMessages - and thus will cause this logging to fail.  The GUID is required because it's used as a key in the BerkeleyDB storage.  My attempts at a simple fix ran afoul of the key lookup from the headers in FlumePersisentManager.WriterThread.run().
LOG4J2-25cb587a$$classloader URI scheme broken or insufficient when using Log4jContextListener$$I'm trying to migrate to Log4j2, and things looked promising when I spotted Log4jContextListener.  However, there are too many holes.  Firstly, I tried using classpath: as a scheme, and nothing blew up, so I assumed I'd got it right.  Then I *looked at the code* (which shouldn't be how we find out) and eventually discovered some code relating to a 'classloader' scheme.  Still silent failure.  It seems that the classpath is not being searched, perhaps just the WAR classloader, not the JARs in WEB-INF/lib.  Next I tried omitting the / (i.e. using classloader:log4j2.xml) and got a NullPointerException.  Can you please document what schemes are supported and what you expect them to do, and *not fail silently* when a configuration file is specified, but nothing happens.
LOG4J2-ca59ece6$$classloader URI scheme broken or insufficient when using Log4jContextListener$$I'm trying to migrate to Log4j2, and things looked promising when I spotted Log4jContextListener.  However, there are too many holes.  Firstly, I tried using classpath: as a scheme, and nothing blew up, so I assumed I'd got it right.  Then I *looked at the code* (which shouldn't be how we find out) and eventually discovered some code relating to a 'classloader' scheme.  Still silent failure.  It seems that the classpath is not being searched, perhaps just the WAR classloader, not the JARs in WEB-INF/lib.  Next I tried omitting the / (i.e. using classloader:log4j2.xml) and got a NullPointerException.  Can you please document what schemes are supported and what you expect them to do, and *not fail silently* when a configuration file is specified, but nothing happens.
LOG4J2-300bc575$$NDCPatternConverter broken in beta7$$After an upgrade from version 2.0-beta4 to beta7 the NDCPatternConverter writes an object-ID instead of the content of the NDC-stack.   We are using an pattern with "[%0.50x]". In beta4 the resulting output looks like "[cbi@CE03178]" which means username and machine. Now in beta7 it looks like "[logging.log4j.spi.MutableThreadContextStack@875ef7]".  I analysed the issue in NDCPatternConverter.format(...) method, where event.getContextStack() is called and the result is passed to StringBuilder.append(...), which means, that the toString()-method will be invoked.  In beta4 getContextStack() returns an instance of ImmutableStack. This class inherits its toString() method from AbstractList, where the elements of the collection will be formatted human-readable.  Now in beta7 there comes an instance of MutableThreadContextStack which isn't derived from AbstractList but implements the Collection-Interface. The toString() method comes from Object and returns the name of the class and an object-ID instead of the context of the unterlying stack/collection.  In my opinion you just need to copy or derive the toString() method from AbstractList to solve this issue. Thank you in advance!
LOG4J2-3f1e0fdc$$SMTPAppender does not send mails with error or fatal level without prior info event$$When using an SMTPAppender a mail is only delivered on a fatal event if there occured an info event before. Prior fatal events are ignored by SMTPAppender - other Appenders log them.  A more detailed explanation/discussion including an example program can be found at: http://stackoverflow.com/questions/17657983/log4j2-smtpappender-does-not-send-mails-with-error-or-fatal-level
LOG4J2-8dead3bb$$Log4j2 doesnt work with Weblogic 12c$$I get a "Context destroyed before it was initialized" exception, the problem seems to be that the servlet filters init method is not being called by WebLogic, not sure why...
LOG4J2-1df1db27$$Log4jServletContextListener does not work on Weblogic 12.1.1 (12c) with web-app version "2.5"$$I have Weblogic 12c running. My web-app is version "2.5".  Following is a snippet from my web.xml   {code:xml} <web-app xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 	xmlns="http://java.sun.com/xml/ns/javaee" 	xsi:schemaLocation="http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd" 	id="WebApp_ID" version="2.5"> 	<display-name>pec-service</display-name> 	<context-param> 		<param-name>log4jConfiguration</param-name> 		<param-value>file:/C:/log4j/dev.log4j.xml</param-value> 	</context-param>  	<listener>  		<listener-class>org.apache.logging.log4j.core.web.Log4jServletContextListener</listener-class>  	</listener>	  	<filter> 		<filter-name>log4jServletFilter</filter-name> 		<filter-class>org.apache.logging.log4j.core.web.Log4jServletFilter</filter-class>  	</filter> 	<filter-mapping> 		<filter-name>log4jServletFilter</filter-name>  		<url-pattern>/*</url-pattern> 		<dispatcher>REQUEST</dispatcher> 		<dispatcher>FORWARD</dispatcher>  		<dispatcher>INCLUDE</dispatcher> 		<dispatcher>ERROR</dispatcher> 	</filter-mapping> 	 </web-app> {code}  However, on my server startup I am getting the following error -  {code} <Aug 16, 2013 3:12:32 PM PDT> <Warning> <HTTP> <BEA-101162> <User defined listener org.apache.logging.log4j.core.web.Log4jServletContextListener failed: java.lang.IllegalStateException: Context destroyed before it was initialized.. java.lang.IllegalStateException: Context destroyed before it was initialized. 	at org.apache.logging.log4j.core.web.Log4jServletContextListener.contextDestroyed(Log4jServletContextListener.java:51) 	at weblogic.servlet.internal.EventsManager FireContextListenerAction.run(EventsManager.java:583) 	at weblogic.security.acl.internal.AuthenticatedSubject.doAs(AuthenticatedSubject.java:321) 	at weblogic.security.service.SecurityManager.runAs(SecurityManager.java:120) 	at weblogic.servlet.provider.WlsSubjectHandle.run(WlsSubjectHandle.java:57) 	Truncated. see log file for complete stacktrace >  <Aug 16, 2013 3:12:32 PM PDT> <Error> <Deployer> <BEA-149265> <Failure occurred in the execution of deployment request with ID "1376691143681" for task "2". Error is: "weblogic.application.ModuleException" weblogic.application.ModuleException 	at weblogic.servlet.internal.WebAppModule.startContexts(WebAppModule.java:1708) 	at weblogic.servlet.internal.WebAppModule.start(WebAppModule.java:781) 	at weblogic.application.internal.flow.ModuleStateDriver 3.next(ModuleStateDriver.java:213) 	at weblogic.application.internal.flow.ModuleStateDriver 3.next(ModuleStateDriver.java:208) 	at weblogic.application.utils.StateMachineDriver.nextState(StateMachineDriver.java:35) 	Truncated. see log file for complete stacktrace Caused By: java.lang.NullPointerException 	at org.apache.logging.log4j.core.web.Log4jServletContainerInitializer.onStartup(Log4jServletContainerInitializer.java:44) 	at weblogic.servlet.internal.WebAppServletContext.initContainerInitializer(WebAppServletContext.java:1271) 	at weblogic.servlet.internal.WebAppServletContext.initContainerInitializers(WebAppServletContext.java:1229) 	at weblogic.servlet.internal.WebAppServletContext.preloadResources(WebAppServletContext.java:1726) 	at weblogic.servlet.internal.WebAppServletContext.start(WebAppServletContext.java:2740) 	Truncated. see log file for complete stacktrace >  <Aug 16, 2013 3:12:32 PM PDT> <Error> <Deployer> <BEA-149202> <Encountered an exception while attempting to commit the 7 task for the application "_auto_generated_ear_".>  <Aug 16, 2013 3:12:32 PM PDT> <Warning> <Deployer> <BEA-149004> <Failures were detected while initiating start task for application "_auto_generated_ear_".>  <Aug 16, 2013 3:12:32 PM PDT> <Warning> <Deployer> <BEA-149078> <Stack trace for message 149004 weblogic.application.ModuleException 	at weblogic.servlet.internal.WebAppModule.startContexts(WebAppModule.java:1708) 	at weblogic.servlet.internal.WebAppModule.start(WebAppModule.java:781) 	at weblogic.application.internal.flow.ModuleStateDriver 3.next(ModuleStateDriver.java:213) 	at weblogic.application.internal.flow.ModuleStateDriver 3.next(ModuleStateDriver.java:208) 	at weblogic.application.utils.StateMachineDriver.nextState(StateMachineDriver.java:35) 	Truncated. see log file for complete stacktrace Caused By: java.lang.NullPointerException 	at org.apache.logging.log4j.core.web.Log4jServletContainerInitializer.onStartup(Log4jServletContainerInitializer.java:44) 	at weblogic.servlet.internal.WebAppServletContext.initContainerInitializer(WebAppServletContext.java:1271) 	at weblogic.servlet.internal.WebAppServletContext.initContainerInitializers(WebAppServletContext.java:1229) 	at weblogic.servlet.internal.WebAppServletContext.preloadResources(WebAppServletContext.java:1726) 	at weblogic.servlet.internal.WebAppServletContext.start(WebAppServletContext.java:2740) 	Truncated. see log file for complete stacktrace {code}  If I remove the listener & the filter, it works fine.  {color:red} I did some research and found that even though the web-app is version "2.5", the {code}Log4jServletContainerInitializer{code} is getting invoked.  {color}
LOG4J2-296ea4a5$$Log4jServletContextListener does not work on Weblogic 12.1.1 (12c) with web-app version "2.5"$$I have Weblogic 12c running. My web-app is version "2.5".  Following is a snippet from my web.xml   {code:xml} <web-app xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 	xmlns="http://java.sun.com/xml/ns/javaee" 	xsi:schemaLocation="http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd" 	id="WebApp_ID" version="2.5"> 	<display-name>pec-service</display-name> 	<context-param> 		<param-name>log4jConfiguration</param-name> 		<param-value>file:/C:/log4j/dev.log4j.xml</param-value> 	</context-param>  	<listener>  		<listener-class>org.apache.logging.log4j.core.web.Log4jServletContextListener</listener-class>  	</listener>	  	<filter> 		<filter-name>log4jServletFilter</filter-name> 		<filter-class>org.apache.logging.log4j.core.web.Log4jServletFilter</filter-class>  	</filter> 	<filter-mapping> 		<filter-name>log4jServletFilter</filter-name>  		<url-pattern>/*</url-pattern> 		<dispatcher>REQUEST</dispatcher> 		<dispatcher>FORWARD</dispatcher>  		<dispatcher>INCLUDE</dispatcher> 		<dispatcher>ERROR</dispatcher> 	</filter-mapping> 	 </web-app> {code}  However, on my server startup I am getting the following error -  {code} <Aug 16, 2013 3:12:32 PM PDT> <Warning> <HTTP> <BEA-101162> <User defined listener org.apache.logging.log4j.core.web.Log4jServletContextListener failed: java.lang.IllegalStateException: Context destroyed before it was initialized.. java.lang.IllegalStateException: Context destroyed before it was initialized. 	at org.apache.logging.log4j.core.web.Log4jServletContextListener.contextDestroyed(Log4jServletContextListener.java:51) 	at weblogic.servlet.internal.EventsManager FireContextListenerAction.run(EventsManager.java:583) 	at weblogic.security.acl.internal.AuthenticatedSubject.doAs(AuthenticatedSubject.java:321) 	at weblogic.security.service.SecurityManager.runAs(SecurityManager.java:120) 	at weblogic.servlet.provider.WlsSubjectHandle.run(WlsSubjectHandle.java:57) 	Truncated. see log file for complete stacktrace >  <Aug 16, 2013 3:12:32 PM PDT> <Error> <Deployer> <BEA-149265> <Failure occurred in the execution of deployment request with ID "1376691143681" for task "2". Error is: "weblogic.application.ModuleException" weblogic.application.ModuleException 	at weblogic.servlet.internal.WebAppModule.startContexts(WebAppModule.java:1708) 	at weblogic.servlet.internal.WebAppModule.start(WebAppModule.java:781) 	at weblogic.application.internal.flow.ModuleStateDriver 3.next(ModuleStateDriver.java:213) 	at weblogic.application.internal.flow.ModuleStateDriver 3.next(ModuleStateDriver.java:208) 	at weblogic.application.utils.StateMachineDriver.nextState(StateMachineDriver.java:35) 	Truncated. see log file for complete stacktrace Caused By: java.lang.NullPointerException 	at org.apache.logging.log4j.core.web.Log4jServletContainerInitializer.onStartup(Log4jServletContainerInitializer.java:44) 	at weblogic.servlet.internal.WebAppServletContext.initContainerInitializer(WebAppServletContext.java:1271) 	at weblogic.servlet.internal.WebAppServletContext.initContainerInitializers(WebAppServletContext.java:1229) 	at weblogic.servlet.internal.WebAppServletContext.preloadResources(WebAppServletContext.java:1726) 	at weblogic.servlet.internal.WebAppServletContext.start(WebAppServletContext.java:2740) 	Truncated. see log file for complete stacktrace >  <Aug 16, 2013 3:12:32 PM PDT> <Error> <Deployer> <BEA-149202> <Encountered an exception while attempting to commit the 7 task for the application "_auto_generated_ear_".>  <Aug 16, 2013 3:12:32 PM PDT> <Warning> <Deployer> <BEA-149004> <Failures were detected while initiating start task for application "_auto_generated_ear_".>  <Aug 16, 2013 3:12:32 PM PDT> <Warning> <Deployer> <BEA-149078> <Stack trace for message 149004 weblogic.application.ModuleException 	at weblogic.servlet.internal.WebAppModule.startContexts(WebAppModule.java:1708) 	at weblogic.servlet.internal.WebAppModule.start(WebAppModule.java:781) 	at weblogic.application.internal.flow.ModuleStateDriver 3.next(ModuleStateDriver.java:213) 	at weblogic.application.internal.flow.ModuleStateDriver 3.next(ModuleStateDriver.java:208) 	at weblogic.application.utils.StateMachineDriver.nextState(StateMachineDriver.java:35) 	Truncated. see log file for complete stacktrace Caused By: java.lang.NullPointerException 	at org.apache.logging.log4j.core.web.Log4jServletContainerInitializer.onStartup(Log4jServletContainerInitializer.java:44) 	at weblogic.servlet.internal.WebAppServletContext.initContainerInitializer(WebAppServletContext.java:1271) 	at weblogic.servlet.internal.WebAppServletContext.initContainerInitializers(WebAppServletContext.java:1229) 	at weblogic.servlet.internal.WebAppServletContext.preloadResources(WebAppServletContext.java:1726) 	at weblogic.servlet.internal.WebAppServletContext.start(WebAppServletContext.java:2740) 	Truncated. see log file for complete stacktrace {code}  If I remove the listener & the filter, it works fine.  {color:red} I did some research and found that even though the web-app is version "2.5", the {code}Log4jServletContainerInitializer{code} is getting invoked.  {color}
LOG4J2-a8a24357$$PatternLayout in 1.2 bridge missing constructor$$java.lang.NoSuchMethodError: org.apache.log4j.PatternLayout.<init>(Ljava/lang/String;)V 	at org.apache.velocity.runtime.log.Log4JLogChute.initAppender(Log4JLogChute.java:117) ~[velocity-1.7-dep.jar:1.7] 	at org.apache.velocity.runtime.log.Log4JLogChute.init(Log4JLogChute.java:85) ~[velocity-1.7-dep.jar:1.7] 	at org.apache.velocity.runtime.log.LogManager.createLogChute(LogManager.java:157) ~[velocity-1.7-dep.jar:1.7] 	at org.apache.velocity.runtime.log.LogManager.updateLog(LogManager.java:269) ~[velocity-1.7-dep.jar:1.7] 	at org.apache.velocity.runtime.RuntimeInstance.initializeLog(RuntimeInstance.java:871) ~[velocity-1.7-dep.jar:1.7] 	at org.apache.velocity.runtime.RuntimeInstance.init(RuntimeInstance.java:262) ~[velocity-1.7-dep.jar:1.7] 	at org.apache.velocity.runtime.RuntimeInstance.requireInitialization(RuntimeInstance.java:302) ~[velocity-1.7-dep.jar:1.7] 	at org.apache.velocity.runtime.RuntimeInstance.getTemplate(RuntimeInstance.java:1531) ~[velocity-1.7-dep.jar:1.7] 	at org.apache.velocity.app.VelocityEngine.mergeTemplate(VelocityEngine.java:343) ~[velocity-1.7-dep.jar:1.7]
LOG4J2-ef8517e4$$Logging generates file named  {sys on some systems$$In a webapp I'm setting a system property in my apps ServletContextListener, and using that system property in my log4j2.xml file, like so: {code} <appender type="FastFile" name="File" fileName=" {sys:catalina.home}/logs/ {sys:application-name}.log"> {code} On my Windows machine, a log file named " {sys." (always 0 bytes) is being created instead of a log file with the application-name. The same war deployed on one of our linux servers does not create a  {sys." file and instead creates a log file with the intended application-name.   I should note that the files DO appear in the directory that sys:catalina.home should resolve to. They appear elsewhere when I don't use sys:catalina.home so I'm quite sure that this variable is resolving correctly and it is the sys:application-name which is the problem.
LOG4J2-7c2ce5cf$$Unable to roll log files monthly$$Attempting to use FastRollingFile appender and configure log file rollover to occur monthly.  When {{filePattern="logs/app-%d\{yyyy-MM}.log.gz"}} is used, at application startup an archive file is created immediately (app-2013-01.log.gz) even if no log previously existed.  A log file is created, but only a single entry is made into the log.
LOG4J2-731c84b5$$Intermittent errors with appenders$$I intermittently receive following errors after upgrading to beta 8. EVERYTHING was working well with beta 6: * 1st error (happens most frequently) 2013-09-05 10:48:37,722 ERROR Attempted to append to non-started appender LogFile  * 2nd error: 2013-09-05 10:49:38,268 ERROR Attempted to append to non-started appender LogFile 2013-09-05 10:49:38,268 ERROR Unable to write to stream log/ui-selenium-tests.log for appender LogFile 2013-09-05 10:49:38,269 ERROR An exception occurred processing Appender LogFile org.apache.logging.log4j.core.appender.AppenderRuntimeException: Error writing to RandomAccessFile log/ui-selenium-tests.log 	at org.apache.logging.log4j.core.appender.rolling.FastRollingFileManager.flush(FastRollingFileManager.java:108) 	at org.apache.logging.log4j.core.appender.rolling.FastRollingFileManager.write(FastRollingFileManager.java:89) 	at org.apache.logging.log4j.core.appender.OutputStreamManager.write(OutputStreamManager.java:129) 	at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.append(AbstractOutputStreamAppender.java:115) 	at org.apache.logging.log4j.core.appender.FastRollingFileAppender.append(FastRollingFileAppender.java:97) 	at org.apache.logging.log4j.core.config.AppenderControl.callAppender(AppenderControl.java:102) 	at org.apache.logging.log4j.core.appender.AsyncAppender AsyncThread.run(AsyncAppender.java:228) Caused by: java.io.IOException: Write error 	at java.io.RandomAccessFile.writeBytes(Native Method) 	at java.io.RandomAccessFile.write(Unknown Source) 	at org.apache.logging.log4j.core.appender.rolling.FastRollingFileManager.flush(FastRollingFileManager.java:105) 	... 6 more
LOG4J2-a19ecc9e$$log4j.configurationFile via classpath URI$$Can't specify log4j.configurationFile as a classpath URI.  For eg: -Dlog4j.configurationFile=classpath:log4j/log4j.xml
LOG4J2-2c966ad9$$DateLookup not parsed for FastRollingFile appender$$I'm trying to create a Log4j2 configuration file that will create a log file using DateLookup so that the current date and time are in the filename (so it matches the logging used in our other products).  This is what the appender configuration looks like:  {code:borderStyle=solid|language=XML} <FastRollingFile name="Rolling" fileName="log/  {date:yyyyMMdd-HHmmss} - myApp.log" filePattern="log/  {date:yyyyMMdd-HHmmss} - myApp-%i.log"> 	<immediateFlush>true</immediateFlush> 	<suppressExceptions>false</suppressExceptions> 	<PatternLayout> 		<pattern>%d %p %c{1.} [%t]   {env:USER} %m%n</pattern> 	</PatternLayout> 	<Policies> 		<OnStartupTriggeringPolicy /> 		<SizeBasedTriggeringPolicy size="100 MB"/> 	</Policies> </FastRollingFile> {code}  However when the log file is generated the filename is " {date".  I've tried different variations and haven't been able to get this lookup to work at all.
LOG4J2-8f0c4871$$java.io.NotSerializableException: org.slf4j.impl.SLF4JLogger$$When i use a the logger object in a model,and the model will set to some cache like memcached,this exception happens.Maybe this class and some other related class need implements Serializable interface?
LOG4J2-238ce8aa$$RFC5424Layout not working with parametrized messages$$Syslog (i.e RFC5424Layout) does not work with parametrized messages. If I do something like this: {code} logger.info("Hello {}", "World"); {code}  I get this at the syslog server:  {code} Oct 16 18:24:33 10.0.0.3 myApp Hello {} {code}  This is the config file I'm using: {code:xml:title=log4j2.xml} <?xml version="1.0" encoding="UTF-8"?> <Configuration>   <Appenders>     <Console name="STDOUT" target="SYSTEM_OUT">       <PatternLayout pattern="%d{ISO8601}: %-5p [%t-%c{2}] - %m%n"/>     </Console>     <Syslog name="syslog" format="RFC5424" host="10.0.0.1" port="514" protocol="TCP" appName="myApp" facility="LOCAL0" newLine="true" includeMDC="true" id="App" reconnectionDelay="1000"/>   </Appenders>   <Loggers>     <Root level="debug">       <AppenderRef ref="STDOUT"/>       <AppenderRef ref="syslog"/>     </Root>   </Loggers> </Configuration> {code}  The log to stdout is ok though.  Attached you find my patch for this bug.
LOG4J2-0343e9c7$$XMLLayout does not include marker name$$Log4j2 supports the notion of markers, but this is not represented by the XMLLayout.  For example, using SerializedLayout with SocketAppender will send marker information, but using XMLLayout with SocketAppender will not.  It would be very helpful to have just the name of the leaf marker sent with the log event, not the corresponding marker hierarchy.
LOG4J2-484c865f$$JSON Syntax: LoggerConfig - multiple AppenderRef entries$$How does one assign multiple AppenderRef entries to a logger when using JSON syntax? I've tried numerous formats but none of them appear to work. Exampe below.  "loggers": {         "logger": [             {                 "name": "helloWorld",                 "level": "info",                 "additivity": "true",                 "AppenderRef": [                     {                         "ref": "File Routing Appender"                     },                     {                         "ref": "Database Routing Appender"                     }                 ]             }         ],         "root": {             "level": "info",             "AppenderRef": {                 "ref": "Console Appender"             }         }     }  2013-12-11 08:32:07,012 DEBUG Calling createLogger on class org.apache.logging.log4j.core.config.LoggerConfig for element logger with params(additivity="true", level="info", name="helloWorld", includeLocation="null", AppenderRef={}, Properties={}, Configuration(Hello World Config), null)  The only way I've been able to hack this (up to a maximum of two AppenderRefs) is to use the appender-ref alias in conjunction with AppenderRef e.g.: "loggers": {             "logger": [                                  {                     "name": "helloWorld",                     "level": "info",                     "additivity": "true",                     "AppenderRef": {                     	"ref":"File Routing Appender"                     },                     "appender-ref": {                     	"ref":"Database Routing Appender"                     }                 }             ],             "root": {                 "level": "info",                 "AppenderRef": {                     "ref": "Console Appender"                 }             }    		} 2013-12-11 08:51:54,977 DEBUG Calling createLogger on class org.apache.logging.log4j.core.config.LoggerConfig for element logger with params(additivity="true", level="info", name="helloWorld", includeLocation="null", AppenderRef={File Routing Appender, Database Routing Appender}, Properties={}, Configuration(Hello World Config), null)
LOG4J2-7b9e48e8$$Cannot load log4j2 config file if path contains plus '+' characters$$Hello,  I was trying to programmatically load a XML config file from the temporary data directory of a MacOS X system. The temp path consists of serveral '\+' characters like MacOS automatically generates this path so we have to take it this way. Even I would agree that it is not nice to have '\+' chars in a path name.  When I tried to load the XML config the framework permanently loaded the DefaultConfig and not the desired XML configuration. By stepping through the debugger I figured out that this was caused by the method fileFromURI() in org.apache.logging.log4j.core.helpers.FileUtils.java . The misbehaviour was basically caused by the call of URL.decode() which converts '+' to ' ' (space) of a given String.  Now I self-compiled the whole framework without the call of URL.decode() and the XML configuration loaded properly.  I can not see why this call is necessary in this method so in my opinion this should be removed.   Kind regards
LOG4J2-50340d0c$$Resolution of  {hostName} in log4j2.xml file only works after first reference$$I am using  \{hostName} to include the hostname in the log file.  When I use it it resolves to " \{hostName}" the first time it is referred to in the log and then the proper hostname after that.  Example configuration (comment out the "Properties" section to duplicate): {code} <?xml version="1.0" encoding="UTF-8"?> <Configuration monitorInterval="60"	name="SMSLog4JConfiguration"> <!-- add this to spit out debug about configuration: 'status="debug"'  -->  	<!-- This seems to be a bug, but the  hostName seems to need to be referenced 	     once before it can be used.  Maybe it gets correct in a future log4j2 release -->  	<Properties> 		<Property name="theHostName"> {hostName}</Property> 	</Properties>   	<Appenders> 		<RollingFile name="RollingFileAppender" fileName="/applicationlogs/CTMSApplicationService- {hostName}.log" 			filePattern="/applicationlogs/ {hostName}-%d{MM-dd-yyyy}-%i.log"> 			<Policies> 				<OnStartupTriggeringPolicy /> 				<TimeBasedTriggeringPolicy interval="24" modulate="true" /> 			</Policies> 			<PatternLayout pattern="[%d{ISO8601}] [%t] %-5level %logger{6} - %msg%n" /> 		</RollingFile> 	</Appenders> 	<Loggers> 		<!-- default for "includeLocation" is false, but I want to be clear --> 		<Root level="debug" includeLocation="false"> 			<AppenderRef ref="RollingFileAppender" /> 		</Root> 	</Loggers> </Configuration> {code}
LOG4J2-11763dee$$The message and ndc fields are not JavaScript escaped in JSONLayout$$The output of the JSONLayout includes the "message" field as is.  If there are any embedded newlines, quote, etc, this renders the JSON output as invalid.  To correct this, the "message" field should be properly JavaScript escaped.
LOG4J2-24a3bed4$$MalformedObjectNameException: Invalid escape sequence... under Jetty$$Although it is not stopping my webapp from running, I am encountering the following exception when running jetty (via Maven) for a webapp using a trunk build of log4j2.  My debug line is also included:  {noformat} loggerContext.getName()= WebAppClassLoader=1320771902@4eb9613e 2014-01-09 13:28:52,904 ERROR Could not register mbeans java.lang.IllegalStateException: javax.management.MalformedObjectNameException: Invalid escape sequence '\=' in quoted value         at org.apache.logging.log4j.core.jmx.LoggerContextAdmin.<init>(LoggerContextAdmin.java:81)         at org.apache.logging.log4j.core.jmx.Server.registerContexts(Server.java:266)         at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:185)         at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:150)         at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:387)         at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:151)         at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:105)         at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:33)         at org.apache.logging.log4j.LogManager.getContext(LogManager.java:222)         at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:103)         at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:63)         at org.apache.logging.log4j.core.web.Log4jWebInitializerImpl.initializeNonJndi(Log4jWebInitializerImpl.java:136)         at org.apache.logging.log4j.core.web.Log4jWebInitializerImpl.initialize(Log4jWebInitializerImpl.java:82)         at org.apache.logging.log4j.core.web.Log4jServletContainerInitializer.onStartup(Log4jServletContainerInitializer.java:41)         at org.eclipse.jetty.plus.annotation.ContainerInitializer.callStartup(ContainerInitializer.java:106)         at org.eclipse.jetty.annotations.ServletContainerInitializerListener.doStart(ServletContainerInitializerListener.java:107)         at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)         at org.eclipse.jetty.util.component.AggregateLifeCycle.doStart(AggregateLifeCycle.java:81)         at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:58)         at org.eclipse.jetty.server.handler.HandlerWrapper.doStart(HandlerWrapper.java:96)         at org.eclipse.jetty.server.handler.ScopedHandler.doStart(ScopedHandler.java:115)         at org.eclipse.jetty.server.handler.ContextHandler.startContext(ContextHandler.java:763)         at org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:249)         at org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1242)         at org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:717)         at org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:494)         at org.mortbay.jetty.plugin.JettyWebAppContext.doStart(JettyWebAppContext.java:298)         at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)         at org.eclipse.jetty.server.handler.HandlerCollection.doStart(HandlerCollection.java:229)         at org.eclipse.jetty.server.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:172)         at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)         at org.eclipse.jetty.server.handler.HandlerCollection.doStart(HandlerCollection.java:229)         at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)         at org.eclipse.jetty.server.handler.HandlerWrapper.doStart(HandlerWrapper.java:95)         at org.eclipse.jetty.server.Server.doStart(Server.java:282)         at org.mortbay.jetty.plugin.JettyServer.doStart(JettyServer.java:65)         at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)         at org.mortbay.jetty.plugin.AbstractJettyMojo.startJetty(AbstractJettyMojo.java:520)         at org.mortbay.jetty.plugin.AbstractJettyMojo.execute(AbstractJettyMojo.java:365)         at org.mortbay.jetty.plugin.JettyRunMojo.execute(JettyRunMojo.java:523)         at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:101)         at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:209)         at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)         at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)         at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)         at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)         at org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)         at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)         at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:320)         at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:156)         at org.apache.maven.cli.MavenCli.execute(MavenCli.java:537)         at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:196)         at org.apache.maven.cli.MavenCli.main(MavenCli.java:141)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)         at java.lang.reflect.Method.invoke(Method.java:606)         at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:290)         at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:230)         at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:409)         at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:352) Caused by: javax.management.MalformedObjectNameException: Invalid escape sequence '\=' in quoted value         at javax.management.ObjectName.construct(ObjectName.java:582)         at javax.management.ObjectName.<init>(ObjectName.java:1382)         at org.apache.logging.log4j.core.jmx.LoggerContextAdmin.<init>(LoggerContextAdmin.java:79)         ... 60 more  2014-01-09 13:28:52.989:INFO:/wallboard:Initializing Spring root WebApplicationContext 2014-01-09 13:29:04.645:INFO:/wallboard:Log4jServletContextListener ensuring that Log4j starts up properly. 2014-01-09 13:29:04.651:INFO:/wallboard:Log4jServletFilter initialized. 2014-01-09 13:29:04.778:WARN:oejsh.RequestLogHandler:!RequestLog 2014-01-09 13:29:04.872:INFO:oejs.AbstractConnector:Started SelectChannelConnector@0.0.0.0:8080 [INFO] Started Jetty Server [INFO] Starting scanner at interval of 10 seconds.  {noformat}
LOG4J2-61ccbb95$$MalformedObjectNameException: Invalid escape sequence... under Jetty$$Although it is not stopping my webapp from running, I am encountering the following exception when running jetty (via Maven) for a webapp using a trunk build of log4j2.  My debug line is also included:  {noformat} loggerContext.getName()= WebAppClassLoader=1320771902@4eb9613e 2014-01-09 13:28:52,904 ERROR Could not register mbeans java.lang.IllegalStateException: javax.management.MalformedObjectNameException: Invalid escape sequence '\=' in quoted value         at org.apache.logging.log4j.core.jmx.LoggerContextAdmin.<init>(LoggerContextAdmin.java:81)         at org.apache.logging.log4j.core.jmx.Server.registerContexts(Server.java:266)         at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:185)         at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:150)         at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:387)         at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:151)         at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:105)         at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:33)         at org.apache.logging.log4j.LogManager.getContext(LogManager.java:222)         at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:103)         at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:63)         at org.apache.logging.log4j.core.web.Log4jWebInitializerImpl.initializeNonJndi(Log4jWebInitializerImpl.java:136)         at org.apache.logging.log4j.core.web.Log4jWebInitializerImpl.initialize(Log4jWebInitializerImpl.java:82)         at org.apache.logging.log4j.core.web.Log4jServletContainerInitializer.onStartup(Log4jServletContainerInitializer.java:41)         at org.eclipse.jetty.plus.annotation.ContainerInitializer.callStartup(ContainerInitializer.java:106)         at org.eclipse.jetty.annotations.ServletContainerInitializerListener.doStart(ServletContainerInitializerListener.java:107)         at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)         at org.eclipse.jetty.util.component.AggregateLifeCycle.doStart(AggregateLifeCycle.java:81)         at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:58)         at org.eclipse.jetty.server.handler.HandlerWrapper.doStart(HandlerWrapper.java:96)         at org.eclipse.jetty.server.handler.ScopedHandler.doStart(ScopedHandler.java:115)         at org.eclipse.jetty.server.handler.ContextHandler.startContext(ContextHandler.java:763)         at org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:249)         at org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1242)         at org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:717)         at org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:494)         at org.mortbay.jetty.plugin.JettyWebAppContext.doStart(JettyWebAppContext.java:298)         at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)         at org.eclipse.jetty.server.handler.HandlerCollection.doStart(HandlerCollection.java:229)         at org.eclipse.jetty.server.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:172)         at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)         at org.eclipse.jetty.server.handler.HandlerCollection.doStart(HandlerCollection.java:229)         at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)         at org.eclipse.jetty.server.handler.HandlerWrapper.doStart(HandlerWrapper.java:95)         at org.eclipse.jetty.server.Server.doStart(Server.java:282)         at org.mortbay.jetty.plugin.JettyServer.doStart(JettyServer.java:65)         at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)         at org.mortbay.jetty.plugin.AbstractJettyMojo.startJetty(AbstractJettyMojo.java:520)         at org.mortbay.jetty.plugin.AbstractJettyMojo.execute(AbstractJettyMojo.java:365)         at org.mortbay.jetty.plugin.JettyRunMojo.execute(JettyRunMojo.java:523)         at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:101)         at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:209)         at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)         at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)         at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)         at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)         at org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)         at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)         at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:320)         at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:156)         at org.apache.maven.cli.MavenCli.execute(MavenCli.java:537)         at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:196)         at org.apache.maven.cli.MavenCli.main(MavenCli.java:141)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)         at java.lang.reflect.Method.invoke(Method.java:606)         at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:290)         at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:230)         at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:409)         at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:352) Caused by: javax.management.MalformedObjectNameException: Invalid escape sequence '\=' in quoted value         at javax.management.ObjectName.construct(ObjectName.java:582)         at javax.management.ObjectName.<init>(ObjectName.java:1382)         at org.apache.logging.log4j.core.jmx.LoggerContextAdmin.<init>(LoggerContextAdmin.java:79)         ... 60 more  2014-01-09 13:28:52.989:INFO:/wallboard:Initializing Spring root WebApplicationContext 2014-01-09 13:29:04.645:INFO:/wallboard:Log4jServletContextListener ensuring that Log4j starts up properly. 2014-01-09 13:29:04.651:INFO:/wallboard:Log4jServletFilter initialized. 2014-01-09 13:29:04.778:WARN:oejsh.RequestLogHandler:!RequestLog 2014-01-09 13:29:04.872:INFO:oejs.AbstractConnector:Started SelectChannelConnector@0.0.0.0:8080 [INFO] Started Jetty Server [INFO] Starting scanner at interval of 10 seconds.  {noformat}
LOG4J2-a759d8ae$$MalformedObjectNameException: Invalid escape sequence... under Jetty$$Although it is not stopping my webapp from running, I am encountering the following exception when running jetty (via Maven) for a webapp using a trunk build of log4j2.  My debug line is also included:  {noformat} loggerContext.getName()= WebAppClassLoader=1320771902@4eb9613e 2014-01-09 13:28:52,904 ERROR Could not register mbeans java.lang.IllegalStateException: javax.management.MalformedObjectNameException: Invalid escape sequence '\=' in quoted value         at org.apache.logging.log4j.core.jmx.LoggerContextAdmin.<init>(LoggerContextAdmin.java:81)         at org.apache.logging.log4j.core.jmx.Server.registerContexts(Server.java:266)         at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:185)         at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:150)         at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:387)         at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:151)         at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:105)         at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:33)         at org.apache.logging.log4j.LogManager.getContext(LogManager.java:222)         at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:103)         at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:63)         at org.apache.logging.log4j.core.web.Log4jWebInitializerImpl.initializeNonJndi(Log4jWebInitializerImpl.java:136)         at org.apache.logging.log4j.core.web.Log4jWebInitializerImpl.initialize(Log4jWebInitializerImpl.java:82)         at org.apache.logging.log4j.core.web.Log4jServletContainerInitializer.onStartup(Log4jServletContainerInitializer.java:41)         at org.eclipse.jetty.plus.annotation.ContainerInitializer.callStartup(ContainerInitializer.java:106)         at org.eclipse.jetty.annotations.ServletContainerInitializerListener.doStart(ServletContainerInitializerListener.java:107)         at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)         at org.eclipse.jetty.util.component.AggregateLifeCycle.doStart(AggregateLifeCycle.java:81)         at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:58)         at org.eclipse.jetty.server.handler.HandlerWrapper.doStart(HandlerWrapper.java:96)         at org.eclipse.jetty.server.handler.ScopedHandler.doStart(ScopedHandler.java:115)         at org.eclipse.jetty.server.handler.ContextHandler.startContext(ContextHandler.java:763)         at org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:249)         at org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1242)         at org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:717)         at org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:494)         at org.mortbay.jetty.plugin.JettyWebAppContext.doStart(JettyWebAppContext.java:298)         at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)         at org.eclipse.jetty.server.handler.HandlerCollection.doStart(HandlerCollection.java:229)         at org.eclipse.jetty.server.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:172)         at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)         at org.eclipse.jetty.server.handler.HandlerCollection.doStart(HandlerCollection.java:229)         at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)         at org.eclipse.jetty.server.handler.HandlerWrapper.doStart(HandlerWrapper.java:95)         at org.eclipse.jetty.server.Server.doStart(Server.java:282)         at org.mortbay.jetty.plugin.JettyServer.doStart(JettyServer.java:65)         at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)         at org.mortbay.jetty.plugin.AbstractJettyMojo.startJetty(AbstractJettyMojo.java:520)         at org.mortbay.jetty.plugin.AbstractJettyMojo.execute(AbstractJettyMojo.java:365)         at org.mortbay.jetty.plugin.JettyRunMojo.execute(JettyRunMojo.java:523)         at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:101)         at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:209)         at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)         at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)         at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)         at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)         at org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)         at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)         at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:320)         at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:156)         at org.apache.maven.cli.MavenCli.execute(MavenCli.java:537)         at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:196)         at org.apache.maven.cli.MavenCli.main(MavenCli.java:141)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)         at java.lang.reflect.Method.invoke(Method.java:606)         at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:290)         at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:230)         at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:409)         at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:352) Caused by: javax.management.MalformedObjectNameException: Invalid escape sequence '\=' in quoted value         at javax.management.ObjectName.construct(ObjectName.java:582)         at javax.management.ObjectName.<init>(ObjectName.java:1382)         at org.apache.logging.log4j.core.jmx.LoggerContextAdmin.<init>(LoggerContextAdmin.java:79)         ... 60 more  2014-01-09 13:28:52.989:INFO:/wallboard:Initializing Spring root WebApplicationContext 2014-01-09 13:29:04.645:INFO:/wallboard:Log4jServletContextListener ensuring that Log4j starts up properly. 2014-01-09 13:29:04.651:INFO:/wallboard:Log4jServletFilter initialized. 2014-01-09 13:29:04.778:WARN:oejsh.RequestLogHandler:!RequestLog 2014-01-09 13:29:04.872:INFO:oejs.AbstractConnector:Started SelectChannelConnector@0.0.0.0:8080 [INFO] Started Jetty Server [INFO] Starting scanner at interval of 10 seconds.  {noformat}
LOG4J2-837dcd89$$LocalizedMessage serialization is broken$$You can serialize a LocalizedMessage  but you get an exception when it is deserialized, an EOF exception.  See the tests in https://svn.apache.org/repos/asf/logging/log4j/log4j2/trunk/log4j-api/src/test/java/org/apache/logging/log4j/message/LocalizedMessageTest.java
LOG4J2-3eb44094$$Level.toLevel throws IllegalArgumentException instead of returning default Level$$org.apache.logging.log4j.Level.toLevel(String, Level) ( Level.java line 100) uses enum static method valueOf(String) which throws IllegalArgumentException instead of returning null when enum const doesnt exists. This makes the methods Level.toLevel throw the exception instead of return default value.  Solution:  You can: a) sorround it with a try-catch statement, like:         try { 			return valueOf(sArg); 		} catch (Exception e) { 			//exception doesnt matter 			return defaultLevel; 		}  b) translate manually de String to a enum constant, like:         for (Level level : values()) { 			if (level.name().equals(sArg)) { 				return level; 			} 		}         return defaultLevel;  I prefer b) because it saves the try-catch context and the for is nearly the same that the valueOf should do.
LOG4J2-bb02fa15$$No header output in RollingRandomAccessFile$$No header output in RollingRandomAccessFile due to DummyOutputStream used when creating RollingRandomAccessFileManager.  {code:title=RollingRandomAccessFileManager.java} ... 162:                return new RollingRandomAccessFileManager(raf, name, data.pattern, +new DummyOutputStream()+, data.append, 163:                        data.immediateFlush, size, time, data.policy, data.strategy, data.advertiseURI, data.layout); {code} When the superclass constructor (OutputStreamManager) writes header, it outputs thus header to nowhere: {code:title=OutputStreamManager.java} 35:    protected OutputStreamManager(final OutputStream os, final String streamName, final Layout<?> layout) { 36:        super(streamName); 37:        this.os = os; 38:        if (layout != null) { 39:            this.footer = layout.getFooter(); 40:            this.header = layout.getHeader(); 41:            if (this.header != null) { 42:                try { 43:!!!                 this.os.write(header, 0, header.length); 44:                } catch (final IOException ioe) { 45:                    LOGGER.error("Unable to write header", ioe); 46:                } 47:            } 48:        } else { 49:            this.footer = null; 50:            this.header = null; 51:        } 52:    } {code} The same fragment from RollingFileManager.java where header output works fine: {code:title=RollingFileManager.java} 306:                os = new FileOutputStream(name, data.append); 307:                if (data.bufferedIO) { 308:                    os = new BufferedOutputStream(os); 309:                } 310:                final long time = file.lastModified(); // LOG4J2-531 create file first so time has valid value 311:                return new RollingFileManager(name, data.pattern, +os+, data.append, size, time, data.policy, 312:                    data.strategy, data.advertiseURI, data.layout); {code}  In this case the "os" variable is a real stream which points to the file.
LOG4J2-3b4b370e$$Unable to recover after loading corrupted XML$$Steps to reproduce: 1) auto-reloading of log4j 2.x configuration from XML is enabled 2) system is started and producing logs 3) change XML configuration, so it's not valid XML any longer 4) Wait till it would be picked up -> no more logging info is produced, exception can be found from logs (see below). 5) Fix XML configuration -> it's not getting reloaded anymore, only java restart can fix the problem.  log4j2.xml org.xml.sax.SAXParseException; lineNumber: 1; columnNumber: 7; The processi ng instruction target matching "[xX][mM][lL]" is not allowed.         at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:257)         at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:347)         at org.apache.logging.log4j.core.config.XMLConfiguration.<init>(XMLConfiguration.java:145)         at org.apache.logging.log4j.core.config.XMLConfiguration.reconfigure(XMLConfiguration.java:286)         at org.apache.logging.log4j.core.LoggerContext.onChange(LoggerContext.java:421)         at org.apache.logging.log4j.core.config.FileConfigurationMonitor.checkConfiguration(FileConfigurationMonitor.java:79)         at org.apache.logging.log4j.core.Logger PrivateConfig.filter(Logger.java:279)         at org.apache.logging.log4j.core.Logger.isEnabled(Logger.java:117)         at org.apache.logging.log4j.spi.AbstractLoggerWrapper.isEnabled(AbstractLoggerWrapper.java:82)         at org.apache.logging.log4j.spi.AbstractLogger.isDebugEnabled(AbstractLogger.java:1071)         at org.slf4j.impl.SLF4JLogger.isDebugEnabled(SLF4JLogger.java:174)         at org.apache.commons.logging.impl.SLF4JLocationAwareLog.isDebugEnabled(SLF4JLocationAwareLog.java:67) ....  ERROR No logging configuration
LOG4J2-a5a1f1a2$$NPE in AsyncLogger.log(..)$$Our production environment suffers from {noformat} java.lang.NullPointerException at org.apache.logging.log4j.core.async.AsyncLogger.log(AsyncLogger.java:273)     at org.apache.logging.log4j.spi.AbstractLoggerWrapper.log(AbstractLoggerWrapper.java:121)     at org.apache.logging.log4j.spi.AbstractLogger.info(AbstractLogger.java:1006)      at org.springframework.context.support.AbstractApplicationContext.doClose(AbstractApplicationContext.java:873)      at org.springframework.context.support.AbstractApplicationContext 1.run(AbstractApplicationContext.java:809) {noformat}  It looks like something in our app is still logging despite the AsyncLogger having been stopped (and the disruptor field set to null).  The logger could print out a more informative message in this situation.
LOG4J2-60f64cc1$$AsyncAppender ignores RingBufferLogEvents from AsyncLoggers (when all loggers async by setting context selector)$$AsyncAppender's #append method currently has this code: {code} if (!(logEvent instanceof Log4jLogEvent)) {     return; // only know how to Serialize Log4jLogEvents } {code}  When all loggers are made asynchronous by setting Log4jContextSelector to {{org.apache.logging.log4j.core.async.AsyncLoggerContextSelector}}, they produce {{RingBufferLogEvent}} instances, not {{Log4jLogEvent}}. These log events will be dropped by AsyncAppender.
LOG4J2-3b2e880e$$Failed to write log event to CouchDB due to error: Connection pool shut down$$I'm trying to setup a NoSQL logger using Apache CouchDB. After logging a single message, the logger fails with the following exception:  {color: blue}   2014-06-22 10:22:18,590 ERROR An exception occurred processing Appender databaseAppender org.apache.logging.log4j.core.appender.AppenderLoggingException: Failed to write log event to CouchDB due to error: Connection pool shut down 	at org.apache.logging.log4j.core.appender.db.nosql.couchdb.CouchDBConnection.insertObject(CouchDBConnection.java:57) 	at org.apache.logging.log4j.core.appender.db.nosql.NoSQLDatabaseManager.writeInternal(NoSQLDatabaseManager.java:148) 	at org.apache.logging.log4j.core.appender.db.AbstractDatabaseManager.write(AbstractDatabaseManager.java:159) 	at org.apache.logging.log4j.core.appender.db.AbstractDatabaseAppender.append(AbstractDatabaseAppender.java:103) 	at org.apache.logging.log4j.core.config.AppenderControl.callAppender(AppenderControl.java:97) 	at org.apache.logging.log4j.core.config.LoggerConfig.callAppenders(LoggerConfig.java:425) 	at org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:406) 	at org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:367) 	at org.apache.logging.log4j.core.Logger.log(Logger.java:112) 	at org.apache.logging.log4j.spi.AbstractLogger.error(AbstractLogger.java:577) 	at be.pw999.kbomap.controller.KboMapController.getJson(KboMapController.java:65) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:601) 	at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory 1.invoke(ResourceMethodInvocationHandlerFactory.java:81) 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:125) 	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider TypeOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:195) 	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:91) 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:346) 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:341) 	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:101) 	at org.glassfish.jersey.server.ServerRuntime 1.run(ServerRuntime.java:224) 	at org.glassfish.jersey.internal.Errors 1.call(Errors.java:271) 	at org.glassfish.jersey.internal.Errors 1.call(Errors.java:267) 	at org.glassfish.jersey.internal.Errors.process(Errors.java:315) 	at org.glassfish.jersey.internal.Errors.process(Errors.java:297) 	at org.glassfish.jersey.internal.Errors.process(Errors.java:267) 	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:317) 	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:198) 	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:946) 	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:323) 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:372) 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:335) 	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:218) 	at org.apache.catalina.core.StandardWrapper.service(StandardWrapper.java:1682) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:344) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:214) 	at org.apache.logging.log4j.core.web.Log4jServletFilter.doFilter(Log4jServletFilter.java:66) 	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:256) 	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:214) 	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:316) 	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:160) 	at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:734) 	at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:673) 	at com.sun.enterprise.web.WebPipeline.invoke(WebPipeline.java:99) 	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:174) 	at org.apache.catalina.connector.CoyoteAdapter.doService(CoyoteAdapter.java:357) 	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:260) 	at com.sun.enterprise.v3.services.impl.ContainerMapper.service(ContainerMapper.java:188) 	at org.glassfish.grizzly.http.server.HttpHandler.runService(HttpHandler.java:191) 	at org.glassfish.grizzly.http.server.HttpHandler.doHandle(HttpHandler.java:168) 	at org.glassfish.grizzly.http.server.HttpServerFilter.handleRead(HttpServerFilter.java:189) 	at org.glassfish.grizzly.filterchain.ExecutorResolver 9.execute(ExecutorResolver.java:119) 	at org.glassfish.grizzly.filterchain.DefaultFilterChain.executeFilter(DefaultFilterChain.java:288) 	at org.glassfish.grizzly.filterchain.DefaultFilterChain.executeChainPart(DefaultFilterChain.java:206) 	at org.glassfish.grizzly.filterchain.DefaultFilterChain.execute(DefaultFilterChain.java:136) 	at org.glassfish.grizzly.filterchain.DefaultFilterChain.process(DefaultFilterChain.java:114) 	at org.glassfish.grizzly.ProcessorExecutor.execute(ProcessorExecutor.java:77) 	at org.glassfish.grizzly.nio.transport.TCPNIOTransport.fireIOEvent(TCPNIOTransport.java:838) 	at org.glassfish.grizzly.strategies.AbstractIOStrategy.fireIOEvent(AbstractIOStrategy.java:113) 	at org.glassfish.grizzly.strategies.WorkerThreadIOStrategy.run0(WorkerThreadIOStrategy.java:115) 	at org.glassfish.grizzly.strategies.WorkerThreadIOStrategy.access 100(WorkerThreadIOStrategy.java:55) 	at org.glassfish.grizzly.strategies.WorkerThreadIOStrategy WorkerThreadRunnable.run(WorkerThreadIOStrategy.java:135) 	at org.glassfish.grizzly.threadpool.AbstractThreadPool Worker.doWork(AbstractThreadPool.java:564) 	at org.glassfish.grizzly.threadpool.AbstractThreadPool Worker.run(AbstractThreadPool.java:544) 	at java.lang.Thread.run(Thread.java:722) Caused by: java.lang.IllegalStateException: Connection pool shut down 	at org.apache.http.util.Asserts.check(Asserts.java:34) 	at org.apache.http.pool.AbstractConnPool.lease(AbstractConnPool.java:169) 	at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.requestConnection(PoolingHttpClientConnectionManager.java:217) 	at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:158) 	at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:195) 	at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:86) 	at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:108) 	at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:186) 	at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:72) 	at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:57) 	at org.lightcouch.CouchDbClientBase.executeRequest(CouchDbClientBase.java:409) 	at org.lightcouch.CouchDbClientBase.put(CouchDbClientBase.java:517) 	at org.lightcouch.CouchDbClientBase.save(CouchDbClientBase.java:273) 	at org.apache.logging.log4j.core.appender.db.nosql.couchdb.CouchDBConnection.insertObject(CouchDBConnection.java:51) 	... 66 more]] {color}   The log4j2.xml file is: {code:xml} <?xml version="1.0" encoding="UTF-8"?> <Configuration status="info">   <Appenders>     <NoSql name="databaseAppender">       <CouchDb databaseName="kbomaplog" protocol="http" server="127.0.0.1" port="5984"                username="loguser" password="meh" />     </NoSql>   </Appenders>   <Loggers>     <Root level="info">       <AppenderRef ref="databaseAppender"/>     </Root>   </Loggers> </Configuration> {code}   And the piece of code I'm using to test is: {code} 	private Logger logger = LogManager.getLogger(KboMapController.class); 	 	/** 	 * Does nothing special. Returns a simple JSON object with a count of the code table for testing purposes. 	 *  	 * @param id unused 	 * @param test unused 	 * @return a JSON representation of the filled in {@link Enterprise} object. 	 */ 	@GET 	@Produces(MediaType.APPLICATION_JSON) 	@Path("{id}/{test}") 	public Enterprise getJson(@PathParam("id") String id, @PathParam("test") String test) { 		try { 			logger.error("whoaaaaaah"); 			return new Enterprise("SUCCESS", "COUNT=" + dao.count()); 		} catch (SQLException e) { 			return new Enterprise("ERROR", e.getMessage()); 		}  	} {code}  The same issue occurs when the logger is {{static final}}
LOG4J2-2afe3dff$$RollingFileAppender does not create parent directories for the archive files and fails to roll.$$FileRenameAction is not creating the parent directories for the archive files. This cause the file rename and file copy to fail.
LOG4J2-4b77622b$$XInclude not working with relative path$$When using XInclude in a log4j2 configuration, it uses the CWD of the running application instead of the location of the log4j configuration as base. I.e. running the application from within eclipse, the CWD of eclipse is used as base for finding the document to be included.  IMO, the problem is in XmlConfiguration: {code}             final InputStream configStream = configSource.getInputStream();             try {                 buffer = toByteArray(configStream);             } finally {                 configStream.close();             }             final InputSource source = new InputSource(new ByteArrayInputStream(buffer));             final Document document = newDocumentBuilder().parse(source); {code} There is no way the DOMParser can know, where the base should be, because it is just parsing an InputStream and has no file location.   The fix would be to add source.setSystemId(configSource.getLocation()) before parsing the document.
LOG4J2-97203de8$$Async loggers convert message parameters toString at log record writing not at log statement execution$$http://javaadventure.blogspot.com/2014/07/log4j-20-async-loggers-and-immutability.html  When using parameterized messages, the toString() method of the log messages is not called when the log message is enqueued, rather after the log message has been dequeued for writing. If any of the message parameters are mutable, they can thus have changed state before the log message is written, thus resulting in the logged message content being incorrect.  From the blog post, code that demonstrates the problem: {code} import org.apache.logging.log4j.LogManager; import org.apache.logging.log4j.Logger; import java.util.concurrent.atomic.AtomicLong;  public class App {     private static final AtomicLong value = new AtomicLong();     public String toString() {         return Long.toString(value.get());     }     public long next() {         return value.incrementAndGet();     }      public static void main(String[] args) {         for (int i = 0; i < 32; i++) {             new Thread() {                 final Logger logger = LogManager.getLogger(App.class);                  final App instance = new App();                 @Override                  public void run() {                      for (int i = 0; i < 100000; i++) {                          logger.warn("{} == {}", instance.next(), instance);                      }                  }             }.start();         }     } } {code}  Here is the first few lines of logging output {code} 2014-07-28 15:59:45,729 WARN t.App [Thread-13] 13 == 13  2014-07-28 15:59:45,730 WARN t.App [Thread-29] 29 == 29  2014-07-28 15:59:45,729 WARN t.App [Thread-15] 15 == 15  2014-07-28 15:59:45,729 WARN t.App [Thread-6] 6 == 6  2014-07-28 15:59:45,730 WARN t.App [Thread-30] 30 == 30  2014-07-28 15:59:45,729 WARN t.App [Thread-20] 20 == 20  2014-07-28 15:59:45,729 WARN t.App [Thread-8] 8 == 8  2014-07-28 15:59:45,730 WARN t.App [Thread-28] 28 == 28  2014-07-28 15:59:45,729 WARN t.App [Thread-19] 19 == 19  2014-07-28 15:59:45,729 WARN t.App [Thread-18] 18 == 18  2014-07-28 15:59:45,729 WARN t.App [Thread-5] 5 == 6  2014-07-28 15:59:45,731 WARN t.App [Thread-13] 33 == 37  2014-07-28 15:59:45,731 WARN t.App [Thread-8] 39 == 39  2014-07-28 15:59:45,731 WARN t.App [Thread-28] 40 == 41  2014-07-28 15:59:45,731 WARN t.App [Thread-18] 42 == 43  2014-07-28 15:59:45,731 WARN t.App [Thread-5] 43 == 43 {code}  To make my previous code work with Asynchronous loggers (other than by fixing the mutable state) I would need to log like this:  {code} if (logger.isWarnEnabled()) {     logger.warn("{} == {}", instance.next(), instance.toString()); } {code}
LOG4J2-b2ec5106$$Async loggers convert message parameters toString at log record writing not at log statement execution$$http://javaadventure.blogspot.com/2014/07/log4j-20-async-loggers-and-immutability.html  When using parameterized messages, the toString() method of the log messages is not called when the log message is enqueued, rather after the log message has been dequeued for writing. If any of the message parameters are mutable, they can thus have changed state before the log message is written, thus resulting in the logged message content being incorrect.  From the blog post, code that demonstrates the problem: {code} import org.apache.logging.log4j.LogManager; import org.apache.logging.log4j.Logger; import java.util.concurrent.atomic.AtomicLong;  public class App {     private static final AtomicLong value = new AtomicLong();     public String toString() {         return Long.toString(value.get());     }     public long next() {         return value.incrementAndGet();     }      public static void main(String[] args) {         for (int i = 0; i < 32; i++) {             new Thread() {                 final Logger logger = LogManager.getLogger(App.class);                  final App instance = new App();                 @Override                  public void run() {                      for (int i = 0; i < 100000; i++) {                          logger.warn("{} == {}", instance.next(), instance);                      }                  }             }.start();         }     } } {code}  Here is the first few lines of logging output {code} 2014-07-28 15:59:45,729 WARN t.App [Thread-13] 13 == 13  2014-07-28 15:59:45,730 WARN t.App [Thread-29] 29 == 29  2014-07-28 15:59:45,729 WARN t.App [Thread-15] 15 == 15  2014-07-28 15:59:45,729 WARN t.App [Thread-6] 6 == 6  2014-07-28 15:59:45,730 WARN t.App [Thread-30] 30 == 30  2014-07-28 15:59:45,729 WARN t.App [Thread-20] 20 == 20  2014-07-28 15:59:45,729 WARN t.App [Thread-8] 8 == 8  2014-07-28 15:59:45,730 WARN t.App [Thread-28] 28 == 28  2014-07-28 15:59:45,729 WARN t.App [Thread-19] 19 == 19  2014-07-28 15:59:45,729 WARN t.App [Thread-18] 18 == 18  2014-07-28 15:59:45,729 WARN t.App [Thread-5] 5 == 6  2014-07-28 15:59:45,731 WARN t.App [Thread-13] 33 == 37  2014-07-28 15:59:45,731 WARN t.App [Thread-8] 39 == 39  2014-07-28 15:59:45,731 WARN t.App [Thread-28] 40 == 41  2014-07-28 15:59:45,731 WARN t.App [Thread-18] 42 == 43  2014-07-28 15:59:45,731 WARN t.App [Thread-5] 43 == 43 {code}  To make my previous code work with Asynchronous loggers (other than by fixing the mutable state) I would need to log like this:  {code} if (logger.isWarnEnabled()) {     logger.warn("{} == {}", instance.next(), instance.toString()); } {code}
LOG4J2-73400bfb$$Log4jLogger only accepts Log4jMarker, not SLF4J's Marker$$We're using Log4j 2 via SLF4J. A Logger's log methods have signatures like this: public abstract void warn(org.slf4j.Marker marker, java.lang.String msg)  If you use an object that is an Marker but not a Log4jMarker this fails at org.apache.logging.slf4j.Log4jLogger.getMarker(Log4jLogger.java:378) due to "cannot be cast to org.apache.logging.slf4j.Log4jMarker".  Use case: we have a defined set of Markers. There's an enum for this, implementing SLF4J's marker interface. Obviously with Log4j we cannot use this enum.  I think an org.apache.logging.slf4j.Log4jLogger cannot expect an org.apache.logging.slf4j.Log4jMarker.
LOG4J2-7bb1ad47$$SimpleLogger throws ArrayIndexOutOfBoundsException for an empty array$$There seems to be an issue with SimpleLogger implementation provided by log4j2. The issue seems to be in the new improved API supporting placeholders and var args when called with an Object Array of size 0.  for e.g logger.error("Hello World {} in {} " , new Object[0]);  A statement above results in an error as shown below  ERROR StatusLogger Unable to locate a logging implementation, using SimpleLogger Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: -1        at org.apache.logging.log4j.simple.SimpleLogger.logMessage(SimpleLogger.java:157)        at org.apache.logging.log4j.spi.AbstractLogger.logMessage(AbstractLogger.java:1347)        at org.apache.logging.log4j.spi.AbstractLogger.logIfEnabled(AbstractLogger.java:1312)        at org.apache.logging.log4j.spi.AbstractLogger.error(AbstractLogger.java:539)        at TestError.main(TestError.java:21)   Solution to place a check in SimpleLogger for checking the size of the array .
LOG4J2-0bea17d7$$MarkerManager Log4jMarker.hasParents() returns opposite of correct result$$Log4JMarker.hasParents() will return false when the marker has parents, and true when it has none.   The javadoc in the Marker interface indicates it should function the other way around:  {quote} "Indicates whether this Marker has references to any other Markers.  Return true if the Marker has parent Markers" {quote}  The code for the implementation (that I could find) demonstrates that it would function in the opposite way as it is described in that javadoc:  {code} @Override  public boolean hasParents() {       return this.parents == null;  } {code}
LOG4J2-411dad65$$ThrowableProxy fails if a class in logged stack trace throws java.lang.Error from initializer$$When the Logger attempts to log a message with an exception stack trace, it uses the ThrowableProxy class to introspect classes in the stack trace frames.  If the class sun.reflect.misc.Trampoline is in the stack trace, the introspection performed by ThrowableProxy will fail causing a java.lang.Error to be thrown by the Logger call.  The sun.reflect.misc.Trampoline class is used by the sun.reflect.misc.MethodUtil class to perform reflection-based method invocations. MethodUtil is widely used by libraries to perform method invocations. I've encountered this problem when invoking methods over JMX and inside Jetty.  I am classifying this as a blocker because it means that any logging statement that is logging a Throwable message containing a MethodUtil-based reflection stack trace can cause a java.lang.Error to be thrown by Log4j2.   I will attach a unit test for this failure.
LOG4J2-d3989b40$$ThrowableProxy throws NoClassDefFoundError$$In method *loadClass* we expect {{ClassNotFoundException}}. But if class comes from another java machine we can get {{NoClassDefFoundError}}.  Possible fix: {code:java} private Class<?> loadClass(final ClassLoader lastLoader, final String className) {        // XXX: this is overly complicated        Class<?> clazz;        if (lastLoader != null) {            try {                clazz = Loader.initializeClass(className, lastLoader);                if (clazz != null) {                    return clazz;                }            } catch (final Throwable ignore) {                // Ignore exception.            }        }        try {            clazz = Loader.loadClass(className);        } catch (final ClassNotFoundException | LinkageError ignored) {            try {                clazz = Loader.initializeClass(className, this.getClass().getClassLoader());            } catch (final ClassNotFoundException | LinkageError ignore) {                return null;            }        }        return clazz;    } {code}
LOG4J2-f9b0bbee$$JUL adapter does not map Log4j's FATAL level to a JUL level$$JUL module does not map Log4j's FATAL level. See {{org.apache.logging.log4j.jul.DefaultLevelConverter}}.
LOG4J2-f8a42197$$ThrowableProxy.getExtendedStackTraceAsString causes NullPointerException$$I'm trying to write a poc with Log4j 2.1, where distributed processes are logging to a remote server. The server is currently running the bundled TcpSocketServer.createSerializedSocketServer with a custom layout plugin.   A process is logging an exception. I can then see in the custom layout plugin at the log server that the LogEvent doesn't contain a thrown, but that it contains a thrownProxy. So far so good. I'm then trying to get hold of a String representation of the message + stacktrace. I thought that I would be able to e.g invoke ThrowableProxy.getExtendedStackTraceAsString(), but that causes a NullPointerException since the throwable in the ThrowableProxy also is null after deserialization. Looks like ThrowableProxy assumes that throwable isn't null in a few methods.   The exception that is logged by the client process is a simple new Exception("A message");  The pom.xml that I'm using: {code:xml} <dependency> 	<groupId>org.apache.logging.log4j</groupId> 	<artifactId>log4j-api</artifactId> 	<version>2.1</version> </dependency> <dependency> 	<groupId>org.apache.logging.log4j</groupId> 	<artifactId>log4j-core</artifactId> 	<version>2.1</version> </dependency> <dependency> 	<groupId>com.lmax</groupId> 	<artifactId>disruptor</artifactId> 	<version>3.3.0</version> </dependency> {code} The stacktrace that I get in the server: {code} 2014-12-05 14:30:44,601 ERROR An exception occurred processing Appender XXXXX java.lang.NullPointerException 	at org.apache.logging.log4j.core.impl.ThrowableProxy.getExtendedStackTraceAsString(ThrowableProxy.java:340) 	at org.apache.logging.log4j.core.impl.ThrowableProxy.getExtendedStackTraceAsString(ThrowableProxy.java:323) {code} Workaround: To invoke ThrowableProxy. getExtendedStackTrace() and format the stacktrace + message with my own format methods.
LOG4J2-d8af1c93$$Variable substitution:  {sys:foo} defaults to <property name=":foo">, should default to <property name="foo">$$The following configuration doesn't work ( {sys:log.level} can't be resolved even though default value is provided).  <?xml version="1.0" encoding="UTF-8"?> <configuration status="OFF">   <properties>     <property name="log.level">error</property>     <property name=":log.level">ACTUALLY_GETS_USED</property>   </properties>   <appenders>     <Console name="Console" target="SYSTEM_OUT">       <PatternLayout pattern="%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n"/>     </Console>   </appenders>   <loggers>     <root level=" {sys:log.level}">       <appender-ref ref="Console"/>     </root>   </loggers> </configuration>   In org.apache.logging.log4j.core.lookup.Interpolator.lookup(LogEvent, String), on line 110, var = var.substring(prefixPos) should be var = var.substring(prefixPos + 1) instead.
LOG4J2-16ad8763$$StringFormattedMessage serialization is incorrect$$The method {{writeObject(final ObjectOutputStream out)}} of the class {{org.apache.logging.log4j.message.StringFormattedMessage}} does not write the stringArgs array into the output stream. This causes {{readObject(final ObjectInputStream in)}} to throw an {{EOFException}} when trying to deserialize.  There is another bug in the same method. The line {{stringArgs[i] = obj.toString();}} throws a {{NullPointerException}} when obj is null.
LOG4J2-43517f15$$System.out no longer works after the Console appender and JANSI are initialized$$h3. Demonstration  The underlining project demonstrate the bug.  The project's build.gradle file:  {code:title=build.gradle} apply plugin: 'java'  version = '1.0'  repositories {     mavenCentral() }  def log4j2Version = '2.2' def log4j2GroupId = "org.apache.logging.log4j"  dependencies {     compile log4j2GroupId + ':log4j-core:' + log4j2Version     compile log4j2GroupId + ":log4j-jcl:" + log4j2Version     compile log4j2GroupId + ":log4j-slf4j-impl:" + log4j2Version     compile 'org.fusesource.jansi:jansi:1.11' } {code}  A log4j2.xml in classpath:  {code:title=log4j2.xml} <?xml version="1.0" encoding="UTF-8"?> <Configuration status="WARN">     <Appenders>         <File name="root" fileName=" {sys:user.home}/logs/windowsbug.log">             <PatternLayout>                 <Pattern>%d %p %c{1.} [%t] %m%n</Pattern>             </PatternLayout>         </File>     </Appenders>     <Loggers>         <Root level="info">             <AppenderRef ref="root"/>         </Root>     </Loggers> </Configuration> {code}  And the main class:  {code:title=Log4j2WindowsBug.java} import org.slf4j.LoggerFactory;  /**  * @author khotyn 15/3/2 下午8:17  */ public class Log4j2WindowsBug {      public static void main(String[] args) {         System.out.println("Able to print on Windows");         LoggerFactory.getLogger(Log4j2WindowsBug.class);         System.out.println("Unable to print on Windows");     } } {code}  The output of the demo under Windows is:  {code} Able to print on Windows {code}  The third line did not print to Windows console.  h3. Reason  It seems that log4j2 will wrapper System.out to WindowsAnsiOutputStream if jansi is available in classpath. And in OutputStreamManager's close method, the wrapper WindowsAnsiOutputStream is not considered, and cause the underling System.out closed.  {code:title=OutputStreamManager.java}     protected synchronized void close() {         final OutputStream stream = os; // access volatile field only once per method         if (stream == System.out || stream == System.err) {             return;         }         try {             stream.close();         } catch (final IOException ex) {             LOGGER.error("Unable to close stream " + getName() + ". " + ex);         }     } {code}
LOG4J2-3cee912e$$Async root logger config is defaulting includeLocation to true without use of Log4jContextSelector system property$$I'm using the approach detailed here - https://logging.apache.org/log4j/2.x/manual/async.html - under "Mixing Synchronous and Asynchronous Loggers" where we have the <asyncRoot> logger defined. I noticed this was slow so looked into it and noticed the location was being captured but I thought this should default to false for async loggers. Looking into this, the line here - https://github.com/apache/logging-log4j2/blob/master/log4j-core/src/main/java/org/apache/logging/log4j/core/async/AsyncLoggerConfig.java#L239 - the call to includeLocation() is actually calling LoggerConfig.includeLocation() which checks for the existence of the system property (which we don't have set), therefore include location defaults to true. I think instead it should be calling the includeLocation() static method inside of AsyncLoggerConfig here - https://github.com/apache/logging-log4j2/blob/master/log4j-core/src/main/java/org/apache/logging/log4j/core/async/AsyncLoggerConfig.java#L204 - which would end up defaulting this to false correctly as the includeLocation value is actually null since I didn't explicitly configured it.
MATH-91d280b7$$ArrayIndexOutOfBoundsException in MathArrays.linearCombination$$When MathArrays.linearCombination is passed arguments with length 1, it throws an ArrayOutOfBoundsException. This is caused by this line:  double prodHighNext = prodHigh[1];  linearCombination should check the length of the arguments and fall back to simple multiplication if length == 1.
MATH-a4ffd393$$EigenDecomposition.Solver should consider tiny values 0 for purposes of determining singularity$$EigenDecomposition.Solver tests for singularity by comparing eigenvalues to 0 for exact equality. Elsewhere in the class and in the code, of course, very small values are considered 0. This causes the solver to consider some singular matrices as non-singular.  The patch here includes a test as well showing the behavior -- the matrix is clearly singular but isn't considered as such since one eigenvalue are ~1e-14 rather than exactly 0.  (What I am not sure of is whether we should really be evaluating the *norm* of the imaginary eigenvalues rather than real/imag components separately. But the javadoc says the solver only supports real eigenvalues anyhow, so it's kind of moot since imag=0 for all eigenvalues.)
MATH-c979a6f0$$EigenDecomposition.Solver should consider tiny values 0 for purposes of determining singularity$$EigenDecomposition.Solver tests for singularity by comparing eigenvalues to 0 for exact equality. Elsewhere in the class and in the code, of course, very small values are considered 0. This causes the solver to consider some singular matrices as non-singular.  The patch here includes a test as well showing the behavior -- the matrix is clearly singular but isn't considered as such since one eigenvalue are ~1e-14 rather than exactly 0.  (What I am not sure of is whether we should really be evaluating the *norm* of the imaginary eigenvalues rather than real/imag components separately. But the javadoc says the solver only supports real eigenvalues anyhow, so it's kind of moot since imag=0 for all eigenvalues.)
MATH-bda25b40$$EigenDecomposition may not converge for certain matrices$$Jama-1.0.3 contains a bugfix for certain matrices where the original code goes into an infinite loop.  The commons-math translations would throw a MaxCountExceededException, so fails to compute the eigen decomposition.  Port the fix from jama to CM.
MATH-4ebd967c$$Beta, LogNormalDistribution, WeibullDistribution give slightly wrong answer for extremely small args due to log/exp inaccuracy$$Background for those who aren't familiar: math libs like Math and FastMath have two mysterious methods, log1p and expm1. log1p(x) = log(1+x) and expm1(x) = exp(x)-1 mathetmatically, but can return a correct answer even when x was small, where floating-point error due to the addition/subtraction introduces a relatively large error.  There are three instances in the code that can employ these specialized methods and gain a measurable improvement in accuracy. See patch and tests for an example -- try the tests without the code change to see the error.
MATH-996c0c16$$EnumeratedRealDistribution.inverseCumulativeProbability returns values not in the samples set$$The method EnumeratedRealDistribution.inverseCumulativeProbability() sometimes returns values that are not in the initial samples domain... I will attach a test to exploit this bug.
MATH-aff82362$$Stack overflow in Beta.regularizedBeta$$In org.apache.commons.math3.special.Beta.regularizedBeta(double,double,double,double,int), the case   } else if (x > (a + 1.0) / (a + b + 2.0)) {       ret = 1.0 - regularizedBeta(1.0 - x, b, a, epsilon, maxIterations); }   is prone to infinite recursion: If x is approximately the tested value, then 1-x is approximately the tested value in the recursion. Thus, due to loss of precision after the subtraction, this condition can be true for the recursive call as well.  Example: double x= Double.longBitsToDouble(4597303555101269224L); double a= Double.longBitsToDouble(4634227472812299606L); double b = Double.longBitsToDouble(4642050131540049920L); System.out.println(x > (a + 1.0) / (a + b + 2.0)); System.out.println(1-x>(b + 1.0) / (b + a + 2.0)); System.out.println(1-(1-x)>(a + 1.0) / (a + b + 2.0));  Possible solution: change the condition to x > (a + 1.0) / (a + b + 2.0) && 1-x<=(b + 1.0) / (b + a + 2.0)
MATH-b12610d3$$KendallsCorrelation suffers from integer overflow for large arrays.$$For large array size (say, over 5,000), numPairs > 10 million. in line 258, (numPairs - tiedXPairs) * (numPairs - tiedYPairs) possibly > 100 billion, which will cause an integer overflow, resulting in a negative number, which will result in the end result in a NaN since the square-root of that number is calculated. This can easily be solved by changing line 163 to final long numPairs = ((long)n) * (n - 1) / 2; // to avoid overflow
MATH-8e5867ed$$Incorrect rounding of float$$package org.apache.commons.math3.util  example of usage of round functions of Precision class:  Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01 Precision.round((float) 0.0, 2) = 0.0 Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0  Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.  I think, same problem will be found at usage of other round modes.
MATH-b285f170$$The LinearConstraintSet shall return its constraints in a deterministic way$$As previously discussed on the mailinglist, the LinearConstraintSet should return its internally stored LinearConstraints in the same iteration order as they have been provided via its constructor.  This ensures that the execution of the same linear problem results in the same results each time it is executed. This is especially important when linear problems are loaded from a file, e.g. mps format, and makes it simpler to debug problems and compare with other solvers which do the same thing.
MATH-63d88c74$$MultidimensionalCounter does not throw "NoSuchElementException"$$The iterator should throw when "next()" is called even though "hasNext()" would return false.
MATH-e91d0f05$$Precision.round() returns different results when provided negative zero as double or float$$Precision.round(-0.0d, x) = 0.0 Precision.round(-0.0f, x) = -0.0  After discussion on the mailinglist, the result should always be -0.0.
MATH-7cfbc0da$$arcs set split covers full circle instead of being empty$$When splitting an arcs set using an arc very close to one of the boundaries (but not at the boundary), the algorithm confuses cases for which end - start = 2pi from cases for which end - start = epsilon.  The following test case shows such a failure: {code}     @Test     public void testSplitWithinEpsilon() {         double epsilon = 1.0e-10;         double a = 6.25;         double b = a - 0.5 * epsilon;         ArcsSet set = new ArcsSet(a - 1, a, epsilon);         Arc arc = new Arc(b, b + FastMath.PI, epsilon);         ArcsSet.Split split = set.split(arc);         Assert.assertEquals(set.getSize(), split.getPlus().getSize(),  epsilon);         Assert.assertNull(split.getMinus());     } {code}  The last assertion (split.getMinus() being null) fails, as with current code split.getMinus() covers the full circle from 0 to 2pi.
MATH-19c1c3bb$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples
MATH-faf99727$$implementation of smallest enclosing ball algorithm sometime fails$$The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.  In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.  Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are  * WelzlEncloser2DTest.testReducingBall * WelzlEncloser2DTest.testLargeSamples * WelzlEncloser3DTest.testInfiniteLoop * WelzlEncloser3DTest.testLargeSamples
MATH-a6f96306$$Convergence Checker Fixes$$None
MATH-e2dc384d$$LevenburgMaquardt switched evaluation and iterations$$None
MATH-2a6c6409$$Constructor of PolyhedronsSet throws NullPointerException$$The following statement throws a NullPointerException: new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);  I found that other numbers also produce that effect. The stack trace: java.lang.NullPointerException         at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)         at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)         at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)         at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)
MATH-f4c926ea$$twod.PolygonsSet.getSize produces NullPointerException if BSPTree has no nodes$$org.apache.commons.math3.geometry.euclidean.twod.PolygonsSet.getSize() uses a tree internally:  final BSPTree<Euclidean2D> tree = getTree(false);  However, if that tree contains no data, it seems that the reference returned is null, which causes a subsequent NullPointerException.  Probably an exception with a message ("tree has no data") would clarify that this is an API usage error.
MATH-5a6ccd58$$Brent optimizer doesn't use the Base optimizer iteration counter$$BrentOptimizer uses "iter" defined in "doOptimize"  to count iterations. It should ideally use the iteration counter defined for the BaseOptimizer.
MATH-a197ba85$$NPE in BSPTree#fitToCell()$$Hello,  I faced a NPE using  BSPTree#fitToCell() from the SVN trunk. I fixed the problem using a small patch I will attach to the ticket.
MATH-ba62c59d$$2.0 equal to -2.0$$The following test fails:  {code}     @Test     public void testMath1127() {         Assert.assertFalse(Precision.equals(2.0, -2.0, 1));     } {code}
MATH-d4f978dd$$Percentile Computation errs$$In the following test, the 75th percentile is _smaller_ than the 25th percentile, leaving me with a negative interquartile range.  {code:title=Bar.java|borderStyle=solid} @Test public void negativePercentiles(){          double[] data = new double[]{                 -0.012086732064244697,                  -0.24975668704012527,                  0.5706168483164684,                  -0.322111769955327,                  0.24166759508327315,                  Double.NaN,                  0.16698443218942854,                  -0.10427763937565114,                  -0.15595963093172435,                  -0.028075857595882995,                  -0.24137994506058857,                  0.47543170476574426,                  -0.07495595384947631,                  0.37445697625436497,                  -0.09944199541668033         };         DescriptiveStatistics descriptiveStatistics = new DescriptiveStatistics(data);          double threeQuarters = descriptiveStatistics.getPercentile(75);         double oneQuarter = descriptiveStatistics.getPercentile(25);          double IQR = threeQuarters - oneQuarter;                  System.out.println(String.format("25th percentile %s 75th percentile %s", oneQuarter, threeQuarters ));                  assert IQR >= 0;              } {code}
MATH-a7363a2a$$Bug in MonotoneChain: a collinear point landing on the existing boundary should be dropped (patch)$$The is a bug on the code in MonotoneChain.java that attempts to handle the case of a point on the line formed by the previous last points and the last point of the chain being constructed. When `includeCollinearPoints` is false, the point should be dropped entirely. In common-math 3,3, the point is added, which in some cases can cause a `ConvergenceException` to be thrown.  In the patch below, the data points are from a case that showed up in testing before we went to production.  {code:java} Index: src/main/java/org/apache/commons/math3/geometry/euclidean/twod/hull/MonotoneChain.java =================================================================== --- src/main/java/org/apache/commons/math3/geometry/euclidean/twod/hull/MonotoneChain.java	(revision 1609491) +++ src/main/java/org/apache/commons/math3/geometry/euclidean/twod/hull/MonotoneChain.java	(working copy) @@ -160,8 +160,8 @@                  } else {                      if (distanceToCurrent > distanceToLast) {                          hull.remove(size - 1); +                        hull.add(point);                      } -                    hull.add(point);                  }                  return;              } else if (offset > 0) { Index: src/test/java/org/apache/commons/math3/geometry/euclidean/twod/hull/ConvexHullGenerator2DAbstractTest.java =================================================================== --- src/test/java/org/apache/commons/math3/geometry/euclidean/twod/hull/ConvexHullGenerator2DAbstractTest.java	(revision 1609491) +++ src/test/java/org/apache/commons/math3/geometry/euclidean/twod/hull/ConvexHullGenerator2DAbstractTest.java	(working copy) @@ -204,6 +204,24 @@      }        @Test +    public void testCollinnearPointOnExistingBoundary() { +        final Collection<Vector2D> points = new ArrayList<Vector2D>(); +        points.add(new Vector2D(7.3152, 34.7472)); +        points.add(new Vector2D(6.400799999999997, 34.747199999999985)); +        points.add(new Vector2D(5.486399999999997, 34.7472)); +        points.add(new Vector2D(4.876799999999999, 34.7472)); +        points.add(new Vector2D(4.876799999999999, 34.1376)); +        points.add(new Vector2D(4.876799999999999, 30.48)); +        points.add(new Vector2D(6.0959999999999965, 30.48)); +        points.add(new Vector2D(6.0959999999999965, 34.1376)); +        points.add(new Vector2D(7.315199999999996, 34.1376)); +        points.add(new Vector2D(7.3152, 30.48)); + +        final ConvexHull2D hull = generator.generate(points); +        checkConvexHull(points, hull); +    } + +    @Test      public void testIssue1123() {            List<Vector2D> points = new ArrayList<Vector2D>(); {code}
MATH-cc4ab51e$$BinomialDistribution deals with degenerate cases incorrectly$$The following calculation returns false results:  {{new BinomialDistribution(0, 0.01).logProbability(0)}}  It evaluates to Double.NaN when it should be 0 (cf., for example, "dbinom(0, 0, 0.01, log=T)" in R).  I attach a patch dealing with the problem. The patch also adds a test for this bug.
MATH-2f2a2dda$$UniformIntegerDistribution should make constructer a exclusive bound or made parameter check more relax$$UniformIntegerDistribution constructer  public UniformIntegerDistribution(RandomGenerator rng,                                       int lower,                                       int upper)  the lower and the upper all inclusive. but the parameter check made a   if (lower >= upper) {             throw new NumberIsTooLargeException(                             LocalizedFormats.LOWER_BOUND_NOT_BELOW_UPPER_BOUND,                             lower, upper, false); check, i think it is too strict to construct UniformIntegerDistribution (0,0)  this should make it possible
MATH-4080feff$$MonotoneChain handling of collinear points drops low points in a near-column$$This code {code} val points = List(   new Vector2D(     16.078200000000184,     -36.52519999989808   ),   new Vector2D(     19.164300000000186,     -36.52519999989808   ),   new Vector2D(     19.1643,     -25.28136477910407   ),   new Vector2D(     19.1643,     -17.678400000004157   ) ) new hull.MonotoneChain().generate(points.asJava) {code}  results in the exception: {code} org.apache.commons.math3.exception.ConvergenceException: illegal state: convergence failed 	at org.apache.commons.math3.geometry.euclidean.twod.hull.AbstractConvexHullGenerator2D.generate(AbstractConvexHullGenerator2D.java:106) 	at org.apache.commons.math3.geometry.euclidean.twod.hull.MonotoneChain.generate(MonotoneChain.java:50) 	at .<init>(<console>:13) 	at .<clinit>(<console>) 	at .<init>(<console>:11) 	at .<clinit>(<console>) 	at  print(<console>) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:597) 	at scala.tools.nsc.interpreter.IMain ReadEvalPrint.call(IMain.scala:704) 	at scala.tools.nsc.interpreter.IMain Request  anonfun 14.apply(IMain.scala:920) 	at scala.tools.nsc.interpreter.Line  anonfun 1.apply mcV sp(Line.scala:43) 	at scala.tools.nsc.io.package  anon 2.run(package.scala:25) 	at java.lang.Thread.run(Thread.java:662) {code}  This will be tricky to fix. Not only is the point (19.164300000000186, -36.52519999989808) is being dropped incorrectly, but any point dropped in one hull risks creating a kink when combined with the other hull.
MATH-596ccd59$$Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer$$The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:  If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.  So the if condition: if (membershipMatrix[i][j] > maxMembership) {                     maxMembership = membershipMatrix[i][j];                     newCluster = j; } will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line: clusters.get(newCluster)                     .addPoint(point);  Adding the following condition can solve the problem: double d; if (sum == 0) d = 1; else d = 1.0/sum;
MATH-4aa4c6d3$$getKernel fails for buckets with only multiple instances of the same value in random.EmpiricalDistribution$$After loading a set of values into an EmpericalDistribution, assume that there's a case where a single bin ONLY contains multiple instances of the same value.  In this case the standard deviation will equal zero.  This will fail when getKernel attempts to create a NormalDistribution.  The other case where stddev=0 is when there is only a single value in the bin, and this is handled by returning a ConstantRealDistribution rather than a NormalDistrbution.  See: https://issues.apache.org/jira/browse/MATH-984
MATH-b148046a$$getKernel fails for buckets with only multiple instances of the same value in random.EmpiricalDistribution$$After loading a set of values into an EmpericalDistribution, assume that there's a case where a single bin ONLY contains multiple instances of the same value.  In this case the standard deviation will equal zero.  This will fail when getKernel attempts to create a NormalDistribution.  The other case where stddev=0 is when there is only a single value in the bin, and this is handled by returning a ConstantRealDistribution rather than a NormalDistrbution.  See: https://issues.apache.org/jira/browse/MATH-984
MATH-a56d4998$$bracket function gives up too early$$In UnivariateSolverUtils.bracket(...) the search ends prematurely if a = lowerBound, which ignores some roots in the interval.
MATH-ce2badf0$$EmpiricalDistribution cumulativeProbability can return NaN when evaluated within a constant bin$$If x belongs to a bin with no variance or to which a ConstantRealDistribution kernel has been assigned, cumulativeProbability(x) can return NaN.
MATH-a06a1584$$PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line$$I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.  However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point "behind" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.  I think the dot product above should always be positive. If multiple intersections exist along a "whole" line then the first one in "front" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.
MATH-c44bfe00$$Exception thrown in ode for a pair of close events$$When two discrete events occur closer to each other than the convergence threshold used for locating them, this sometimes triggers a NumberIsTooLargeException.  The exception happens because the EventState class think the second event is simply a numerical artifact (a repetition of the already triggerred first event) and tries to skip past it. If there are no other event in the same step later on, one interval boundary finally reach step end and the interval bounds are reversed.
MATH-96eb80ef$$SimplexSolver returning wrong answer from optimize$$SimplexSolver fails for the following linear program:  min 2x1 +15x2 +18x3  Subject to    -x1 +2x2  -6x3 <=-10             x2  +2x3 <= 6    2x1      +10x3 <= 19     -x1  +x2       <= -2     x1,x2,x3 >= 0  Solution should be x1 = 7 x2 = 0 x3 = 1/2 Objective function = 23  Instead, it is returning x1 = 9.5 x2 = 1/8 x3 = 0 Objective function = 20.875  Constraint number 1 is violated by this answer
MATH-8f35fcb8$$UknownParameterException message prints {0} instead of parameter name$$The constructor for UnknownParameterException stores the parameter name internally but does not forward it to the base class which creates the error message.
MATH-471e6b07$$Digamma calculation produces SOE on NaN argument$$Digamma doesn't work particularly well with NaNs.  How to reproduce: call Gamma.digamma(Double.NaN)  Expected outcome: returns NaN or throws a meaningful exception  Real outcome: crashes with StackOverflowException, as digamma enters infinite recursion.
MATH-09fe956a$$ResizableDoubleArray does not work with double array of size 1$$When attempting to create a ResizableDoubleArray with an array of a single value (e.g. {4.0}), the constructor creates an internal array with 16 entries that are all 0.0  Bug looks like it might be on line 414 of ResizableDoubleArray.java:          if (data != null && data.length > 1) {
MATH-41f29780$$Interval class upper and lower check$$In class Interval, which is in the package org.apache.commons.math4.geometry.euclidean.oned it is possible to pass the value for variable upper  less than the value of variable lower, which is logically incorrect and  also causes the method getSize() to return negative value.  For example:   @Test   public void test1()  throws Throwable  {       Interval interval0 = new Interval(0.0, (-1.0));       double double0 = interval0.getSize();       assertEquals((-1.0), double0, 0.01D);   }
MATH-03178c8b$$NormalDistribution.cumulativeProbability() suffers from cancellation$$I see the following around line 194: {noformat}         return 0.5 * (1 + Erf.erf(dev / (standardDeviation * SQRT2))); {noformat}  When erf() returns a very small value, this cancels in the addition with the "1.0" which leads to poor precision in the results.  I would suggest changing this line to read more like: {noformat} return 0.5 * Erf.erfc( -dev / standardDeviation * SQRT2 ); {noformat}   Should you want some test cases for "extreme values" (one might argue that within 10 standard deviations isn't all that extreme) then you can check the following: http://www.jstatsoft.org/v52/i07/ then look in the v52i07-xls.zip at replication-01-distribution-standard-normal.xls  I think you will also find that evaluation of expressions such as {noformat}NormalDistribution( 0, 1 ).cumulativeProbability( -10.0 );{noformat} are pretty far off.
MATH-4c4b3e2e$$Overflow checks in Fraction multiply(int) / divide(int)$$The member methods multiply(int) / divide(int) in the class org.apache.commons.math3.fraction.Fraction do not have overflow checks.  {code:java} return new Fraction(numerator * i, denominator); {code}  should be  {code:java} return new Fraction(ArithmeticUtils.mulAndCheck(numerator, i), denominator); {code}  or, considering the case gcd(i, denominator) > 1,  {code:java} return multiply(new Fraction(i)); {code}
MATH-a94ff90a$$FastMath.exp may return NaN for non-NaN arguments$$I have observed that FastMath.exp(709.8125) returns NaN. However, the exponential function must never return NaN (if the argument is not NaN). The result must always be non-negative or positive infinity.
MATH-26e878ab$$FastMath.pow(double, long) enters an infinite loop with Long.MIN_VALUE$$FastMath.pow(double, long) enters an infinite loop with Long.MIN_VALUE. It cannot be negated, so unsigned shift (>>>) is required instead of a signed one (>>).
MATH-fb007815$$Incorrect Kendall Tau calc due to data type mistmatch$$The Kendall Tau calculation returns a number from -1.0 to 1.0  due to a mixing of ints and longs, a mistake occurs on large size columns (arrays) passed to the function. an array size of > 50350 triggers the condition in my case - although it may be data dependent  the ver 3.5 library returns 2.6 as a result (outside of the defined range of Kendall Tau)  with the cast to long below - the result reutns to its expected value   commons.math3.stat.correlation.KendallsCorrelation.correlation   here's the sample code I used: I added the cast to long of swaps in the   			int swaps = 1077126315; 			 final long numPairs = sum(50350 - 1); 			    long tiedXPairs = 0; 		        long tiedXYPairs = 0; 		        long tiedYPairs = 0; 		         		  final long concordantMinusDiscordant = numPairs - tiedXPairs - tiedYPairs + tiedXYPairs - 2 * (long) swaps; 	        final double nonTiedPairsMultiplied = 1.6e18; 	        double myTest = concordantMinusDiscordant / FastMath.sqrt(nonTiedPairsMultiplied);
MATH-9e0c5ad4$$Gamma function computation$$In the gamma method, when handling the case "absX > 20", the computation of gammaAbs should replace "x" (see code below with x in bold) by "absX". For large negative values of x, the function returns with the wrong sign.  final double gammaAbs = SQRT_TWO_PI / *x* *                                      FastMath.pow(y, absX + 0.5) *                                      FastMath.exp(-y) * lanczos(absX);
MATH-56434517$$multistep integrator start failure triggers NPE$$Multistep ODE integrators like Adams-Bashforth and Adams-Moulton require a starter procedure. If the starter integrator is not configured properly, it will not create the necessary number of initial points and the multistep integrator will not be initialized correctly. This results in NullPointErException when the scaling array is referenced later on.  The following test case (with an intentionally wrong starter configuration) shows the problem.  {code} @Test public void testStartFailure() {       TestProblem1 pb = new TestProblem1();       double minStep = 0.0001 * (pb.getFinalTime() - pb.getInitialTime());       double maxStep = pb.getFinalTime() - pb.getInitialTime();       double scalAbsoluteTolerance = 1.0e-6;       double scalRelativeTolerance = 1.0e-7;        MultistepIntegrator integ =           new AdamsBashforthIntegrator(4, minStep, maxStep,                                                             scalAbsoluteTolerance,                                                             scalRelativeTolerance);       integ.setStarterIntegrator(new DormandPrince853Integrator(0.2 * (pb.getFinalTime() - pb.getInitialTime()),                                                                 pb.getFinalTime() - pb.getInitialTime(),                                                                 0.1, 0.1));       TestProblemHandler handler = new TestProblemHandler(pb, integ);       integ.addStepHandler(handler);       integ.integrate(pb,                              pb.getInitialTime(), pb.getInitialState(),                              pb.getFinalTime(), new double[pb.getDimension()]);      } {code}  Failure to start the integrator should be detected and an appropriate exception should be triggered.
MATH-1d635088$$BitsStreamGenerator#nextBytes(byte[]) is wrong$$Sequential calls to the BitsStreamGenerator#nextBytes(byte[]) must generate the same sequence of bytes, no matter by chunks of which size it was divided. This is also how java.util.Random#nextBytes(byte[]) works.  When nextBytes(byte[]) is called with a bytes array of length multiple of 4 it makes one unneeded call to next(int) method. This is wrong and produces an inconsistent behavior of classes like MersenneTwister.  I made a new implementation of the BitsStreamGenerator#nextBytes(byte[]) see attached code.
MATH-dbdff075$$SimplexSolver not working as expected?$$I guess (but I could be wrong) that SimplexSolver does not always return the optimal solution, nor satisfies all the constraints...  Consider this LP:  max: 0.8 x0 + 0.2 x1 + 0.7 x2 + 0.3 x3 + 0.6 x4 + 0.4 x5; r1: x0 + x2 + x4 = 23.0; r2: x1 + x3 + x5 = 23.0; r3: x0 >= 10.0; r4: x2 >= 8.0; r5: x4 >= 5.0;  LPSolve returns 25.8, with x0 = 10.0, x1 = 0.0, x2 = 8.0, x3 = 0.0, x4 = 5.0, x5 = 23.0;  The same LP expressed in Apache commons math is:  LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 0.8, 0.2, 0.7, 0.3, 0.6, 0.4 }, 0 ); Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint>(); constraints.add(new LinearConstraint(new double[] { 1, 0, 1, 0, 1, 0 }, Relationship.EQ, 23.0)); constraints.add(new LinearConstraint(new double[] { 0, 1, 0, 1, 0, 1 }, Relationship.EQ, 23.0)); constraints.add(new LinearConstraint(new double[] { 1, 0, 0, 0, 0, 0 }, Relationship.GEQ, 10.0)); constraints.add(new LinearConstraint(new double[] { 0, 0, 1, 0, 0, 0 }, Relationship.GEQ, 8.0)); constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 1, 0 }, Relationship.GEQ, 5.0));  RealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true);  that returns 22.20, with x0 = 15.0, x1 = 23.0, x2 = 8.0, x3 = 0.0, x4 = 0.0, x5 = 0.0;  Is it possible SimplexSolver is buggy that way? The returned value is 22.20 instead of 25.8, and the last constraint (x4 >= 5.0) is not satisfied...  Am I using the interface wrongly?
MATH-38983e82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution.  Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b;  /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5;  /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10   Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[]{7, 3, 0, 0}, 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[]{1, 0, 0, 0}, Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[]{0, 1, 0, 0}, Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[]{3, 0, -5, 0}, Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[]{2, 0, 0, -5}, Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[]{0, 2, -5, 0}, Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[]{0, 3, 0, -5}, Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[]{3, 2, 0, 0}, Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[]{2, 3, 0, 0}, Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5  P.S. I used the latest software from the repository (including MATH-286 fix).
MATH-b01fcc31$$NullPointerException in SimplexTableau.initialize$$SimplexTableau throws a NullPointerException when no solution can be found instead of a NoFeasibleSolutionException  Here is the code that causes the NullPointerException:  LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 1, 5 }, 0 ); Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint>(); constraints.add(new LinearConstraint(new double[] { 2, 0 }, Relationship.GEQ, -1.0));  RealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MINIMIZE, true);  Note: Tested both with Apache Commons Math 2.0 release and SVN trunk
MATH-59a0da9c$$Matrix's "OutOfBoundException" in SimplexSolver$$Hi all, This bug is somehow related to incident MATH-286, but not necessarily...  Let's say I have an LP and I solve it using SimplexSolver. Then I create a second LP similar to the first one, but with "stronger" constraints. The second LP has the following properties: * the only point in the feasible region for the second LP is the solution returned for the first LP * the solution returned for the first LP is also the (only possible) solution to the second LP  This shows the problem:  {code:borderStyle=solid} LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 0.8, 0.2, 0.7, 0.3, 0.4, 0.6}, 0 ); Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint>(); constraints.add(new LinearConstraint(new double[] { 1, 0, 1, 0, 1, 0 }, Relationship.EQ, 30.0)); constraints.add(new LinearConstraint(new double[] { 0, 1, 0, 1, 0, 1 }, Relationship.EQ, 30.0)); constraints.add(new LinearConstraint(new double[] { 0.8, 0.2, 0.0, 0.0, 0.0, 0.0 }, Relationship.GEQ, 10.0)); constraints.add(new LinearConstraint(new double[] { 0.0, 0.0, 0.7, 0.3, 0.0, 0.0 }, Relationship.GEQ, 10.0)); constraints.add(new LinearConstraint(new double[] { 0.0, 0.0, 0.0, 0.0, 0.4, 0.6 }, Relationship.GEQ, 10.0));  RealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true);  double valA = 0.8 * solution.getPoint()[0] + 0.2 * solution.getPoint()[1]; double valB = 0.7 * solution.getPoint()[2] + 0.3 * solution.getPoint()[3]; double valC = 0.4 * solution.getPoint()[4] + 0.6 * solution.getPoint()[5];  f = new LinearObjectiveFunction(new double[] { 0.8, 0.2, 0.7, 0.3, 0.4, 0.6}, 0 ); constraints = new ArrayList<LinearConstraint>(); constraints.add(new LinearConstraint(new double[] { 1, 0, 1, 0, 1, 0 }, Relationship.EQ, 30.0)); constraints.add(new LinearConstraint(new double[] { 0, 1, 0, 1, 0, 1 }, Relationship.EQ, 30.0)); constraints.add(new LinearConstraint(new double[] { 0.8, 0.2, 0.0, 0.0, 0.0, 0.0 }, Relationship.GEQ, valA)); constraints.add(new LinearConstraint(new double[] { 0.0, 0.0, 0.7, 0.3, 0.0, 0.0 }, Relationship.GEQ, valB)); constraints.add(new LinearConstraint(new double[] { 0.0, 0.0, 0.0, 0.0, 0.4, 0.6 }, Relationship.GEQ, valC));  solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true); {code}   Instead of returning the solution, SimplexSolver throws an Exception:  {noformat} Exception in thread "main" org.apache.commons.math.linear.MatrixIndexException: no entry at indices (0, 7) in a 6x7 matrix 	at org.apache.commons.math.linear.Array2DRowRealMatrix.getEntry(Array2DRowRealMatrix.java:356) 	at org.apache.commons.math.optimization.linear.SimplexTableau.getEntry(SimplexTableau.java:408) 	at org.apache.commons.math.optimization.linear.SimplexTableau.getBasicRow(SimplexTableau.java:258) 	at org.apache.commons.math.optimization.linear.SimplexTableau.getSolution(SimplexTableau.java:336) 	at org.apache.commons.math.optimization.linear.SimplexSolver.doOptimize(SimplexSolver.java:182) 	at org.apache.commons.math.optimization.linear.AbstractLinearOptimizer.optimize(AbstractLinearOptimizer.java:106){noformat}   I was too optimistic with the bug MATH-286 ;-)
MATH-2c8a114f$$RandomDataImpl.nextPoisson fails for means in range 6.0 - 19.99$$math.random.RandomDataImpl.nextPoisson(double mean) fails frequently (but not always) for values of mean between 6.0 and 19.99 inclusive. For values below 6.0 (where I see there is a branch in the logic) and above 20.0 it seems to be okay (though I've only randomly sampled the space and run a million trials for the values I've tried)  When it fails, the exception is as follows (this for a mean of 6.0)  org.apache.commons.math.MathRuntimeException 4: must have n >= 0 for n!, got n = -2 	at org.apache.commons.math.MathRuntimeException.createIllegalArgumentException(MathRuntimeException.java:282) 	at org.apache.commons.math.util.MathUtils.factorialLog(MathUtils.java:561) 	at org.apache.commons.math.random.RandomDataImpl.nextPoisson(RandomDataImpl.java:434)   ie MathUtils.factorialLog is being called with a negative input  To reproduce:      JDKRandomGenerator random = new JDKRandomGenerator();     random.setSeed(123456);     RandomData randomData = new RandomDataImpl(random);      for (int i=0; i< 1000000; i++){         randomData.nextPoisson(6.0);     }
MATH-ef9b639a$$NPE in  KMeansPlusPlusClusterer unittest$$When running this unittest, I am facing this NPE: java.lang.NullPointerException 	at org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer.assignPointsToClusters(KMeansPlusPlusClusterer.java:91)  This is the unittest:   package org.fao.fisheries.chronicles.calcuation.cluster;  import static org.junit.Assert.assertEquals; import static org.junit.Assert.assertTrue;  import java.util.Arrays; import java.util.List; import java.util.Random;  import org.apache.commons.math.stat.clustering.Cluster; import org.apache.commons.math.stat.clustering.EuclideanIntegerPoint; import org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer; import org.fao.fisheries.chronicles.input.CsvImportProcess; import org.fao.fisheries.chronicles.input.Top200Csv; import org.junit.Test;  public class ClusterAnalysisTest {   	@Test 	public void testPerformClusterAnalysis2() { 		KMeansPlusPlusClusterer<EuclideanIntegerPoint> transformer = new KMeansPlusPlusClusterer<EuclideanIntegerPoint>( 				new Random(1746432956321l)); 		EuclideanIntegerPoint[] points = new EuclideanIntegerPoint[] { 				new EuclideanIntegerPoint(new int[] { 1959, 325100 }), 				new EuclideanIntegerPoint(new int[] { 1960, 373200 }), }; 		List<Cluster<EuclideanIntegerPoint>> clusters = transformer.cluster(Arrays.asList(points), 1, 1); 		assertEquals(1, clusters.size());  	}  }
MATH-0596e314$$nextExponential parameter check bug - patch supplied$$Index: src/main/java/org/apache/commons/math/random/RandomDataImpl.java =================================================================== --- src/main/java/org/apache/commons/math/random/RandomDataImpl.java	(revision 830102) +++ src/main/java/org/apache/commons/math/random/RandomDataImpl.java	(working copy) @@ -462,7 +462,7 @@       * @return the random Exponential value       */      public double nextExponential(double mean) { -        if (mean < 0.0) { +        if (mean <= 0.0) {              throw MathRuntimeException.createIllegalArgumentException(                    "mean must be positive ({0})", mean);          }
MATH-83f18d52$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0 {code}     public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      } {code}
MATH-b2f3f6db$$NaN singular value from SVD$$The following jython code Start code  from org.apache.commons.math.linear import *   Alist = [[1.0, 2.0, 3.0],[2.0,3.0,4.0],[3.0,5.0,7.0]]   A = Array2DRowRealMatrix(Alist)   decomp = SingularValueDecompositionImpl(A)   print decomp.getSingularValues()  End code  prints array('d', [11.218599757513008, 0.3781791648535976, nan]) The last singular value should be something very close to 0 since the matrix is rank deficient.  When i use the result from getSolver() to solve a system, i end  up with a bunch of NaNs in the solution.  I assumed i would get back a least squares solution.  Does this SVD implementation require that the matrix be full rank?  If so, then i would expect an exception to be thrown from the constructor or one of the methods.
MATH-c06cc933$$NaN singular value from SVD$$The following jython code Start code  from org.apache.commons.math.linear import *   Alist = [[1.0, 2.0, 3.0],[2.0,3.0,4.0],[3.0,5.0,7.0]]   A = Array2DRowRealMatrix(Alist)   decomp = SingularValueDecompositionImpl(A)   print decomp.getSingularValues()  End code  prints array('d', [11.218599757513008, 0.3781791648535976, nan]) The last singular value should be something very close to 0 since the matrix is rank deficient.  When i use the result from getSolver() to solve a system, i end  up with a bunch of NaNs in the solution.  I assumed i would get back a least squares solution.  Does this SVD implementation require that the matrix be full rank?  If so, then i would expect an exception to be thrown from the constructor or one of the methods.
MATH-ce185345$$getLInfNorm() uses wrong formula in both ArrayRealVector and OpenMapRealVector (in different ways)$$the L_infinity norm of a finite dimensional vector is just the max of the absolute value of its entries.  The current implementation in ArrayRealVector has a typo:  {code}     public double getLInfNorm() {         double max = 0;         for (double a : data) {             max += Math.max(max, Math.abs(a));         }         return max;     } {code}  the += should just be an =.  There is sadly a unit test assuring us that this is the correct behavior (effectively a regression-only test, not a test for correctness).  Worse, the implementation in OpenMapRealVector is not even positive semi-definite:  {code}        public double getLInfNorm() {         double max = 0;         Iterator iter = entries.iterator();         while (iter.hasNext()) {             iter.advance();             max += iter.value();         }         return max;     } {code}  I would suggest that this method be moved up to the AbstractRealVector superclass and implemented using the sparseIterator():  {code}   public double getLInfNorm() {     double norm = 0;     Iterator<Entry> it = sparseIterator();     Entry e;     while(it.hasNext() && (e = it.next()) != null) {       norm = Math.max(norm, Math.abs(e.getValue()));     }     return norm;   } {code}  Unit tests with negative valued vectors would be helpful to check for this kind of thing in the future.
MATH-262fe4c0$$Maximal number of iterations (540) exceeded$$I have a matrix of size 49x19 and when I apply SVD on this matrix it raises the following exception. The problem which I am facing is that SVD works for some matrix and doesn't work for others. I have no clue what is the possible reason.  Exception:: CorrespondenceAnalysis: org.apache.commons.math.MaxIterationsExceededException: Maximal number of iterations (540) exceeded  [org.apache.commons.math.linear.EigenDecompositionImpl.processGeneralBlock(EigenDecompositionImpl.java:881), org.apache.commons.math.linear.EigenDecompositionImpl.findEigenvalues(EigenDecompositionImpl.java:651), org.apache.commons.math.linear.EigenDecompositionImpl.decompose(EigenDecompositionImpl.java:243), org.apache.commons.math.linear.EigenDecompositionImpl.<init>(EigenDecompositionImpl.java:202), org.apache.commons.math.linear.SingularValueDecompositionImpl.<init>(SingularValueDecompositionImpl.java:114),   RealMatrix m = [[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573]]  RealMatrix rcp = MatrixUtils.createRealMatrix(CP);	 SingularValueDecomposition svd = new SingularValueDecompositionImpl(rcp);		  RealMatrix U = svd.getU(); RealMatrix S = svd.getS(); RealMatrix Vt = svd.getVT(); double[] singularValues = svd.getSingularValues();
MATH-6dd3724b$$In stat.Frequency, getPct(Object) uses getCumPct(Comparable) instead of getPct(Comparable)$$Drop in Replacement of 1.2 with 2.0 not possible because all getPct calls will be cummulative without code change  Frequency.java     /**       * Returns the percentage of values that are equal to v      * @deprecated replaced by {@link #getPct(Comparable)} as of 2.0      */     @Deprecated     public double getPct(Object v) {         return getCumPct((Comparable<?>) v);     }
MATH-8dd22390$$Wrong parameter for first step size guess for Embedded Runge Kutta methods$$In a space application using DOP853 i detected what seems to be a bad parameter in the call to the method  initializeStep of class AdaptiveStepsizeIntegrator.  Here, DormandPrince853Integrator is a subclass for EmbeddedRungeKuttaIntegrator which perform the call to initializeStep at the beginning of its method integrate(...)  The problem comes from the array "scale" that is used as a parameter in the call off initializeStep(..)  Following the theory described by Hairer in his book "Solving Ordinary Differential Equations 1 : Nonstiff Problems", the scaling should be :  sci = Atol i + |y0i| * Rtoli  Whereas EmbeddedRungeKuttaIntegrator uses :  sci = Atoli  Note that the Gragg-Bulirsch-Stoer integrator uses the good implementation "sci = Atol i + |y0i| * Rtoli  " when he performs the call to the same method initializeStep(..)  In the method initializeStep, the error leads to a wrong step size h used to perform an  Euler step. Most of the time it is unvisible for the user. But in my space application the Euler step with this wrong step size h (much bigger than it should be)  makes an exception occur (my satellite hits the ground...)   To fix the bug, one should use the same algorithm as in the rescale method in GraggBulirschStoerIntegrator For exemple :   final double[] scale= new double[y0.length];;                      if (vecAbsoluteTolerance == null) {               for (int i = 0; i < scale.length; ++i) {                 final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));                 scale[i] = scalAbsoluteTolerance + scalRelativeTolerance * yi;               }             } else {               for (int i = 0; i < scale.length; ++i) {                 final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));                 scale[i] = vecAbsoluteTolerance[i] + vecRelativeTolerance[i] * yi;               }             }                      hNew = initializeStep(equations, forward, getOrder(), scale,                            stepStart, y, yDotK[0], yTmp, yDotK[1]);    Sorry for the length of this message, looking forward to hearing from you soon  Vincent Morand
MATH-f6dd42b4$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.
MATH-a0b4b4b7$$Brent solver returns the wrong value if either bracket endpoint is root$$The solve(final UnivariateRealFunction f, final double min, final double max, final double initial) function returns yMin or yMax if min or max are deemed to be roots, respectively, instead of min or max.
MATH-4cc9a49d$$Dangerous code in "PoissonDistributionImpl"$$In the following excerpt from class "PoissonDistributionImpl":  {code:title=PoissonDistributionImpl.java|borderStyle=solid}     public PoissonDistributionImpl(double p, NormalDistribution z) {         super();         setNormal(z);         setMean(p);     } {code}  (1) Overridable methods are called within the constructor. (2) The reference "z" is stored and modified within the class.  I've encountered problem (1) in several classes while working on issue 348. In those cases, in order to remove potential problems, I copied/pasted the body of the "setter" methods inside the constructor but I think that a more elegant solution would be to remove the "setters" altogether (i.e. make the classes immutable). Problem (2) can also create unexpected behaviour. Is it really necessary to pass the "NormalDistribution" object; can't it be always created within the class?
MATH-061f5017$$ODE integrator goes past specified end of integration range$$End of integration range in ODE solving is handled as an event. In some cases, numerical accuracy in events detection leads to error in events location. The following test case shows the end event is not handled properly and an integration that should cover a 60s range in fact covers a 160s range, more than twice the specified range. {code}   public void testMissedEvent() throws IntegratorException, DerivativeException {           final double t0 = 1878250320.0000029;           final double t =  1878250379.9999986;           FirstOrderDifferentialEquations ode = new FirstOrderDifferentialEquations() {                          public int getDimension() {                 return 1;             }                          public void computeDerivatives(double t, double[] y, double[] yDot)                 throws DerivativeException {                 yDot[0] = y[0] * 1.0e-6;             }         };          DormandPrince853Integrator integrator = new DormandPrince853Integrator(0.0, 100.0,                                                                                1.0e-10, 1.0e-10);          double[] y = { 1.0 };         integrator.setInitialStepSize(60.0);         double finalT = integrator.integrate(ode, t0, y, t, y);         Assert.assertEquals(t, finalT, 1.0e-6);     }  {code}
MATH-3a15d8ce$$AbstractRealVector.sparseIterator fails when vector has exactly one non-zero entry$$The following program: === import java.util.Iterator; import org.apache.commons.math.linear.*;  public class SparseIteratorTester {     public static void main(String[] args) {         double vdata[] = { 0.0, 1.0, 0.0 };         RealVector v = new ArrayRealVector(vdata);         Iterator<RealVector.Entry> iter = v.sparseIterator();         while(iter.hasNext()) {             RealVector.Entry entry = iter.next();             System.out.printf("%d: %f\n", entry.getIndex(), entry.getValue());         }        }        }  === generates this output:  1: 1.000000 Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: -1 	at org.apache.commons.math.linear.ArrayRealVector.getEntry(ArrayRealVector.java:995) 	at org.apache.commons.math.linear.AbstractRealVector EntryImpl.getValue(AbstractRealVector.java:850) 	at test.SparseIteratorTester.main(SparseIteratorTester.java:13) ===  This patch fixes it, and simplifies AbstractRealVector.SparseEntryIterator  (sorry, i don't see any form entry for attaching a file) === Index: src/main/java/org/apache/commons/math/linear/AbstractRealVector.java =================================================================== --- src/main/java/org/apache/commons/math/linear/AbstractRealVector.java	(revision 936985) +++ src/main/java/org/apache/commons/math/linear/AbstractRealVector.java	(working copy) @@ -18,6 +18,7 @@  package org.apache.commons.math.linear;    import java.util.Iterator; +import java.util.NoSuchElementException;    import org.apache.commons.math.FunctionEvaluationException;  import org.apache.commons.math.MathRuntimeException; @@ -875,40 +876,25 @@          /** Dimension of the vector. */          private final int dim;   -        /** Temporary entry (reused on each call to {@link #next()}. */ -        private EntryImpl tmp = new EntryImpl(); - -        /** Current entry. */ +        /** Last entry returned by #next(). */          private EntryImpl current;   -        /** Next entry. */ +        /** Next entry for #next() to return. */          private EntryImpl next;            /** Simple constructor. */          protected SparseEntryIterator() {              dim = getDimension();              current = new EntryImpl(); -            if (current.getValue() == 0) { -                advance(current); -            } -            if(current.getIndex() >= 0){ -                // There is at least one non-zero entry -                next = new EntryImpl(); -                next.setIndex(current.getIndex()); +            next = new EntryImpl(); +            if(next.getValue() == 0)                  advance(next); -            } else { -                // The vector consists of only zero entries, so deny having a next -                current = null; -            }          }   -        /** Advance an entry up to the next non null one. +        /** Advance an entry up to the next nonzero value.           * @param e entry to advance           */          protected void advance(EntryImpl e) { -            if (e == null) { -                return; -            }              do {                  e.setIndex(e.getIndex() + 1);              } while (e.getIndex() < dim && e.getValue() == 0); @@ -919,22 +905,17 @@            /** {@inheritDoc} */          public boolean hasNext() { -            return current != null; +            return next.getIndex() >= 0;          }            /** {@inheritDoc} */          public Entry next() { -            tmp.setIndex(current.getIndex()); -            if (next != null) { -                current.setIndex(next.getIndex()); -                advance(next); -                if (next.getIndex() < 0) { -                    next = null; -                } -            } else { -                current = null; -            } -            return tmp; +            int index = next.getIndex(); +            if(index < 0) +                throw new NoSuchElementException(); +            current.setIndex(index); +            advance(next); +            return current;          }            /** {@inheritDoc} */
MATH-f4a4464b$$BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException$$Method       BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)    invokes       BisectionSolver.solve(double min, double max)   which throws NullPointerException, as member variable      UnivariateRealSolverImpl.f   is null.  Instead the method:      BisectionSolver.solve(final UnivariateRealFunction f, double min, double max)  should be called.  Steps to reproduce:  invoke:       new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5);  NullPointerException will be thrown.
MATH-495f04bc$$NaN in "equals" methods$$In "MathUtils", some "equals" methods will return true if both argument are NaN. Unless I'm mistaken, this contradicts the IEEE standard.  If nobody objects, I'm going to make the changes.
MATH-bb005b56$$PearsonsCorrelation.getCorrelationPValues() precision limited by machine epsilon$$Similar to the issue described in MATH-201, using PearsonsCorrelation.getCorrelationPValues() with many treatments results in p-values that are continuous down to 2.2e-16 but that drop to 0 after that.  In MATH-201, the problem was described as such: > So in essence, the p-value returned by TTestImpl.tTest() is: >  > 1.0 - (cumulativeProbability(t) - cumulativeProbabily(-t)) >  > For large-ish t-statistics, cumulativeProbabilty(-t) can get quite small, and cumulativeProbabilty(t) can get very close to 1.0. When  > cumulativeProbability(-t) is less than the machine epsilon, we get p-values equal to zero because: >  > 1.0 - 1.0 + 0.0 = 0.0  The solution in MATH-201 was to modify the p-value calculation to this: > p = 2.0 * cumulativeProbability(-t)  Here, the problem is similar.  From PearsonsCorrelation.getCorrelationPValues():   p = 2 * (1 - tDistribution.cumulativeProbability(t));  Directly calculating the p-value using identical code as PearsonsCorrelation.getCorrelationPValues(), but with the following change seems to solve the problem:   p = 2 * (tDistribution.cumulativeProbability(-t));
MATH-bfe4623c$$StatUtils.sum returns NaN for zero-length arrays$$StatUtils.sum returns NaN for zero-length arrays, which is:  1. inconsistent with the mathematical notion of sum: in maths, sum_{i=0}^{N-1} a_i will be 0 for N=0. In particular, the identity  sum_{i=0}^{k-1} a_i + sum_{i=k}^{N-1} = sum_{i=0}^{N-1}  is broken for k = 0, since NaN + x = NaN, not x.  2. introduces hard to debug erros (returning a NaN is one of the worst forms of reporting an exceptional condition, as NaNs propagate silently and require manual tracing during the debugging)  3. enforces "special case" handling when the user expects that the summed array can have a zero length.  The correct behaviour is, in my opinion, to return 0.0, not NaN in the above case.
MATH-c640932d$$weight versus sigma in AbstractLeastSquares$$In AbstractLeastSquares, residualsWeights contains the WEIGHTS assigned to each observation.  In the method getRMS(), these weights are multiplicative as they should. unlike in getChiSquare() where it appears at the denominator!   If the weight is really the weight of the observation, it should multiply the square of the residual even in the computation of the chi2.   Once corrected, getRMS() can even reduce   public double getRMS() {return Math.sqrt(getChiSquare()/rows);}
MATH-d4b02f6a$$Method "getResult()" in "MultiStartUnivariateRealOptimizer"$$In "MultiStartUnivariateRealOptimizer" (package "optimization"), the method "getResult" returns the result of the last run of the "underlying" optimizer; this last result might not be the best one, in which case it will not correspond to the value returned by the "optimize" method. This is confusing and does not seem very useful. I think that "getResult" should be defined as {code}  public double getResult() {     return optima[0]; } {code} and similarly {code} public double getFunctionValue() {     return optimaValues[0]; } {code}
MATH-962315ba$$Bugs in "BrentOptimizer"$$I apologize for having provided a buggy implementation of Brent's optimization algorithm (class "BrentOptimizer" in package "optimization.univariate"). The unit tests didn't show that there was something wrong, although (from the "changes.xml" file) I discovered that, at the time, Luc had noticed something weird in the implementation's behaviour. Comparing with an implementation in Python, I could figure out the fixes. I'll modify "BrentOptimizer" and add a test. I also propose to change the name of the unit test class from "BrentMinimizerTest" to "BrentOptimizerTest".
MATH-784e4f69$$Inconsistent result from Levenberg-Marquardt$$Levenberg-Marquardt (its method doOptimize) returns a VectorialPointValuePair.  However, the class holds the optimum point, the vector of the objective function, the cost and residuals.  The value returns by doOptimize does not always corresponds to the point which leads to the residuals and cost
MATH-51aa6e6c$$Miscellaneous issues concerning the "optimization" package$$Revision 990792 contains changes triggered the following issues: * [MATH-394|https://issues.apache.org/jira/browse/MATH-394] * [MATH-397|https://issues.apache.org/jira/browse/MATH-397] * [MATH-404|https://issues.apache.org/jira/browse/MATH-404]  This issue collects the currently still unsatisfactory code (not necessarily sorted in order of annoyance): # "BrentOptimizer": a specific convergence checker must be used. "LevenbergMarquardtOptimizer" also has specific convergence checks. # Trying to make convergence checking independent of the optimization algorithm creates problems (conceptual and practical):  ** See "BrentOptimizer" and "LevenbergMarquardtOptimizer", the algorithm passes "points" to the convergence checker, but the actual meaning of the points can very well be different in the caller (optimization algorithm) and the callee (convergence checker).  ** In "PowellOptimizer" the line search ("BrentOptimizer") tolerances depend on the tolerances within the main algorithm. Since tolerances come with "ConvergenceChecker" and so can be changed at any time, it is awkward to adapt the values within the line search optimizer without exposing its internals ("BrentOptimizer" field) to the enclosing class ("PowellOptimizer"). # Given the numerous changes, some Javadoc comments might be out-of-sync, although I did try to update them all. # Class "DirectSearchOptimizer" (in package "optimization.direct") inherits from class "AbstractScalarOptimizer" (in package "optimization.general"). # Some interfaces are defined in package "optimization" but their base implementations (abstract class that contain the boiler-plate code) are in package "optimization.general" (e.g. "DifferentiableMultivariateVectorialOptimizer" and "BaseAbstractVectorialOptimizer"). # No check is performed to ensure the the convergence checker has been set (see e.g. "BrentOptimizer" and "PowellOptimizer"); if it hasn't there will be a NPE. The alternative is to initialize a default checker that will never be used in case the user had intended to explicitly sets the checker. # "NonLinearConjugateGradientOptimizer": Ugly workaround for the checked "ConvergenceException". # Everywhere, we trail the checked "FunctionEvaluationException" although it is never used. # There remains some duplicate code (such as the "multi-start loop" in the various "MultiStart..." implementations). # The "ConvergenceChecker" interface is very general (the "converged" method can take any number of "...PointValuePair"). However there remains a "semantic" problem: One cannot be sure that the list of points means the same thing for the caller of "converged" and within the implementation of the "ConvergenceChecker" that was independently set. # It is not clear whether it is wise to aggregate the counter of gradient evaluations to the function evaluation counter. In "LevenbergMarquartdOptimizer" for example, it would be unfair to do so. Currently I had to remove all tests referring to gradient and Jacobian evaluations. # In "AbstractLeastSquaresOptimizer" and "LevenbergMarquardtOptimizer", occurences of "OptimizationException" were replaced by the unchecked "ConvergenceException" but in some cases it might not be the most appropriate one. # "MultiStartUnivariateRealOptimizer": in the other classes ("MultiStartMultivariate...") similar to this one, the randomization is on the firts-guess value while in this class, it is on the search interval. I think that here also we should randomly choose the start value (within the user-selected interval). # The Javadoc utility raises warnings (see output of "mvn site") which I couldn't figure out how to correct. # Some previously existing classes and interfaces have become no more than a specialisation of new "generics" classes; it might be interesting to remove them in order to reduce the number of classes and thus limit the potential for confusion.
MATH-5fe9b36c$$ConvergenceException in NormalDistributionImpl.cumulativeProbability()$$I get a ConvergenceException in  NormalDistributionImpl.cumulativeProbability() for very large/small parameters including Infinity, -Infinity. For instance in the following code:  	@Test 	public void testCumulative() { 		final NormalDistribution nd = new NormalDistributionImpl(); 		for (int i = 0; i < 500; i++) { 			final double val = Math.exp(i); 			try { 				System.out.println("val = " + val + " cumulative = " + nd.cumulativeProbability(val)); 			} catch (MathException e) { 				e.printStackTrace(); 				fail(); 			} 		} 	}  In version 2.0, I get no exception.   My suggestion is to change in the implementation of cumulativeProbability(double) to catch all ConvergenceException (and return for very large and very small values), not just MaxIterationsExceededException.
MATH-133cbc2d$$SimplexSolver returns unfeasible solution$$The SimplexSolver is returning an unfeasible solution:  import java.util.ArrayList; import java.text.DecimalFormat; import org.apache.commons.math.linear.ArrayRealVector; import org.apache.commons.math.optimization.GoalType; import org.apache.commons.math.optimization.OptimizationException; import org.apache.commons.math.optimization.linear.*;  public class SimplexSolverBug {          public static void main(String[] args) throws OptimizationException {                  LinearObjectiveFunction c = new LinearObjectiveFunction(new double[]{0.0d, 1.0d, 1.0d, 0.0d, 0.0d, 0.0d, 0.0d}, 0.0d);                  ArrayList<LinearConstraint> cnsts = new ArrayList<LinearConstraint>(5);         LinearConstraint cnst;         cnst = new LinearConstraint(new double[] {1.0d, -0.1d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d}, Relationship.EQ, -0.1d);         cnsts.add(cnst);         cnst = new LinearConstraint(new double[] {1.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d}, Relationship.GEQ, -1e-18d);         cnsts.add(cnst);         cnst = new LinearConstraint(new double[] {0.0d, 1.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d}, Relationship.GEQ, 0.0d);         cnsts.add(cnst);         cnst = new LinearConstraint(new double[] {0.0d, 0.0d, 0.0d, 1.0d, 0.0d, -0.0128588d, 1e-5d}, Relationship.EQ, 0.0d);         cnsts.add(cnst);         cnst = new LinearConstraint(new double[] {0.0d, 0.0d, 0.0d, 0.0d, 1.0d, 1e-5d, -0.0128586d}, Relationship.EQ, 1e-10d);         cnsts.add(cnst);         cnst = new LinearConstraint(new double[] {0.0d, 0.0d, 1.0d, -1.0d, 0.0d, 0.0d, 0.0d}, Relationship.GEQ, 0.0d);         cnsts.add(cnst);         cnst = new LinearConstraint(new double[] {0.0d, 0.0d, 1.0d, 1.0d, 0.0d, 0.0d, 0.0d}, Relationship.GEQ, 0.0d);         cnsts.add(cnst);         cnst = new LinearConstraint(new double[] {0.0d, 0.0d, 1.0d, 0.0d, -1.0d, 0.0d, 0.0d}, Relationship.GEQ, 0.0d);         cnsts.add(cnst);         cnst = new LinearConstraint(new double[] {0.0d, 0.0d, 1.0d, 0.0d, 1.0d, 0.0d, 0.0d}, Relationship.GEQ, 0.0d);         cnsts.add(cnst);                          DecimalFormat df = new java.text.DecimalFormat("0.#####E0");                  System.out.println("Constraints:");         for(LinearConstraint con : cnsts) {             for (int i = 0; i < con.getCoefficients().getDimension(); ++i)                 System.out.print(df.format(con.getCoefficients().getData()[i]) + " ");             System.out.println(con.getRelationship() + " " + con.getValue());         }                  SimplexSolver simplex = new SimplexSolver(1e-7);         double[] sol = simplex.optimize(c, cnsts, GoalType.MINIMIZE, false).getPointRef();         System.out.println("Solution:\n" + new ArrayRealVector(sol));         System.out.println("Second constraint is violated!");     } }   It's an odd problem, but something I ran across.  I tracked the problem to the getPivotRow routine in SimplexSolver.  It was choosing a pivot that resulted in a negative right-hand-side.  I recommend a fix by replacing                 ...                 if (MathUtils.equals(ratio, minRatio, epsilon)) {                 ... with                 ...                 if (MathUtils.equals(ratio, minRatio, Math.abs(epsilon/entry))) {                 ...  I believe this would be more appropriate (and at least resolves this particular problem).  Also, you may want to consider making a change in getPivotColumn to replace             ...             if (MathUtils.compareTo(tableau.getEntry(0, i), minValue, epsilon) < 0) {             ... with             ...             if (tableau.getEntry(0, i) < minValue)              ... because I don't see the point of biasing earlier columns when multiple entries are within epsilon of each other.  Why not pick the absolute smallest.  I don't know that any problem can result from doing it the other way, but the latter may be a safer bet.  VERY IMPORTANT: I discovered another bug that occurs when not restricting to non-negatives.  In SimplexTableu::getSolution(),            ...                     if (basicRows.contains(basicRow))                // if multiple variables can take a given value               // then we choose the first and set the rest equal to 0               coefficients[i] = 0;           ... should be           ...                     if (basicRows.contains(basicRow)) {               // if multiple variables can take a given value               // then we choose the first and set the rest equal to 0               coefficients[i] = (restrictToNonNegative ? 0 : -mostNegative);           ... If necessary, I can give an example of where this bug causes a problem, but it should be fairly obvious why this was wrong.
MATH-a4b1948b$$MathUtils.equals(double, double) does not work properly for floats$$MathUtils.equals(double, double) does not work properly for floats.  There is no equals(float,float) so float parameters are automatically promoted to double. However, that is not necessarily appropriate, given that the ULP for a double is much smaller than the ULP for a float.  So for example:  {code} double oneDouble = 1.0d; assertTrue(MathUtils.equals(oneDouble, Double.longBitsToDouble(1 + Double.doubleToLongBits(oneDouble)))); // OK float oneFloat = 1.0f; assertTrue(MathUtils.equals(oneFloat, Float.intBitsToFloat(1 + Float.floatToIntBits(oneFloat)))); // FAILS float  f1 = 333.33334f; double d1 = 333.33334d; assertTrue(MathUtils.equals(d1, f1)); // FAILS {code}  I think the equals() methods need to be duplicated with the appropriate changes for floats to avoid any problems with the promotion of floats.
MATH-6d6649ef$$FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f$$FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f.  This is because the wrong variable is returned.  The bug was not detected by the test case "testMinMaxFloat()" because that has a bug too - it tests doubles, not floats.
MATH-26a61077$$GaussianFitter Unexpectedly Throws NotStrictlyPositiveException$$Running the following:      	double[] observations =      	{      			1.1143831578403364E-29,      			 4.95281403484594E-28,      			 1.1171347211930288E-26,      			 1.7044813962636277E-25,      			 1.9784716574832164E-24,      			 1.8630236407866774E-23,      			 1.4820532905097742E-22,      			 1.0241963854632831E-21,      			 6.275077366673128E-21,      			 3.461808994532493E-20,      			 1.7407124684715706E-19,      			 8.056687953553974E-19,      			 3.460193945992071E-18,      			 1.3883326374011525E-17,      			 5.233894983671116E-17,      			 1.8630791465263745E-16,      			 6.288759227922111E-16,      			 2.0204433920597856E-15,      			 6.198768938576155E-15,      			 1.821419346860626E-14,      			 5.139176445538471E-14,      			 1.3956427429045787E-13,      			 3.655705706448139E-13,      			 9.253753324779779E-13,      			 2.267636001476696E-12,      			 5.3880460095836855E-12,      			 1.2431632654852931E-11      	};        	GaussianFitter g =      		new GaussianFitter(new LevenbergMarquardtOptimizer());     	     	for (int index = 0; index < 27; index++)     	{     		g.addObservedPoint(index, observations[index]);     	}        	g.fit();  Results in:  org.apache.commons.math.exception.NotStrictlyPositiveException: -1.277 is smaller than, or equal to, the minimum (0) 	at org.apache.commons.math.analysis.function.Gaussian Parametric.validateParameters(Gaussian.java:184) 	at org.apache.commons.math.analysis.function.Gaussian Parametric.value(Gaussian.java:129)   I'm guessing the initial guess for sigma is off.
MATH-b6bf8f41$$Truncation issue in KMeansPlusPlusClusterer$$The for loop inside KMeansPlusPlusClusterer.chooseInitialClusters defines a variable   int sum = 0; This variable should have type double, rather than int.  Using an int causes the method to truncate the distances between points to (square roots of) integers.  It's especially bad when the distances between points are typically less than 1.  As an aside, in version 2.2, this bug manifested itself by making the clusterer return empty clusters.  I wonder if the EmptyClusterStrategy would still be necessary if this bug were fixed.
MATH-fbbb96eb$$Vector3D.crossProduct is sensitive to numerical cancellation$$Cross product implementation uses the naive formulas (y1 z2 - y2 z1, ...). These formulas fail when vectors are almost colinear, like in the following example: {code} Vector3D v1 = new Vector3D(9070467121.0, 4535233560.0, 1); Vector3D v2 = new Vector3D(9070467123.0, 4535233561.0, 1); System.out.println(Vector3D.crossProduct(v1, v2)); {code}  The previous code displays { -1, 2, 0 } instead of the correct answer { -1, 2, 1 }
MATH-328513f3$$MathUtils round method should propagate rather than wrap Runitme exceptions$$MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.
MATH-fc409e88$$Remove "assert" from "MathUtils.equals"$$The "assert" in methods "equals(double,double,int)" and "equals(float,float,int)" is not necessary.
MATH-2123f780$$Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same$$For both Complex add and subtract, the javadoc states that  {code}      * If either this or <code>rhs</code> has a NaN value in either part,      * {@link #NaN} is returned; otherwise Inifinite and NaN values are      * returned in the parts of the result according to the rules for      * {@link java.lang.Double} arithmetic {code}  Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).
MATH-334c01e6$$"RegulaFalsiSolver" failure$$The following unit test: {code} @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); } {code} fails with {noformat} illegal state: maximal count (100) exceeded: evaluations {noformat}  Using "PegasusSolver", the answer is found after 17 evaluations.
MATH-c0b49542$$"RegulaFalsiSolver" failure$$The following unit test: {code} @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); } {code} fails with {noformat} illegal state: maximal count (100) exceeded: evaluations {noformat}  Using "PegasusSolver", the answer is found after 17 evaluations.
MATH-ebc61de9$$"RegulaFalsiSolver" failure$$The following unit test: {code} @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); } {code} fails with {noformat} illegal state: maximal count (100) exceeded: evaluations {noformat}  Using "PegasusSolver", the answer is found after 17 evaluations.
MATH-8b418000$$numerical problems in rotation creation$$building a rotation from the following vector pairs leads to NaN: u1 = -4921140.837095533, -2.1512094250440013E7, -890093.279426377 u2 = -2.7238580938724895E9, -2.169664921341876E9, 6.749688708885301E10 v1 = 1, 0, 0 v2 = 0, 0, 1  The constructor first changes the (v1, v2) pair into (v1', v2') ensuring the following scalar products hold:  <v1'|v1'> == <u1|u1>  <v2'|v2'> == <u2|u2>  <u1 |u2>  == <v1'|v2'>  Once the (v1', v2') pair has been computed, we compute the cross product:   k = (v1' - u1)^(v2' - u2)  and the scalar product:   c = <k | (u1^u2)>  By construction, c is positive or null and the quaternion axis we want to build is q = k/[2*sqrt(c)]. c should be null only if some of the vectors are aligned, and this is dealt with later in the algorithm.  However, there are numerical problems with the vector above with the way these computations are done, as shown by the following comparisons, showing the result we get from our Java code and the result we get from manual computation with the same formulas but with enhanced precision:  commons math:   k = 38514476.5,            -84.,                           -1168590144 high precision: k = 38514410.36093388...,  -0.374075245201180409222711..., -1168590152.10599715208...  and it becomes worse when computing c because the vectors are almost orthogonal to each other, hence inducing additional cancellations. We get: commons math    c = -1.2397173627587605E20 high precision: c =  558382746168463196.7079627...  We have lost ALL significant digits in cancellations, and even the sign is wrong!
MATH-98556fed$$AbstractRandomGenerator nextInt() and nextLong() default implementations generate only positive values$$The javadoc for these methods (and what is specified in the RandomGenerator interface) says that all int / long values should be in the range of these methods.  The default implementations provided in this class do not generate negative values.
MATH-32b0f733$$Division by zero$$In class {{Complex}}, division by zero always returns NaN. I think that it should return NaN only when the numerator is also {{ZERO}}, otherwise the result should be {{INF}}. See [here|http://en.wikipedia.org/wiki/Riemann_sphere#Arithmetic_operations].
MATH-97b440fc$$Division by zero$$In class {{Complex}}, division by zero always returns NaN. I think that it should return NaN only when the numerator is also {{ZERO}}, otherwise the result should be {{INF}}. See [here|http://en.wikipedia.org/wiki/Riemann_sphere#Arithmetic_operations].
MATH-5e638976$$Integer overflow in OpenMapRealMatrix$$computeKey() has an integer overflow. Since it is a sparse matrix, this is quite easily encountered long before heap space is exhausted. The attached code demonstrates the problem, which could potentially be a security vulnerability (for example, if one was to use this matrix to store access control information).  Workaround: never create an OpenMapRealMatrix with more cells than are addressable with an int.
MATH-118f0cc0$$Statistics.setVarianceImpl makes getStandardDeviation produce NaN$$Invoking SummaryStatistics.setVarianceImpl(new Variance(true/false) makes getStandardDeviation produce NaN. The code to reproduce it:  {code:java} int[] scores = {1, 2, 3, 4}; SummaryStatistics stats = new SummaryStatistics(); stats.setVarianceImpl(new Variance(false)); //use "population variance" for(int i : scores) {   stats.addValue(i); } double sd = stats.getStandardDeviation(); System.out.println(sd); {code}  A workaround suggested by Mikkel is: {code:java}   double sd = FastMath.sqrt(stats.getSecondMoment() / stats.getN()); {code}
MATH-7980a242$$Incomplete reinitialization with some events handling$$I get a bug with event handling: I track 2 events that occur in the same step, when the first one is accepted, it resets the state but the reinitialization is not complete and the second one becomes unable to find its way. I can't give my context, which is rather large, but I tried a patch that works for me, unfortunately it breaks the unit tests.
MATH-b2e24119$$inverseCumulativeDistribution fails with cumulative distribution having a plateau$$This bug report follows MATH-692. The attached unit test fails. As required by the definition in MATH-692, the lower-bound of the interval on which the cdf is constant should be returned. This is not so at the moment.
MATH-3f645310$$One of Variance.evaluate() methods does not work correctly$$The method org.apache.commons.math.stat.descriptive.moment.Variance.evaluate(double[] values, double[] weights, double mean, int begin, int length) does not work properly. Looks loke it ignores the length parameter and grabs the whole dataset. Similar method in Mean class seems to work. I did not check other methods taking the part of the array; they may have the same problem.  Workaround: I had to shrink my arrays and use the method without the length.
MATH-645d642b$$DormandPrince853 integrator leads to revisiting of state events$$See the attached ReappearingEventTest.java. It has two unit tests, which use either the DormandPrince853 or the GraggBulirschStoer integrator, on the same ODE problem. It is a problem starting at time 6.0, with 7 variables, and 1 state event. The state event was previously detected at time 6.0, which is why I start there now. I provide and end time of 10.0. Since I start at the state event, I expect to integrate all the way to the end (10.0). For the GraggBulirschStoer this is what happens (see attached ReappearingEventTest.out). For the DormandPrince853Integerator, it detects a state event and stops integration at 6.000000000000002.  I think the problem becomes clear by looking at the output in ReappearingEventTest.out, in particular these lines:  {noformat} computeDerivatives: t=6.0                  y=[2.0                 , 2.0                 , 2.0                 , 4.0                 , 2.0                 , 7.0                 , 15.0                ] (...) g                 : t=6.0                  y=[1.9999999999999996  , 1.9999999999999996  , 1.9999999999999996  , 4.0                 , 1.9999999999999996  , 7.0                 , 14.999999999999998  ] (...) final result      : t=6.000000000000002    y=[2.0000000000000013  , 2.0000000000000013  , 2.0000000000000013  , 4.000000000000002   , 2.0000000000000013  , 7.000000000000002   , 15.0                ] {noformat}  The initial value of the last variable in y, the one that the state event refers to, is 15.0. However, the first time it is given to the g function, the value is 14.999999999999998. This value is less than 15, and more importantly, it is a value from the past (as all functions are increasing), *before* the state event. This makes that the state event re-appears immediately, and integration stops at 6.000000000000002 because of the detected state event.  I find it puzzling that for the DormandPrince853Integerator the y array that is given to the first evaluation of the g function, has different values than the y array that is the input to the problem. For GraggBulirschStoer is can be seen that the y arrays have identical values.
MATH-f656676e$$Negative value with restrictNonNegative$$Problem: commons-math-2.2 SimplexSolver.  A variable with 0 coefficient may be assigned a negative value nevertheless restrictToNonnegative flag in call: SimplexSolver.optimize(function, constraints, GoalType.MINIMIZE, true);  Function 1 * x + 1 * y + 0  Constraints: 1 * x + 0 * y = 1  Result: x = 1; y = -1;  Probably variables with 0 coefficients are omitted at some point of computation and because of that the restrictions do not affect their values.
MATH-faa77857$$BracketingNthOrderBrentSolver exceeds maxIterationCount while updating always the same boundary$$In some cases, the aging feature in BracketingNthOrderBrentSolver fails. It attempts to balance the bracketing points by targeting a non-zero value instead of the real root. However, the chosen target is too close too zero, and the inverse polynomial approximation is always on the same side, thus always updates the same bracket. In the real used case for a large program, I had a bracket point xA = 12500.0, yA = 3.7e-16, agingA = 0, which is the (really good) estimate of the zero on one side of the root and xB = 12500.03, yB = -7.0e-5, agingB = 97. This shows that the bracketing interval is completely unbalanced, and we never succeed to rebalance it as we always updates (xA, yA) and never updates (xB, yB).
MATH-3a08bfa6$$inverseCumulativeProbability of BinomialDistribution returns wrong value for large trials.$$The inverseCumulativeProbability method of the BinomialDistributionImpl class returns wrong value for large trials.  Following code will be reproduce the problem.  {{System.out.println(new BinomialDistributionImpl(1000000, 0.5).inverseCumulativeProbability(0.5));}}  This returns 499525, though it should be 499999.  I'm not sure how it should be fixed, but the cause is that the cumulativeProbability method returns Infinity, not NaN.  As the result the checkedCumulativeProbability method doesn't work as expected.
MATH-95d15eff$$[math] Complex Tanh for "big" numbers$$Hi,  In Complex.java the tanh is computed with the following formula:  tanh(a + bi) = sinh(2a)/(cosh(2a)+cos(2b)) + [sin(2b)/(cosh(2a)+cos(2b))]i  The problem that I'm finding is that as soon as "a" is a "big" number, both sinh(2a) and cosh(2a) are infinity and then the method tanh returns in the real part NaN (infinity/infinity) when it should return 1.0.  Wouldn't it be appropiate to add something as in the FastMath library??:  if (real>20.0){       return createComplex(1.0, 0.0); } if (real<-20.0){       return createComplex(-1.0, 0.0); }   Best regards,  JBB
MATH-1352a70f$$BitStreamGenerators (MersenneTwister, Well generators) do not clear normal deviate cache on setSeed$$The BitStream generators generate normal deviates (for nextGaussian) in pairs, caching the last value generated. When reseeded, the cache should be cleared; otherwise seeding two generators with the same value is not guaranteed to generate the same sequence.
MATH-9c8bb934$$RandomDataImpl.nextInt does not distribute uniformly for negative lower bound$$When using the RandomDataImpl.nextInt function to get a uniform sample in a [lower, upper] interval, when the lower value is less than zero, the output is not uniformly distributed, as the lowest value is practically never returned.  See the attached NextIntUniformTest.java file. It uses a [-3, 5] interval. For several values between 0 and 1, testNextIntUniform1 prints the return value of RandomDataImpl.nextInt (as double and as int). We see that -2 through 5 are returned several times. The -3 value however, is only returned for 0.0, and is thus under-respresented in the integer samples. The output of test method testNextIntUniform2 also clearly shows that value -3 is never sampled.
MATH-69273dca$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.
MATH-d2777388$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.
MATH-f64b6a90$$Incomplete beta function I(x, a, b) is inaccurate for large values of a and/or b$$This was first reported in MATH-718. The result of the current implementation of the incomplete beta function I(x, a, b) is inaccurate when a and/or b are large-ish.     I've skimmed through [slatec|http://www.netlib.org/slatec/fnlib/betai.f], GSL, [Boost|http://www.boost.org/doc/libs/1_38_0/libs/math/doc/sf_and_dist/html/math_toolkit/special/sf_beta/ibeta_function.html] as well as NR. At first sight, neither uses the same method to compute this function. I think [TOMS-708|http://www.netlib.org/toms/708] is probably the best option.    _Issue moved from MATH project on January 27, 2018 (concerned implementation was moved to module {{commons-numbers-gamma}} of "Commons Numbers")._
MATH-8a83581e$$BigFraction.doubleValue() returns Double.NaN for large numerators or denominators$$The current implementation of doubleValue() divides numerator.doubleValue() / denominator.doubleValue().  BigInteger.doubleValue() fails for any number greater than Double.MAX_VALUE.  So if the user has 308-digit numerator or denominator, the resulting quotient fails, even in cases where the result would be well inside Double's range.  I have a patch to fix it, if I can figure out how to attach it here I will.
MATH-76b7413d$$ResizableDoubleArray is not thread-safe yet has some synch. methods$$ResizableDoubleArray has several synchronised methods, but is not thread-safe, because class variables are not always accessed using the lock.  Is the class supposed to be thread-safe?  If so, all accesses (read and write) need to be synch.  If not, the synch. qualifiers could be dropped.  In any case, the protected fields need to be made private.
MATH-b9ca51f0$$Need range checks for elitismRate in ElitisticListPopulation constructors.$$There is a range check for setting the elitismRate via ElitisticListPopulation's setElitismRate method, but not via the constructors.
MATH-5b9302d5$$Dfp Dfp.multiply(int x) does not comply with the general contract FieldElement.multiply(int n)$$In class {{org.apache.commons.math3.Dfp}},  the method {{multiply(int n)}} is limited to {{0 <= n <= 9999}}. This is not consistent with the general contract of {{FieldElement.multiply(int n)}}, where there should be no limitation on the values of {{n}}.
MATH-ebadb558$$ListPopulation Iterator allows you to remove chromosomes from the population.$$Calling the iterator method of ListPopulation returns an iterator of the protected modifiable list. Before returning the iterator we should wrap it in an unmodifiable list.
MATH-dd6cefb0$$BSPTree class and recovery of a Euclidean 3D BRep$$New to the work here. Thanks for your efforts on this code.  I create a BSPTree from a BoundaryRep (Brep) my test Brep is a cube as represented by a float array containing 8 3D points in(x,y,z) order and an array of indices (12 triplets for the 12 faces of the cube). I construct a BSPMesh() as shown in the code below. I can construct the PolyhedronsSet() but have problems extracting the faces from the BSPTree to reconstruct the BRep. The attached code (BSPMesh2.java) shows that a small change to 1 of the vertex positions causes/corrects the problem.  Any ideas?
MATH-3c4cb189$$SimplexSolver gives bad results$$Methode SimplexSolver.optimeze(...) gives bad results with commons-math3-3.0 in a simple test problem. It works well in commons-math-2.2.
MATH-621bbb8f$$Correlated random vector generator fails (silently) when faced with zero rows in covariance matrix$$The following three matrices (which are basically permutations of each other) produce different results when sampling a multi-variate Gaussian with the help of CorrelatedRandomVectorGenerator (sample covariances calculated in R, based on 10,000 samples):  Array2DRowRealMatrix{ {0.0,0.0,0.0,0.0,0.0}, {0.0,0.013445532,0.01039469,0.009881156,0.010499559}, {0.0,0.01039469,0.023006616,0.008196856,0.010732709}, {0.0,0.009881156,0.008196856,0.019023866,0.009210099}, {0.0,0.010499559,0.010732709,0.009210099,0.019107243}}  > cov(data1)    V1 V2 V3 V4 V5 V1 0 0.000000000 0.00000000 0.000000000 0.000000000 V2 0 0.013383931 0.01034401 0.009913271 0.010506733 V3 0 0.010344006 0.02309479 0.008374730 0.010759306 V4 0 0.009913271 0.00837473 0.019005488 0.009187287 V5 0 0.010506733 0.01075931 0.009187287 0.019021483  Array2DRowRealMatrix{ {0.013445532,0.01039469,0.0,0.009881156,0.010499559}, {0.01039469,0.023006616,0.0,0.008196856,0.010732709}, {0.0,0.0,0.0,0.0,0.0}, {0.009881156,0.008196856,0.0,0.019023866,0.009210099}, {0.010499559,0.010732709,0.0,0.009210099,0.019107243}}  > cov(data2)             V1 V2 V3 V4 V5 V1 0.006922905 0.010507692 0 0.005817399 0.010330529 V2 0.010507692 0.023428918 0 0.008273152 0.010735568 V3 0.000000000 0.000000000 0 0.000000000 0.000000000 V4 0.005817399 0.008273152 0 0.004929843 0.009048759 V5 0.010330529 0.010735568 0 0.009048759 0.018683544   Array2DRowRealMatrix{ {0.013445532,0.01039469,0.009881156,0.010499559}, {0.01039469,0.023006616,0.008196856,0.010732709}, {0.009881156,0.008196856,0.019023866,0.009210099}, {0.010499559,0.010732709,0.009210099,0.019107243}}  > cov(data3)             V1          V2          V3          V4 V1 0.013445047 0.010478862 0.009955904 0.010529542 V2 0.010478862 0.022910522 0.008610113 0.011046353 V3 0.009955904 0.008610113 0.019250975 0.009464442 V4 0.010529542 0.011046353 0.009464442 0.019260317   I've traced this back to the RectangularCholeskyDecomposition, which does not seem to handle the second matrix very well (decompositions in the same order as the matrices above):  CorrelatedRandomVectorGenerator.getRootMatrix() =  Array2DRowRealMatrix{{0.0,0.0,0.0,0.0,0.0},{0.0759577418122063,0.0876125188474239,0.0,0.0,0.0},{0.07764443622513505,0.05132821221460752,0.11976381821791235,0.0,0.0},{0.06662930527909404,0.05501661744114585,0.0016662506519307997,0.10749324207653632,0.0},{0.13822895138139477,0.0,0.0,0.0,0.0}} CorrelatedRandomVectorGenerator.getRank() = 5  CorrelatedRandomVectorGenerator.getRootMatrix() =  Array2DRowRealMatrix{{0.0759577418122063,0.034512751379448724,0.0},{0.07764443622513505,0.13029949164628746,0.0},{0.0,0.0,0.0},{0.06662930527909404,0.023203936694855674,0.0},{0.13822895138139477,0.0,0.0}} CorrelatedRandomVectorGenerator.getRank() = 3  CorrelatedRandomVectorGenerator.getRootMatrix() =  Array2DRowRealMatrix{{0.0759577418122063,0.034512751379448724,0.033913748226348225,0.07303890149947785},{0.07764443622513505,0.13029949164628746,0.0,0.0},{0.06662930527909404,0.023203936694855674,0.11851573313229945,0.0},{0.13822895138139477,0.0,0.0,0.0}} CorrelatedRandomVectorGenerator.getRank() = 4  Clearly, the rank of each of these matrices should be 4. The first matrix does not lead to incorrect results, but the second one does. Unfortunately, I don't know enough about the Cholesky decomposition to find the flaw in the implementation, and I could not find documentation for the "rectangular" variant (also not at the links provided in the javadoc).
MATH-118e94b5$$Quaternion not normalized after construction$$The use of the Rotation(Vector3D u1,Vector3D u2,Vector3D v1,Vector3D v2) constructor with normalized angle can apparently lead to un-normalized quaternion. This case appeared to me with the following data : u1 = (0.9999988431610581, -0.0015210774290851095, 0.0) u2 = (0.0, 0.0, 1.0) and  v1 = (0.9999999999999999, 0.0, 0.0) v2 = (0.0, 0.0, -1.0)  This lead to the following quaternion : q0 = 225783.35177064248 q1 = 0.0 q2 = 0.0 q3 = -3.3684446110762543E-9  I was expecting to have a normalized quaternion, as input vector's are normalized. Does the quaternion shouldn't be normalized ? I've joined the corresponding piece of code as JUnit Test case
MATH-607c9ec6$$In RealVector, dotProduct and outerProduct return wrong results due to misuse of sparse iterators$$In class {{RealVector}}, the default implementation of {{RealMatrix outerProduct(RealVector)}} uses sparse iterators on the entries of the two vectors. The rationale behind this is that {{0d * x == 0d}} is {{true}} for all {{double x}}. This assumption is in fact false, since {{0d * NaN == NaN}}.  Proposed fix is to loop through *all* entries of both vectors. This can have a significant impact on the CPU cost, but robustness should probably be preferred over speed in default implementations.  Same issue occurs with {{double dotProduct(RealVector)}}, which uses sparse iterators for {{this}} only.  Another option would be to through an exception if {{isNaN()}} is {{true}}, in which case caching could be used for both {{isNaN()}} and {{isInfinite()}}.
MATH-6eb46555$$In RealVector, dotProduct and outerProduct return wrong results due to misuse of sparse iterators$$In class {{RealVector}}, the default implementation of {{RealMatrix outerProduct(RealVector)}} uses sparse iterators on the entries of the two vectors. The rationale behind this is that {{0d * x == 0d}} is {{true}} for all {{double x}}. This assumption is in fact false, since {{0d * NaN == NaN}}.  Proposed fix is to loop through *all* entries of both vectors. This can have a significant impact on the CPU cost, but robustness should probably be preferred over speed in default implementations.  Same issue occurs with {{double dotProduct(RealVector)}}, which uses sparse iterators for {{this}} only.  Another option would be to through an exception if {{isNaN()}} is {{true}}, in which case caching could be used for both {{isNaN()}} and {{isInfinite()}}.
MATH-a49e443c$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables.  In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result.  The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values.  What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem.  The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L = {l(1), l(2), ..., l(R)} (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR
MATH-63a48705$$Fraction percentageValue rare overflow$$The percentageValue() method of the Fraction class works by first multiplying the Fraction by 100, then converting the Fraction to a double. This causes overflows when the numerator is greater than Integer.MAX_VALUE/100, even when the value of the fraction is far below this value.  The patch changes the method to first convert to a double value, and then multiply this value by 100 - the result should be the same, but with less overflows. An addition to the test for the method that covers this bug is also included.
MATH-d7c0f27e$$Fraction(double, int) constructor strange behaviour$$The Fraction constructor Fraction(double, int) takes a double value and a int maximal denominator, and approximates a fraction. When the double value is a large, negative number with many digits in the fractional part, and the maximal denominator is a big, positive integer (in the 100'000s), two distinct bugs can manifest:  1: the constructor returns a positive Fraction. Calling Fraction(-33655.1677817278, 371880) returns the fraction 410517235/243036, which both has the wrong sign, and is far away from the absolute value of the given value  2: the constructor does not manage to reduce the Fraction properly. Calling Fraction(-43979.60679604749, 366081) returns the fraction -1651878166/256677, which should have* been reduced to -24654898/3831.  I have, as of yet, not found a solution. The constructor looks like this:  public Fraction(double value, int maxDenominator)         throws FractionConversionException     {        this(value, 0, maxDenominator, 100);     }  Increasing the 100 value (max iterations) does not fix the problem for all cases. Changing the 0-value (the epsilon, maximum allowed error) to something small does not work either, as this breaks the tests in FractionTest.   The problem is not neccissarily that the algorithm is unable to approximate a fraction correctly. A solution where a FractionConversionException had been thrown in each of these examples would probably be the best solution if an improvement on the approximation algorithm turns out to be hard to find.  This bug has been found when trying to explore the idea of axiom-based testing (http://bldl.ii.uib.no/testing.html). Attached is a java test class FractionTestByAxiom (junit, goes into org.apache.commons.math3.fraction) which shows these bugs through a simplified approach to this kind of testing, and a text file describing some of the value/maxDenominator combinations which causes one of these failures.  * It is never specified in the documentation that the Fraction class guarantees that completely reduced rational numbers are constructed, but a comment inside the equals method claims that "since fractions are always in lowest terms, numerators and can be compared directly for equality", so it seems like this is the intention.
MATH-7994d3ee$$"HarmonicFitter.ParameterGuesser" sometimes fails to return sensible values$$The inner class "ParameterGuesser" in "HarmonicFitter" (package "o.a.c.m.optimization.fitting") fails to compute a usable guess for the "amplitude" parameter.
MATH-ad252a8c$$EigenDecomposition fails for certain matrices$$The Schurtransformation of the following matrix fails, which is a preliminary step for the Eigendecomposition:  RealMatrix m = MatrixUtils.DEFAULT_FORMAT.parse("{{0.184944928,-0.0646971046,0.0774755812,-0.0969651755,-0.0692648806,0.3282344352,-0.0177423074,0.206313634},{-0.0742700134,-0.028906303,-0.001726946,-0.0375550146,-0.0487737922,-0.2616837868,-0.0821201295,-0.2530000167},{0.2549910127,0.0995733692,-0.0009718388,0.0149282808,0.1791878897,-0.0823182816,0.0582629256,0.3219545182},{-0.0694747557,-0.1880649148,-0.2740630911,0.0720096468,-0.1800836914,-0.3518996425,0.2486747833,0.6257938167},{0.0536360918,-0.1339297778,0.2241579764,-0.0195327484,-0.0054103808,0.0347564518,0.5120802482,-0.0329902864},{-0.5933332356,-0.2488721082,0.2357173629,0.0177285473,0.0856630593,-0.35671263,-0.1600668126,-0.1010899621},{-0.0514349819,-0.0854319435,0.1125050061,0.006345356,-0.2250000688,-0.220934309,0.1964623477,-0.1512329924},{0.0197395947,-0.1997170581,-0.1425959019,-0.274947791,-0.0969467073,0.060368852,-0.2826905192,0.1794315473}}");
MATH-350f726c$$"BrentOptimizer" not always reporting the best point$${{BrentOptimizer}} (package "o.a.c.m.optimization.univariate") does not check that the point it is going to return is indeed the best one it has encountered. Indeed, the last evaluated point might be slightly worse than the one before last.
MATH-ac597cc1$$"BrentOptimizer" not always reporting the best point$${{BrentOptimizer}} (package "o.a.c.m.optimization.univariate") does not check that the point it is going to return is indeed the best one it has encountered. Indeed, the last evaluated point might be slightly worse than the one before last.
MATH-66dece12$$Fix and then deprecate isSupportXxxInclusive in RealDistribution interface$$The conclusion from [1] was never implemented. We should deprecate these properties from the RealDistribution interface, but since removal will have to wait until 4.0, we should agree on a precise definition and fix the code to match it in the mean time.  The definition that I propose is that isSupportXxxInclusive means that when the density function is applied to the upper or lower bound of support returned by getSupportXxxBound, a finite (i.e. not infinite), not NaN value is returned.  [1] http://markmail.org/message/dxuxh7eybl7xejde
MATH-abe53a53$$CMAESOptimizer does not enforce bounds$$The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.
MATH-b55e0206$$Wide bounds to CMAESOptimizer result in NaN parameters passed to fitness function$$If you give large values as lower/upper bounds (for example -Double.MAX_VALUE as a lower bound), the optimizer can call the fitness function with parameters set to NaN.  My guess is this is due to FitnessFunction.encode/decode generating NaN when normalizing/denormalizing parameters.  For example, if the difference between the lower and upper bound is greater than Double.MAX_VALUE, encode could divide infinity by infinity.
MATH-bfbb156d$$CMAESOptimizer with bounds fits finely near lower bound and coarsely near upper bound.$$When fitting with bounds, the CMAESOptimizer fits finely near the lower bound and coarsely near the upper bound.  This is because it internally maps the fitted parameter range into the interval [0,1].  The unit of least precision (ulp) between floating point numbers is much smaller near zero than near one.  Thus, fits have much better resolution near the lower bound (which is mapped to zero) than the upper bound (which is mapped to one).  I will attach a example program to demonstrate.
MATH-2a9cbbab$$Polygon difference produces erronious results in some cases$$The 2D polygon difference method is returning incorrect results.  Below is a test case of subtracting two polygons (Sorry, this is the simplest case that I could find that duplicates the problem).    There are three problems with the result. The first is that the first point of the first set of vertices is null (and the first point of the second set is also null).  The second is that, even if the first null points are ignored,  the returned polygon is not the correct result. The first and last points are way off, and the remaining points do not match the original polygon boundaries.  Additionally, there are two holes that are returned in the results.  This subtraction case should not have holes.  {code:title="Complex Polygon Difference Test"} public void testComplexDifference() {         Vector2D[][] vertices1 = new Vector2D[][] {             new Vector2D[] {                     new Vector2D( 90.08714908223715,  38.370299337260235),                     new Vector2D( 90.08709517675004,  38.3702895991413),                     new Vector2D( 90.08401538704919,  38.368849330127944),                     new Vector2D( 90.08258210430711,  38.367634558585564),                     new Vector2D( 90.08251455106665,  38.36763409247078),                     new Vector2D( 90.08106599752608,  38.36761621664249),                     new Vector2D( 90.08249585300035,  38.36753627557965),                     new Vector2D( 90.09075743352184,  38.35914647644972),                     new Vector2D( 90.09099945896571,  38.35896264724079),                     new Vector2D( 90.09269383800086,  38.34595756121246),                     new Vector2D( 90.09638631543191,  38.3457988093121),                     new Vector2D( 90.09666417351019,  38.34523360999418),                     new Vector2D( 90.1297082145872,  38.337670454923625),                     new Vector2D( 90.12971687748956,  38.337669827794684),                     new Vector2D( 90.1240820219179,  38.34328502001131),                     new Vector2D( 90.13084259656404,  38.34017811765017),                     new Vector2D( 90.13378567942857,  38.33860579180606),                     new Vector2D( 90.13519557833206,  38.33621054663689),                     new Vector2D( 90.13545616732307,  38.33614965452864),                     new Vector2D( 90.13553111202748,  38.33613962818305),                     new Vector2D( 90.1356903436448,  38.33610227127048),                     new Vector2D( 90.13576283227428,  38.33609255422783),                     new Vector2D( 90.13595870833188,  38.33604606376991),                     new Vector2D( 90.1361556630693,  38.3360024198866),                     new Vector2D( 90.13622408795709,  38.335987048115726),                     new Vector2D( 90.13696189099994,  38.33581914328681),                     new Vector2D( 90.13746655304897,  38.33616706665265),                     new Vector2D( 90.13845973716064,  38.33650776167099),                     new Vector2D( 90.13950901827667,  38.3368469456463),                     new Vector2D( 90.14393814424852,  38.337591835857495),                     new Vector2D( 90.14483839716831,  38.337076122362475),                     new Vector2D( 90.14565474433601,  38.33769000964429),                     new Vector2D( 90.14569421179482,  38.3377117256905),                     new Vector2D( 90.14577067124333,  38.33770883625908),                     new Vector2D( 90.14600350631684,  38.337714326520995),                     new Vector2D( 90.14600355139731,  38.33771435193319),                     new Vector2D( 90.14600369112401,  38.33771443882085),                     new Vector2D( 90.14600382486884,  38.33771453466096),                     new Vector2D( 90.14600395205912,  38.33771463904344),                     new Vector2D( 90.14600407214999,  38.337714751520764),                     new Vector2D( 90.14600418462749,  38.337714871611695),                     new Vector2D( 90.14600422249327,  38.337714915811034),                     new Vector2D( 90.14867838361471,  38.34113888210675),                     new Vector2D( 90.14923750157374,  38.341582537502575),                     new Vector2D( 90.14877083250991,  38.34160685841391),                     new Vector2D( 90.14816667319519,  38.34244232585684),                     new Vector2D( 90.14797696744586,  38.34248455284745),                     new Vector2D( 90.14484318014337,  38.34385573215269),                     new Vector2D( 90.14477919958296,  38.3453797747614),                     new Vector2D( 90.14202393306448,  38.34464324839456),                     new Vector2D( 90.14198920640195,  38.344651155237216),                     new Vector2D( 90.14155207025175,  38.34486424263724),                     new Vector2D( 90.1415196143314,  38.344871730519),                     new Vector2D( 90.14128611910814,  38.34500196593859),                     new Vector2D( 90.14047850603913,  38.34600084496253),                     new Vector2D( 90.14045907000337,  38.34601860032171),                     new Vector2D( 90.14039496493928,  38.346223030432384),                     new Vector2D( 90.14037626063737,  38.346240203360026),                     new Vector2D( 90.14030005823724,  38.34646920000705),                     new Vector2D( 90.13799164754806,  38.34903093011013),                     new Vector2D( 90.11045289492762,  38.36801537312368),                     new Vector2D( 90.10871471476526,  38.36878044144294),                     new Vector2D( 90.10424901707671,  38.374300101757),                     new Vector2D( 90.10263482039932,  38.37310041316073),                     new Vector2D( 90.09834601753448,  38.373615053823414),                     new Vector2D( 90.0979455456843,  38.373578376172475),                     new Vector2D( 90.09086514328669,  38.37527884194668),                     new Vector2D( 90.09084931407364,  38.37590801712463),                     new Vector2D( 90.09081227075944,  38.37526295920463),                     new Vector2D( 90.09081378927135,  38.375193883266434)             }         };         PolygonsSet set1 = buildSet(vertices1);          Vector2D[][] vertices2 = new Vector2D[][] {             new Vector2D[] {                     new Vector2D( 90.13067558880044,  38.36977255037573),                     new Vector2D( 90.12907570488,  38.36817308242706),                     new Vector2D( 90.1342774136516,  38.356886880294724),                     new Vector2D( 90.13090330629757,  38.34664392676211),                     new Vector2D( 90.13078571364593,  38.344904617518466),                     new Vector2D( 90.1315602208914,  38.3447185040846),                     new Vector2D( 90.1316336226821,  38.34470643148342),                     new Vector2D( 90.134020944832,  38.340936644972885),                     new Vector2D( 90.13912536387306,  38.335497255122334),                     new Vector2D( 90.1396178806582,  38.334878075552126),                     new Vector2D( 90.14083049696671,  38.33316530644106),                     new Vector2D( 90.14145252901329,  38.33152722916191),                     new Vector2D( 90.1404779335565,  38.32863516047786),                     new Vector2D( 90.14282712131586,  38.327504432532066),                     new Vector2D( 90.14616669875488,  38.3237354115015),                     new Vector2D( 90.14860976050608,  38.315714862457924),                     new Vector2D( 90.14999277782437,  38.3164932507504),                     new Vector2D( 90.15005207194997,  38.316534677663356),                     new Vector2D( 90.15508513859612,  38.31878731691609),                     new Vector2D( 90.15919938519221,  38.31852743183782),                     new Vector2D( 90.16093758658837,  38.31880662005153),                     new Vector2D( 90.16099420184912,  38.318825953291594),                     new Vector2D( 90.1665411125756,  38.31859497874757),                     new Vector2D( 90.16999653861313,  38.32505772048029),                     new Vector2D( 90.17475243391698,  38.32594398441148),                     new Vector2D( 90.17940844844992,  38.327427213761325),                     new Vector2D( 90.20951909541378,  38.330616833491774),                     new Vector2D( 90.2155400467941,  38.331746223670336),                     new Vector2D( 90.21559881391778,  38.33175551425302),                     new Vector2D( 90.21916646426041,  38.332584299620805),                     new Vector2D( 90.23863749852285,  38.34778978875795),                     new Vector2D( 90.25459855175802,  38.357790570608984),                     new Vector2D( 90.25964298227257,  38.356918010203174),                     new Vector2D( 90.26024593994703,  38.361692743151366),                     new Vector2D( 90.26146187570015,  38.36311080550837),                     new Vector2D( 90.26614159359622,  38.36510808579902),                     new Vector2D( 90.26621342936448,  38.36507942500333),                     new Vector2D( 90.26652190211962,  38.36494042196722),                     new Vector2D( 90.26621240678867,  38.365113172030874),                     new Vector2D( 90.26614057102057,  38.365141832826794),                     new Vector2D( 90.26380080055299,  38.3660381760273),                     new Vector2D( 90.26315345241,  38.36670658276421),                     new Vector2D( 90.26251574942881,  38.367490323488084),                     new Vector2D( 90.26247873448426,  38.36755266444749),                     new Vector2D( 90.26234628016698,  38.36787989125406),                     new Vector2D( 90.26214559424784,  38.36945909356126),                     new Vector2D( 90.25861728442555,  38.37200753430875),                     new Vector2D( 90.23905557537864,  38.375405314295904),                     new Vector2D( 90.22517251874075,  38.38984691662256),                     new Vector2D( 90.22549955153215,  38.3911564273979),                     new Vector2D( 90.22434386063355,  38.391476432092134),                     new Vector2D( 90.22147729457276,  38.39134652252034),                     new Vector2D( 90.22142070120117,  38.391349167741964),                     new Vector2D( 90.20665060751588,  38.39475580900313),                     new Vector2D( 90.20042268367109,  38.39842558622888),                     new Vector2D( 90.17423771242085,  38.402727751805344),                     new Vector2D( 90.16756796257476,  38.40913898597597),                     new Vector2D( 90.16728283954308,  38.411255399912875),                     new Vector2D( 90.16703538220418,  38.41136059866693),                     new Vector2D( 90.16725865657685,  38.41013618805954),                     new Vector2D( 90.16746107640665,  38.40902614307544),                     new Vector2D( 90.16122795307462,  38.39773101873203)             }         };         PolygonsSet set2 = buildSet(vertices2);         PolygonsSet set  = (PolygonsSet) new RegionFactory<Euclidean2D>().difference(set1.copySelf(),                set2.copySelf());          Vector2D[][] verticies = set.getVertices();         Assert.assertTrue(verticies[0][0] != null);         Assert.assertEquals(1, verticies.length);     } {code}
MATH-2b852d79$$SpearmansCorrelation fails when using NaturalRanking together with NaNStrategy.REMOVED$$As reported by Martin Rosellen on the users mailinglist:  Using a NaturalRanking with a REMOVED NaNStrategy can result in an exception when NaN are contained in the input arrays.  The current implementation just removes the NaN values where they occur, without taken care to remove the corresponding values in the other array.
MATH-ce126bdb$$A random crash of MersenneTwister random generator$$There is a very small probability that MersenneTwister generator gives a following error:  java.lang.ArrayIndexOutOfBoundsException: 624 in MersenneTwister.java line 253 The error is completely random and its probability is about 1e-8.  UPD: The problem most probably arises only in multy-thread mode.
MATH-6844aba9$$FastMath.pow deviates from Math.pow for negative, finite base values with an exponent 2^52 < y < 2^53$$As reported by Jeff Hain:  pow(double,double): Math.pow(-1.0,5.000000000000001E15) = -1.0 FastMath.pow(-1.0,5.000000000000001E15) = 1.0 ===> This is due to considering that power is an even integer if it is >= 2^52, while you need to test that it is >= 2^53 for it. ===> replace "if (y >= TWO_POWER_52 || y <= -TWO_POWER_52)" with "if (y >= 2*TWO_POWER_52 || y <= -2*TWO_POWER_52)" and that solves it.
MATH-2836a6f9$$new multivariate vector optimizers cannot be used with large number of weights$$When using the Weigth class to pass a large number of weights to multivariate vector optimizers, an nxn full matrix is created (and copied) when a n elements vector is used. This exhausts memory when n is large.  This happens for example when using curve fitters (even simple curve fitters like polynomial ones for low degree) with large number of points. I encountered this with curve fitting on 41200 points, which created a matrix with 1.7 billion elements.
MATH-b07ecae3$$new multivariate vector optimizers cannot be used with large number of weights$$When using the Weigth class to pass a large number of weights to multivariate vector optimizers, an nxn full matrix is created (and copied) when a n elements vector is used. This exhausts memory when n is large.  This happens for example when using curve fitters (even simple curve fitters like polynomial ones for low degree) with large number of points. I encountered this with curve fitting on 41200 points, which created a matrix with 1.7 billion elements.
MATH-185e3033$$GammaDistribution cloning broken$$Serializing a GammaDistribution and deserializing it, does not result in a cloned distribution that produces the same samples.  Cause: GammaDistribution inherits from AbstractRealDistribution, which implements Serializable. AbstractRealDistribution has random, in which we have a Well19937c instance, which inherits from AbstractWell. AbstractWell implements Serializable. AbstractWell inherits from BitsStreamGenerator, which is not Serializable, but does have a private field 'nextGaussian'.  Solution: Make BitStreamGenerator implement Serializable as well.  This probably affects other distributions as well.
MATH-cedf0d27$$MultivariateNormalDistribution.density(double[]) returns wrong value when the dimension is odd$$To reproduce: {code} Assert.assertEquals(0.398942280401433, new MultivariateNormalDistribution(new double[]{0}, new double[][]{{1}}).density(new double[]{0}), 1e-15); {code}
MATH-724795b5$$Complex.ZERO.reciprocal() returns NaN but should return INF.$$Complex.ZERO.reciprocal() returns NaN but should return INF.  Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version  Id: Complex.java 1416643 2012-12-03 19:37:14Z tn
MATH-48dde378$$DerivativeStructure.atan2(y,x) does not handle special cases properly$$The four special cases +/-0 for both x and y should give the same values as Math.atan2 and FastMath.atan2. However, they give NaN for the value in all cases.
MATH-73605560$$Line.revert() is imprecise$$Line.revert() only maintains ~10 digits for the direction. This becomes an issue when the line's position is evaluated far from the origin. A simple fix would be to use Vector3D.negate() for the direction.  Also, is there a reason why Line is not immutable? It is just comprised of two vectors.
MATH-49444ee6$$stat.correlation.Covariance should allow one-column matrices$$Currently (rev 1453206), passing 1-by-M matrix to the Covariance constructor throws IllegalArgumentException. For consistency, the Covariance class should work for a single-column matrix (i.e., for a N-dimensional random variable with N=1) and it should return 1-by-1 covariance matrix with the variable's variance in its only element.
MATH-0d057fc6$$DiscreteDistribution.sample(int) may throw an exception if first element of singletons of sub-class type$$Creating an array with {{Array.newInstance(singletons.get(0).getClass(), sampleSize)}} in DiscreteDistribution.sample(int) is risky. An exception will be thrown if: * {{singleons.get(0)}} is of type T1, an sub-class of T, and * {{DiscreteDistribution.sample()}} returns an object which is of type T, but not of type T1.  To reproduce: {code} List<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>(); list.add(new Pair<Object, Double>(new Object() {}, new Double(0))); list.add(new Pair<Object, Double>(new Object() {}, new Double(1))); new DiscreteDistribution<Object>(list).sample(1); {code}  Attaching a patch.
MATH-f83bbc1d$$LevenbergMarquardtOptimizer reports 0 iterations$$The method LevenbergMarquardtOptimizer.getIterations() does not report the correct number of iterations; It always returns 0. A quick look at the code shows that only SimplexOptimizer calls BaseOptimizer.incrementEvaluationsCount()  I've put a test case below. Notice how the evaluations count is correctly incremented, but the iterations count is not.  {noformat}     @Test     public void testGetIterations() {         // setup         LevenbergMarquardtOptimizer otim = new LevenbergMarquardtOptimizer();          // action         otim.optimize(new MaxEval(100), new Target(new double[] { 1 }),                 new Weight(new double[] { 1 }), new InitialGuess(                         new double[] { 3 }), new ModelFunction(                         new MultivariateVectorFunction() {                             @Override                             public double[] value(double[] point)                                     throws IllegalArgumentException {                                 return new double[] { FastMath.pow(point[0], 4) };                             }                         }), new ModelFunctionJacobian(                         new MultivariateMatrixFunction() {                             @Override                             public double[][] value(double[] point)                                     throws IllegalArgumentException {                                 return new double[][] { { 0.25 * FastMath.pow(                                         point[0], 3) } };                             }                         }));          // verify         assertThat(otim.getEvaluations(), greaterThan(1));         assertThat(otim.getIterations(), greaterThan(1));     }  {noformat}
MATH-424cbd20$$event state not updated if an unrelated event triggers a RESET_STATE during ODE integration$$When an ODE solver manages several different event types, there are some unwanted side effects.  If one event handler asks for a RESET_STATE (for integration state) when its eventOccurred method is called, the other event handlers that did not trigger an event in the same step are not updated correctly, due to an early return.  As a result, when the next step is processed with a reset integration state, the forgotten event still refer to the start date of the previous state. This implies that when these event handlers will be checked for In some cases, the function defining an event g(double t, double[] y) is called with state parameters y that are completely wrong. In one case when the y array should have contained values between -1 and +1, one function call got values up to 1.0e20.  The attached file reproduces the problem.
MATH-9aabf587$$Use analytical function for UniformRealDistribution.inverseCumulativeProbability$$The inverse CDF is currently solved by a root finding function. It would be much simpler (and faster) to use the analytical expression. This would save the user from having to set the inverseCumAccuracy correctly.  I've attached a patch that implements this.
MATH-d270055e$$NPE when calling SubLine.intersection() with non-intersecting lines$$When calling SubLine.intersection() with two lines that not intersect, then a NullPointerException is thrown in Line.toSubSpace(). This bug is in the twod and threed implementations.  The attached patch fixes both implementations and adds the required test cases.
MATH-86545dab$$Fraction specified with maxDenominator and a value very close to a simple fraction should not throw an overflow exception$$An overflow exception is thrown when a Fraction is initialized with a maxDenominator from a double that is very close to a simple fraction.  For example:  double d = 0.5000000001; Fraction f = new Fraction(d, 10);  Patch with unit test on way.
OAK-83427028$$Property value converion ignores reisdual property definition$$Assume following node type which a property defined with type and a residual unnamed property also defined  {noformat} [oak:foo]  - stringProp (String)  - * (undefined) {noformat}  For such node type if a property {{stringProp}} is being set with a binary value then Oak converts it into a String property thereby causing binary stream to change. In JR2 conversion would not happen as conversion logic treats setting (stringProp,BINARY) as a residual property
OAK-2b5d3afb$$Full-text search on the traversing index fails if the condition contains a slash$$A full-text search on the traversing index falls back to a sort of manual evaluation of results.  This is handled by the _FullTextTerm_ class, and it appears that it passes the constraint text through a cleanup process where it strips most of the characters that are neither _Character.isLetterOrDigit(c)_ not in the list _+-:&_  I'm not exactly sure where this list comes from, but I see the '/' character is missing which causes a certain type of query to fail.  Example: {code} //*[jcr:contains(., 'text/plain')] {code}
OAK-e39b4d96$$Full-text search on the traversing index fails if the condition contains a slash$$A full-text search on the traversing index falls back to a sort of manual evaluation of results.  This is handled by the _FullTextTerm_ class, and it appears that it passes the constraint text through a cleanup process where it strips most of the characters that are neither _Character.isLetterOrDigit(c)_ not in the list _+-:&_  I'm not exactly sure where this list comes from, but I see the '/' character is missing which causes a certain type of query to fail.  Example: {code} //*[jcr:contains(., 'text/plain')] {code}
OAK-ecc5bdfd$$Full-text search on the traversing index fails if the condition contains a slash$$A full-text search on the traversing index falls back to a sort of manual evaluation of results.  This is handled by the _FullTextTerm_ class, and it appears that it passes the constraint text through a cleanup process where it strips most of the characters that are neither _Character.isLetterOrDigit(c)_ not in the list _+-:&_  I'm not exactly sure where this list comes from, but I see the '/' character is missing which causes a certain type of query to fail.  Example: {code} //*[jcr:contains(., 'text/plain')] {code}
OAK-b2ca8baa$$Property Index: cost calculation is wrong (zero) when searching for many values$$Currently, for queries of the form   {code} select * from [nt:unstructured] where type = 'xyz' {code}  the node type index is used in some cases, even if there is an index on the property "type". The reason is that the cost for the node type index is 0 due to a bug. The node type index internally uses the property index on the property "jcr:primaryType", and asks for the cost using all possible children node types of "nt:unstructured". The returned cost is 0 because of this bug. The cost estimation is an extrapolation of the number of entries for the first 3 values. It is currently coded as:  {code} count = count / size / i; {code}  when in fact it should be written as:  {code} count = count * size / i; {code}
OAK-0adf3a6e$$Folder containing an admin user should not be removed$$The action of removing a folder that contains the admin user should fail.  This is already the case if it is tried to remove the admin node .  Attaching unit test
OAK-79467350$$XPath query failures for mvps$$Adding some cases related to mvps that are not currently covered by the existing (jackrabbit) tests.
OAK-9238264d$$XPath failures for typed properties$$It looks like there are some failures in xpath queries that expect a match only on properties of a certain type (which is to be inferred from the query)
OAK-4ce4e3c9$$Node.getNodes throwing exception if user does not have access to any child node$$When trying to obtain child iterator via Node.getNodes {{InvalidItemStateException}} is thrown if user does not have access to its content  {code:java}     @Test     public void testGetChildren() throws Exception {         deny(path, privilegesFromName(PrivilegeConstants.JCR_ADD_CHILD_NODES));         NodeIterator it1 = testSession.getNode(path).getNodes();         while(it1.hasNext()){             Node n = it1.nextNode();             NodeIterator it2 = n.getNodes();         }     } {code}  Executing above code leads to following exception  {noformat} javax.jcr.InvalidItemStateException: Item is stale 	at org.apache.jackrabbit.oak.jcr.delegate.NodeDelegate.getTree(NodeDelegate.java:827) 	at org.apache.jackrabbit.oak.jcr.delegate.NodeDelegate.getChildren(NodeDelegate.java:336) 	at org.apache.jackrabbit.oak.jcr.session.NodeImpl 8.perform(NodeImpl.java:546) 	at org.apache.jackrabbit.oak.jcr.session.NodeImpl 8.perform(NodeImpl.java:543) 	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.perform(SessionDelegate.java:125) 	at org.apache.jackrabbit.oak.jcr.session.ItemImpl.perform(ItemImpl.java:113) 	at org.apache.jackrabbit.oak.jcr.session.NodeImpl.getNodes(NodeImpl.java:543) 	at org.apache.jackrabbit.oak.jcr.security.authorization.ReadPropertyTest.testGetChildren(ReadPropertyTest.java:135) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:464) 	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83) 	at org.junit.runner.JUnitCore.run(JUnitCore.java:157) 	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:77) 	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:195) 	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:63) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:120) {noformat}  The exception is thrown for path {{/testroot/node1/rep:policy}}.   The issue occurs because the {{NodeIterator}} {{it1}} includes {{rep:policy}} and later when its child are accessed security check leads to exception. Probably the {{it1}} should not include {{rep:policy}} as part of child list and filter it out
OAK-531aca78$$IllegalArgumentException on Row.getValues()$$Calling {{row.getValues()}} is throwing an {{IllegalArgumentException}} when called on the {{QueryResult}} of the query {{SELECT properties FROM \[nt:base\] WHERE \[sling:resourceType\]="cq/personalization/components/contextstores/surferinfo"}}  {quote} java.lang.IllegalArgumentException 	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:76) 	at org.apache.jackrabbit.oak.plugins.value.ValueImpl.checkSingleValued(ValueImpl.java:85) 	at org.apache.jackrabbit.oak.plugins.value.ValueImpl.<init>(ValueImpl.java:72) 	at org.apache.jackrabbit.oak.plugins.value.ValueFactoryImpl.createValue(ValueFactoryImpl.java:95) 	at org.apache.jackrabbit.oak.jcr.query.QueryResultImpl.createValue(QueryResultImpl.java:266) 	at org.apache.jackrabbit.oak.jcr.query.RowImpl.getValues(RowImpl.java:99) 	at com.day.cq.analytics.sitecatalyst.impl.FrameworkComponentImpl.getListProperty(FrameworkComponentImpl.java:128) 	at com.day.cq.analytics.sitecatalyst.impl.FrameworkComponentImpl.<init>(FrameworkComponentImpl.java:91) {quote}
OAK-d7f0f180$$IllegalArgumentException on Row.getValues()$$Calling {{row.getValues()}} is throwing an {{IllegalArgumentException}} when called on the {{QueryResult}} of the query {{SELECT properties FROM \[nt:base\] WHERE \[sling:resourceType\]="cq/personalization/components/contextstores/surferinfo"}}  {quote} java.lang.IllegalArgumentException 	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:76) 	at org.apache.jackrabbit.oak.plugins.value.ValueImpl.checkSingleValued(ValueImpl.java:85) 	at org.apache.jackrabbit.oak.plugins.value.ValueImpl.<init>(ValueImpl.java:72) 	at org.apache.jackrabbit.oak.plugins.value.ValueFactoryImpl.createValue(ValueFactoryImpl.java:95) 	at org.apache.jackrabbit.oak.jcr.query.QueryResultImpl.createValue(QueryResultImpl.java:266) 	at org.apache.jackrabbit.oak.jcr.query.RowImpl.getValues(RowImpl.java:99) 	at com.day.cq.analytics.sitecatalyst.impl.FrameworkComponentImpl.getListProperty(FrameworkComponentImpl.java:128) 	at com.day.cq.analytics.sitecatalyst.impl.FrameworkComponentImpl.<init>(FrameworkComponentImpl.java:91) {quote}
OAK-2e20589f$$CacheLIRS implementation incomplete$$The current CacheLIRS implementation is not complete and e.g. does not provide a write through ConcurrentMap view on {{asMap()}}. For OAK-1088 it would be good to have this implementation as it allows conditional and atomic updates of cache entries.
OAK-be44b816$$QueryManager does not have autorefresh$$Having two sessions A and B. A writes something for example /content/page/text = "text" Accessing B's QueryManager and exexcute a query for "text" nothing will be found. Triggering an explicit refresh on B before the query and the hit is found.  I assume that the autorefresh is missed for that case
OAK-7ae92779$$SegmentNodeStore rebase operation assumes wrong child node order$$This popped up during the async merge process. The merge first does a rebase which can fail, making some index files look like they disappeared [0], wrapping the actual root cause.  The problem is that the rebase failed and removed the missing file. This can be seen by analyzing the ':conflict' marker info: bq. addExistingNode {_b_Lucene41_0.doc, _b.fdx, _b.fdt, _b_4.del, } so it points to something trying to add some index related files twice, almost like a concurrent commit exception.  Digging even deeper I found that the rebase operation during the state comparison phase assumes a certain order of child nodes [1], and based on that tries to read the mentioned nodes again, thinking that they are new ones, when if fact they are already present in the list [2]. This causes a conflict which fails the entire async update process, but also any lucene search, as the index files are now gone and the index is in a corrupted state.   [0]  {noformat} *WARN* [pool-5-thread-2] org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate Index update async failed org.apache.jackrabbit.oak.api.CommitFailedException: OakLucene0004: Failed to close the Lucene index 	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditor.leave(LuceneIndexEditor.java:122) 	at org.apache.jackrabbit.oak.spi.commit.VisibleEditor.leave(VisibleEditor.java:64) 	at org.apache.jackrabbit.oak.spi.commit.VisibleEditor.leave(VisibleEditor.java:64) 	at org.apache.jackrabbit.oak.plugins.index.IndexUpdate.leave(IndexUpdate.java:129) 	at org.apache.jackrabbit.oak.spi.commit.EditorDiff.process(EditorDiff.java:56) 	at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.run(AsyncIndexUpdate.java:100) 	at org.apache.sling.commons.scheduler.impl.QuartzJobExecutor.execute(QuartzJobExecutor.java:105) 	at org.quartz.core.JobRunShell.run(JobRunShell.java:207) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) 	at java.util.concurrent.ThreadPoolExecutor Worker.run(ThreadPoolExecutor.java:615) 	at java.lang.Thread.run(Thread.java:724) Caused by: java.io.FileNotFoundException: _b_Lucene41_0.doc at org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory.openInput(OakDirectory.java:145) {noformat}  [1] http://svn.apache.org/viewvc/jackrabbit/oak/trunk/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/segment/MapRecord.java?view=markup#l329  [2] before child list {noformat} [_b_Lucene41_0.doc, _b.fdx, _b.fdt, segments_34, _b_4.del, _b_Lucene41_0.pos, _b.nvm, _b.nvd, _b.fnm, _3n.si, _b_Lucene41_0.tip, _b_Lucene41_0.tim, _3n.cfe, segments.gen, _3n.cfs, _b.si] {noformat}  after list {noformat} _b_Lucene41_0.pos, _3k.cfs, _3j_1.del, _b.nvm, _b.nvd, _3d.cfe, _3d.cfs, _b.fnm, _3j.si, _3h.si, _3i.cfe, _3i.cfs, _3e_2.del, _3f.si, _b_Lucene41_0.tip, _b_Lucene41_0.tim, segments.gen, _3e.cfe, _3e.cfs, _b.si,_3g.si, _3l.si, _3i_1.del, _3d_3.del, _3e.si, _3d.si, _b_Lucene41_0.doc, _3h_2.del, _3i.si, _3k_1.del, _3j.cfe, _3j.cfs, _b.fdx, _b.fdt, _3g_1.del, _3k.si, _3l.cfe, _3l.cfs, segments_33, _3f_1.del, _3h.cfe, _3h.cfs, _b_4.del, _3f.cfe, _3f.cfs, _3g.cfe, _3g.cfs {noformat}
OAK-a8c925e0$$Query constraints marked as invalid in the case of an mvp$$It seems that in the case of a query that has more constraints on the same property, like bq. //*[(@prop = 'aaa' and @prop = 'bbb' and @prop = 'ccc')]  the filter is marked as invalid (_#isAlwaysFalse_) and the query returns no results.  This is incorrect and affects queries that search for multi-valued properties on nodes.  This comes from/affects OAK-1075.
OAK-459bd065$$Node#setProperty(String, Calendar) doesn't take time zone in account$$Node#setProperty(String, Calendar) doesn't take time zone in account.  It looks the Calendar value is straightly stored as a long without take in consideration the time zone,  Unit test to follow
OAK-5286861d$$Empty branch commit returns head revision on trunk$$MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.
OAK-2f95b81f$$Repeated MongoMK.rebase() always adds new revision$$MongoMK always adds a new revision to the branch on rebase, even when the branch is already up-to-date.
OAK-f64e8adc$$PropertyIndex cost calculation is faulty$$The cost calculation can easily go out of bounds when it needs to estimate (whenever there are more than 100 nodes). The high value it returns can be higher than the traversal index which has a max of 10M, but can be less smaller.   For example:   100 nodes in the index:   with a single level /content cost is 6250000   adding a second level /content/data cost jumps to 1.544804416E9    101 nodes in the index:   with a single level /content cost is 100   adding a second level /content/data stays at 100    100 nodes, 12 levels deep, cost is 2.147483647E9   101 nodes, 12 levels deep, cost is 6.7108864E7
OAK-c05cec12$$Invalid JCR paths not caught$${{NamePathMapper.getOakPath}} should return {{null}} when called with an invalid JCR path like {{foo:bar]baz}}, but it doesn't.
OAK-61c877d8$$NPE if checking for a non-existing node in version storage$$NPE If a tree given to CompiledPermissionImpl.getTreePermission() does not have a primary type, e.g. for a "hidden" oak node:  {noformat} 	  at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:191) 	  at org.apache.jackrabbit.oak.security.authorization.permission.CompiledPermissionImpl.getTreePermission(CompiledPermissionImpl.java:160) 	  at org.apache.jackrabbit.oak.security.authorization.permission.CompiledPermissionImpl TreePermissionImpl.getChildPermission(CompiledPermissionImpl.java:443) 	  at org.apache.jackrabbit.oak.core.SecureNodeBuilder.getTreePermission(SecureNodeBuilder.java:352) 	  at org.apache.jackrabbit.oak.core.SecureNodeBuilder.exists(SecureNodeBuilder.java:129) 	  at org.apache.jackrabbit.oak.core.SecureNodeBuilder.hasChildNode(SecureNodeBuilder.java:271) 	  at org.apache.jackrabbit.oak.core.AbstractTree.getChildrenCount(AbstractTree.java:248) {noformat}  The tree passed here to get the children count is: {{/jcr:system/jcr:versionStorage}} and the child node not having a primary type is {{:index}}
OAK-342809f7$$Inconsistent handling of invalid names/paths$$Passing an invalid name to a JCR method might or might not throw a {{RepositoryException}} depending on whether name re-mappings exist or not:  {code} session.itemExists("/jcr:cont]ent"); {code}  returns {{false}} if no name re-mappings exist but throws a {{RepositoryException}} otherwise.
OAK-84fb6b29$$MutableTree#isNew: replace implementation by NodeBuilder#isNew$$Similar to the issue described in OAK-1177 we may consider replacing the implementation of MutableTree#isNew by the corresponding call on the NodeBuilder.  See also OAK-947.
OAK-f2bb1a17$$MutableTree#isNew: replace implementation by NodeBuilder#isNew$$Similar to the issue described in OAK-1177 we may consider replacing the implementation of MutableTree#isNew by the corresponding call on the NodeBuilder.  See also OAK-947.
OAK-f72dd8d1$$Uploading large number of files to single folder fails.$$Repository: OAK with TarPM  Upload is successful till 254 files and it started failing afterwards with exception in server logs.  [1]  {code} 14.11.2013 12:36:34.608 *ERROR* [10.40.146.206 [1384412794576] POST /content/dam/cq9032/./Coconut-5mb-110.jpg HTTP/1.1] org.apache.sling.servlets.post.impl.operations.ModifyOperation Exception during response processing. java.lang.IllegalStateException: null 	at com.google.common.base.Preconditions.checkState(Preconditions.java:133) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeRecordId(SegmentWriter.java:259) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeListBucket(SegmentWriter.java:346) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeList(SegmentWriter.java:508) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeProperty(SegmentWriter.java:669) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:847) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter 3.childNodeChanged(SegmentWriter.java:806) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:387) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:797) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter 3.childNodeChanged(SegmentWriter.java:806) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:387) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:797) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter 3.childNodeChanged(SegmentWriter.java:806) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:387) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:797) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentRootBuilder.getNodeState(SegmentRootBuilder.java:53) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentRootBuilder.getNodeState(SegmentRootBuilder.java:21) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore.rebase(SegmentNodeStore.java:135) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore.merge(SegmentNodeStore.java:113) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.merge(SegmentNodeStoreService.java:174) ~[na:na] 	at org.apache.jackrabbit.oak.core.AbstractRoot.commit(AbstractRoot.java:260) ~[na:na] 	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.commit(SessionDelegate.java:224) ~[na:na] 	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.commit(SessionDelegate.java:219) ~[na:na] 	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.commit(SessionDelegate.java:207) ~[na:na] 	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.save(SessionDelegate.java:332) ~[na:na] 	at org.apache.jackrabbit.oak.jcr.session.SessionImpl 8.perform(SessionImpl.java:399) ~[na:na] 	at org.apache.jackrabbit.oak.jcr.session.SessionImpl 8.perform(SessionImpl.java:396) ~[na:na] 	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.perform(SessionDelegate.java:128) ~[na:na] 	at org.apache.jackrabbit.oak.jcr.session.SessionImpl.perform(SessionImpl.java:117) ~[na:na] 	at org.apache.jackrabbit.oak.jcr.session.SessionImpl.save(SessionImpl.java:396) ~[na:na] 	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source) ~[na:na] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[na:1.6.0_26] 	at java.lang.reflect.Method.invoke(Unknown Source) ~[na:1.6.0_26] 	at org.apache.sling.jcr.base.SessionProxyHandler SessionProxyInvocationHandler.invoke(SessionProxyHandler.java:109) ~[na:na] 	at  Proxy9.save(Unknown Source) ~[na:na] {code}
OAK-52372042$$Parallel execution of ConcurrentReadAccessControlledTreeTest fails with MongoMK$$The is caused by concurrent creation of test content and the conflict it creates in the index. Every Oak test instance tries to create {{/oak:index/nodetype/:index/nt%3Afile}}, but only one will succeed. AFAICS there are two options how to handle this:  - Implement conflict annotation (OAK-1185), though I'm not sure this will really work. On commit, the rebase happens first, when changes from the other Oak instance may not be visible yet. Then, the commit hook runs and perform another branch commit with the changes, which works fine. Only the last step fails, when MongoMK tries to merge the branch. This is the point when the conflict may be detected.  - Implement a retry logic in MongoMK/NS
OAK-cb3ac20d$$Lucene Index should ignore property existence checks$$Some optimizations on the query engine transform certain clauses in property existence checks. ie (p = 'somevalue' turns into 'p is not null').  This doesn't play well with lucene as it can not  effectively build a 'not null' query, even worse the query doesn't return any results.  As a fix I'll just skip the existence constraints from the generated lucene query.
OAK-a9efe3c4$$Wildcards in relative property paths don't work in search expressions$$A search XPath of the form: {code} /jcr:root/etc/commerce/products//*[@size='M' or */@size='M'] {code} returns: {code} Invalid path: * {code} (This works fine in Jackrabbit.)
OAK-e403e003$$Path parsing must support SNS indexes, irrespective of SNS support$${code} Session.getNode("/foo/bar[2]"); {code}  throws {{javax.jcr.RepositoryException: Invalid name or path: /foo/bar\[2]}}  This should be an ItemNotFoundException (if the item does not exist), irrespective if the repository supports SNS or not.
OAK-3535afe2$$Session.nodeExists("/foo/bar[2]") must not throw PathNotFoundException$$similar to OAK-1216, Session.nodeExists() of an SNS path with indexes > 1 should return false.
OAK-117b0a3d$$Node.hasNode("foo[2]") must not throw PathNotFoundException$$similar to OAK-1225, Node.hasNode("foo[2]") should return false
OAK-1beb2a50$$Upgrade should not overwrite new oak specific builtin nodetypes$$None
OAK-b4a93c81$$Always create new UUID on ImportBehavior.IMPORT_UUID_CREATE_NEW$$The implementation should create a new UUID for each referenceable node even if there is no existing node with that UUID. This spec says:  bq.  Incoming nodes are assigned newly created identifiers upon addition to the workspace. As a result, identifier collisions never occur.  This will break backward compatibility, but is IMO the correct behavior and the only way to guarantee import of referenceable nodes does not fail in a concurrent import scenario. See OAK-1186 for more details.
OAK-0c3b3306$$Guard against invalid/missing checkpoints$$Playing with the backup revealed a case where a checkpoint can become invalid after a manual restore of the repository. [0] The NodeStore#retrieve apis already specify that this can return null in the case the checkpoint doesn't exist anymore, but it looks like the storage bits aren't yet prepared for that scenario.    [0] {noformat} org.apache.sling.commons.scheduler.impl.QuartzScheduler Exception during job execution of org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate@3a6d47 : Failed to load segment 8a8b281c-1a02-4950-aad5-aad8e436a0d8 java.lang.IllegalStateException: Failed to load segment 8a8b281c-1a02-4950-aad5-aad8e436a0d8 	at org.apache.jackrabbit.oak.plugins.segment.AbstractStore.readSegment(AbstractStore.java:109) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.Segment.getSegment(Segment.java:189) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.Record.getSegment(Record.java:97) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getTemplate(SegmentNodeState.java:56) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getChildNode(SegmentNodeState.java:209) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore.retrieve(SegmentNodeStore.java:175) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.retrieve(SegmentNodeStoreService.java:198) ~[na:na] 	at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.run(AsyncIndexUpdate.java:97) ~[na:na] 	at org.apache.sling.commons.scheduler.impl.QuartzJobExecutor.execute(QuartzJobExecutor.java:105) ~[org.apache.sling.commons.scheduler-2.4.2.jar:na] 	at org.quartz.core.JobRunShell.run(JobRunShell.java:207) [org.apache.sling.commons.scheduler-2.4.2.jar:na] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_40] 	at java.util.concurrent.ThreadPoolExecutor Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_40] 	at java.lang.Thread.run(Thread.java:724) [na:1.7.0_40] Caused by: java.lang.IllegalStateException: Segment 8a8b281c-1a02-4950-aad5-aad8e436a0d8 not found 	at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.loadSegment(FileStore.java:184) ~[na:na] {noformat}
OAK-25a70439$$Parallel execution of SimpleSearchTest fails with MongoMK$$At some point in the benchmark run one MongoMK instance will fail to read a node created by another instance. The exception is very similar to *E1* reported in OAK-1204.
OAK-b8fe2ded$$NodeType index doesn't respect the declaringNodeTypes setting$$Following the OAK-1150 discussion, I've noticed that the node type index doesn't respect the declaringNodeTypes setting. Setting a restriction on the node type index definition breaks the index - there are 0 query hits.
OAK-70564c7c$$Revisit full-text queries in case of multiple tokens$$There's still an issue with tokenizing the search terms when trying for example to search for a fulltext term that will split into 2 actual terms because of the analyzer.  Taking 'hello-world*' this will break into 2 tokens 'hello' and 'world*' which when treated as a PhraseQuery will not work, so I want to change this into a MutiPhraseQuery based on the simple tokens provided and all the existing tokens that match the wildchar character.
OAK-14849e22$$java.lang.IllegalArgumentException when running FlatTreeWithAceForSamePrincipalTest$$Running  {code} java -jar oak-run*.jar benchmark FlatTreeWithAceForSamePrincipalTest Oak-Tar {code} will end with {code} java.lang.IllegalArgumentException 	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:77) 	at org.apache.jackrabbit.oak.plugins.segment.ListRecord.<init>(ListRecord.java:37) 	at org.apache.jackrabbit.oak.plugins.segment.ListRecord.getEntries(ListRecord.java:80) 	at org.apache.jackrabbit.oak.plugins.segment.SegmentPropertyState.getValue(SegmentPropertyState.java:130) 	at org.apache.jackrabbit.oak.util.PropertyBuilder.assignFrom(PropertyBuilder.java:225) 	at org.apache.jackrabbit.oak.util.PropertyBuilder.copy(PropertyBuilder.java:136) 	at org.apache.jackrabbit.oak.core.MutableTree.addChild(MutableTree.java:216) 	at org.apache.jackrabbit.oak.util.TreeUtil.addChild(TreeUtil.java:190) 	at org.apache.jackrabbit.oak.jcr.delegate.NodeDelegate.internalAddChild(NodeDelegate.java:841) 	at org.apache.jackrabbit.oak.jcr.delegate.NodeDelegate.addChild(NodeDelegate.java:684) 	at org.apache.jackrabbit.oak.jcr.session.NodeImpl 5.perform(NodeImpl.java:288) 	at org.apache.jackrabbit.oak.jcr.session.NodeImpl 5.perform(NodeImpl.java:253) 	at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.perform(SessionDelegate.java:125) 	at org.apache.jackrabbit.oak.jcr.session.ItemImpl.perform(ItemImpl.java:111) 	at org.apache.jackrabbit.oak.jcr.session.NodeImpl.addNode(NodeImpl.java:253) 	at org.apache.jackrabbit.oak.jcr.session.NodeImpl.addNode(NodeImpl.java:238) 	at org.apache.jackrabbit.oak.benchmark.FlatTreeWithAceForSamePrincipalTest.beforeSuite(FlatTreeWithAceForSamePrincipalTest.java:56) 	at org.apache.jackrabbit.oak.benchmark.AbstractTest.setUp(AbstractTest.java:113) 	at org.apache.jackrabbit.oak.benchmark.FlatTreeWithAceForSamePrincipalTest.setUp(FlatTreeWithAceForSamePrincipalTest.java:31) 	at org.apache.jackrabbit.oak.benchmark.AbstractTest.runTest(AbstractTest.java:151) 	at org.apache.jackrabbit.oak.benchmark.AbstractTest.run(AbstractTest.java:138) 	at org.apache.jackrabbit.oak.benchmark.FlatTreeWithAceForSamePrincipalTest.run(FlatTreeWithAceForSamePrincipalTest.java:31) 	at org.apache.jackrabbit.oak.benchmark.BenchmarkRunner.main(BenchmarkRunner.java:195) 	at org.apache.jackrabbit.oak.run.Main.main(Main.java:81)  {code}
OAK-0c3e3d70$$Range check fails with IllegalArgumentException$${{Range.includes()}} fails with IllegalArgumentException when provided revision is from another cluster node:  {noformat} java.lang.IllegalArgumentException: Trying to compare revisions of different cluster ids: r142f43d2f0f-0-2 and r142f43d46fb-0-1 	at org.apache.jackrabbit.oak.plugins.mongomk.Revision.compareRevisionTime(Revision.java:84) 	at org.apache.jackrabbit.oak.plugins.mongomk.Range.includes(Range.java:55) {noformat}  The IllegalArgumentException was introduced with OAK-1274.
OAK-73cc2442$$MoveDetector does not detect moved nodes that have been moved in an earlier commit already$$None
OAK-69ba2a54$$XPath queries with ISO9075 escaped properties don't work$$XPath queries with ISO9075 escaped properties or relative path don't work as expected. Example:   {code} /jcr:root//*/element(*,rep:User)[_x002e_tokens/@jcr:primaryType] {code}  The relative property should be converted to ".tokens/@jcr:primaryType", but is not.  This issue is similar to OAK-1000, but for property names or relative properties.
OAK-64045631$$Inconsistent state in Mongo/KernelRootBuilder$$The state of Kernel- and MongoRootBuilder may turn inconsistent when a NodeStoreBranch.merge() performs a rebase followed by a failed merge on the underlying storage. The head and base are not properly updated to reflect the successful rebase.
OAK-bc7b7e8c$$ACE merging not behaving correctly if not using managed principals$${{org.apache.jackrabbit.api.security.JackrabbitAccessControlList#addEntry()}} does not work correctly, if the given principal is not retrieved from the PrincipalManager.  Exception: {noformat} Caused by: org.apache.jackrabbit.oak.api.CommitFailedException: OakAccessControl0013: Duplicate ACE found in policy 	at org.apache.jackrabbit.oak.security.authorization.accesscontrol.AccessControlValidator.accessViolation(AccessControlValidator.java:278) 	at org.apache.jackrabbit.oak.security.authorization.accesscontrol.AccessControlValidator.checkValidPolicy(AccessControlValidator.java:188) {noformat}  this used to work in jackrabbit 2.x.  the problem is probably in {{org.apache.jackrabbit.oak.security.authorization.accesscontrol.ACL#internalAddEntry}} where the principals are "equalled" instead of comparing their names.  note, that adding an ACE with such a principal works, just the merging/overwriting detection doesn't.  test: {code}   Principal p1 = new Principal() { getName(){return "foo"}};   Principal p2 = new Principal() { getName(){return "foo"}};   acl.addEntry(p1, privileges, true);   acl.addEntry(p2, privileges, false);   ...   save(); // throws {code}
OAK-438e31a7$$Better support for RangeIterators$$Currently all RangeIterators returned from the JCR API don't implement the {{getSize()}} method but rather return {{-1}}. We should return the size of the iterator if and where feasible.
OAK-69b68890$$TokenLoginModule does not set userId on auth info$$the token login module does not set the userid in the authinfo (because it does not know it). and the LoginModuleImpl does not overwrite the AuthInfo if it already exists.  the consequence: {{Session.getUserID()}} returns {{NULL}} for logins that create a token.  I think that the authinfos should be added even if they already exist. and all users of the public credentials need to be aware that authinfos can exist that are not complete.
OAK-05c89637$$CacheLIRS concurrency issue$$Some of the methods of the cache can throw a NullPointerException when the cache is used concurrently. Example stack trace:  {code} java.lang.NullPointerException: null org.apache.jackrabbit.oak.cache.CacheLIRS.values(CacheLIRS.java:470)  org.apache.jackrabbit.oak.cache.CacheLIRS 1.values(CacheLIRS.java:1432) org.apache.jackrabbit.oak.plugins.segment.file.FileStore.flush(FileStore.java:205) {code}
OAK-b481a14c$$CacheLIRS concurrency issue$$Some of the methods of the cache can throw a NullPointerException when the cache is used concurrently. Example stack trace:  {code} java.lang.NullPointerException: null org.apache.jackrabbit.oak.cache.CacheLIRS.values(CacheLIRS.java:470)  org.apache.jackrabbit.oak.cache.CacheLIRS 1.values(CacheLIRS.java:1432) org.apache.jackrabbit.oak.plugins.segment.file.FileStore.flush(FileStore.java:205) {code}
OAK-ce0b0955$$XPath queries: compatibility for missing @ in front of property names$$XPath queries with conditions of the form {noformat}[id='test']{noformat} are not problematic. Jackrabbit 2.x interpreted such conditions as {noformat}[@id='test']{noformat}, and Oak currently interprets them as {noformat}[@id/* = 'test']{noformat}, as this is the expected behavior for conditions of the form {noformat}[jcr:contains(id, 'test')]{noformat}.  I believe the condition {noformat}[id='test']{noformat} is illegal, and it would be better to throw an exception instead, saying a @ is missing.
OAK-279bb3ce$$Slow event listeners do not scale as expected$${{org.apache.jackrabbit.oak.jcr.LargeOperationIT#slowListener}} does not scale to {{O n log n}} on the document node store.
OAK-c2f5ca6c$$Slow event listeners do not scale as expected$${{org.apache.jackrabbit.oak.jcr.LargeOperationIT#slowListener}} does not scale to {{O n log n}} on the document node store.
OAK-808ac9c0$$Query: use "union" for complex XPath queries that use multiple "or"$$The following XPath query is converted to a union, however there is still an "or" in the converted query, which means the query still can't use all indexes and has to traverse the whole repository:  {noformat} /jcr:root/a/b//element(*, nt:unstructured)[( (@sling:resourceType = 'x'  or @sling:resourceType = 'dam/collection')  or @sling:resourceSuperType = 'dam/collection')]  {noformat}
OAK-f1ba7a42$$:childOrder out of sync when node is made orderable concurrently$$The ChildOrderConflictHandler does not merge the :childOrder when an addExistingProperty conflict occurs.
OAK-dde7de85$$Commit.rollback() may remove changes from other commit$$Commit.rollback() removes documents it previously created. With concurrent commits it may happen that this method removes documents some other commit modified in the meantime.
OAK-7c62bd81$$PhraseQuery fails due to missing posiion info in indexed fields$$Following OAK-1487 I've introduced a regression in the indexing of fields on the Lucene index. There are some types of queries (the ones that use property restrictions) that cannot run anymore.  bq. /jcr:root/content/dam//*[jcr:contains(jcr:content/metadata/@dc:format, 'application/pdf')]   bq. Caused by: java.lang.IllegalStateException: field "dc:format" was indexed without position data; cannot run PhraseQuery (term=text)  I could not reproduce this in an unit test so far.
OAK-86edbffb$$Oak Analyzer can't tokenize chinese phrases$$It looks like the _WhitespaceTokenizer_ cannot properly split Chinese phrases, for example '美女衬衫'. I could not find a reference to this issue other than LUCENE-5096.  The fix is to switch to the _ClassicTokenizer_ which seems better equipped for this kind of task.
OAK-6d8146f8$$Item names with trailing spaces should not be allowed$$the following should fail:  {code}         Node hello = session.getRootNode().addNode("hello");         session.save();          Node illegal = hello.addNode("test "); <-- here         session.save();          assertEquals("/hello/test ", illegal.getPath()); <-- and here          Node other = session.getNode("/hello/test "); <-- and here         assertTrue(other.isSame(illegal));         assertTrue(session.nodeExists("/hello/test ")); <-- and here {code}
OAK-fdc54465$$Creating multiple checkpoint on same head revision overwrites previous entries$$Currently when a checkpoint is created in DocumentNodeStore then it is saved in form of currentHeadRev=>expiryTime. Now if multiple checkpoints are created where head revision has not changed then only the last one would be saved and previous entries would be overridden as revision is used as key  One fix would be to change the expiry time only if the new expiry time is greater than previous entry. However doing that safely in a cluster (check then save) is currently not possible with DocumentStore API as the modCount check if only supported for Nodes.
OAK-01a8b283$$DataStoreBlobStore does not take into maxLastModifiedTime when fetching all chunks$$Currently the {{DataStoreBlobStore}} has a pending TODO  {code}  @Override     public Iterator<String> getAllChunkIds(long maxLastModifiedTime) throws Exception {         //TODO Ignores the maxLastModifiedTime currently.         return Iterators.transform(delegate.getAllIdentifiers(), new Function<DataIdentifier, String>() {             @Nullable             @Override             public String apply(@Nullable DataIdentifier input) {                 return input.toString();             }         });     } {code}  Due to this it currently returns all blobId. This would issue when new binary gets created while a blob gc is running as such binaries might be considered orphan and deleted
OAK-c91bfa54$$DataStoreBlobStore does not take into maxLastModifiedTime when fetching all chunks$$Currently the {{DataStoreBlobStore}} has a pending TODO  {code}  @Override     public Iterator<String> getAllChunkIds(long maxLastModifiedTime) throws Exception {         //TODO Ignores the maxLastModifiedTime currently.         return Iterators.transform(delegate.getAllIdentifiers(), new Function<DataIdentifier, String>() {             @Nullable             @Override             public String apply(@Nullable DataIdentifier input) {                 return input.toString();             }         });     } {code}  Due to this it currently returns all blobId. This would issue when new binary gets created while a blob gc is running as such binaries might be considered orphan and deleted
OAK-3efb5cbf$$Node not accessible after document split$$In a cluster setup it may happen that a node becomes inaccessible when all remaining local revision entries after a split are not yet visible to a cluster node.
OAK-63070cf9$$Lucene should not serve queries for what it doesn't index$$If a query is asked and Lucene is chosen as index for serving it, it will try to serve all the restrictions of the query, even the one that are not indexed.
OAK-073b814c$$Node isNew() is false in case the node is removed and added in same commit$$When you remove a Node /path/a transiently and add one add /path/a again. The transiently added Node isNew() check will be false. {code} root.getNode(name).remove(); Node newNode = root.addNode(name); nowNode.isNew() => false {code}  The API says {quote} Returns true if this is a new item, meaning that it exists only in transient storage on the Session and has not yet been saved. Within a transaction, isNew on an Item may return false (because the item has been saved) even if that Item is not in persistent storage (because the transaction has not yet been committed).... {quote}
OAK-1552be04$$Unresolved conflicts in TokenProviderImpl#createToken()$$In certain situations (e.g. heavy load) {{TokenProviderImpl#createToken()}} might create some unresolved conflicts.  e.g.   {code} org.apache.jackrabbit.oak.api.CommitFailedException: OakState0001: Unresolved conflicts in /home/users/..../..../.tokens/2014-04-07T11.55.58.167+02.00 {code}  and  {code} 01.04.2014 17:52:41.216 *WARN* [qtp218544742-286] org.apache.jackrabbit.oak.security.authentication.token.TokenProviderImpl Failed to create login token. 01.04.2014 17:52:41.218 *WARN* [qtp218544742-300] org.eclipse.jetty.servlet.ServletHandler /projects.html java.lang.IllegalArgumentException: Invalid token ''     at org.apache.jackrabbit.api.security.authentication.token.TokenCredentials.<init>(TokenCredentials.java:42) {code}
OAK-c3773d53$$Missing commit hooks in upgrade$$There's a TODO in the RepositoryUpgrade class about missing commit hooks. For example the PermissionHook isn't currently run as a part of the upgrade, which breaks permission evaluation even though the actual ACL nodes are present after the upgrade.
OAK-26041fe7$$Cross foreign cluster revision comparison may be wrong$$Running one of the access control related benchmarks concurrently on a MongoDB may result in strange conflicts even when DocumentNodeStore retries the commit. The root cause may be a wrong revision comparison when both revision to compare are from a foreign cluster node and one of them is not withing the known seen-at revision ranges.
OAK-7ba9dd66$$DocumentNodeStore revision GC removes intermediate docs$$The revision garbage collection in DocumentNodeStore removes intermediate documents of the revision history of a node even if it is still in use.
OAK-024e5d37$$Repository upgrade does not copy default values of property definitions$$The {{RepositoryUpgrade}} class needs to copy also the default values of property definitions in the node types being upgraded. See the TODO in https://github.com/apache/jackrabbit-oak/blob/jackrabbit-oak-0.20.0/oak-upgrade/src/main/java/org/apache/jackrabbit/oak/upgrade/RepositoryUpgrade.java#L485.
OAK-8188ef54$$Incorrect handling of multivalued comparisons in queries$$[Section 6.7.14|http://www.day.com/specs/jcr/2.0/6_Query.html#6.7.16 Comparison] of the JCR 2.0 spec says:  bq. ... operand1 may evaluate to an array of values (for example, the values of a multi-valued property), in which case the comparison is separately performed for each element of the array, and the Comparison constraint is satisfied as a whole if the comparison against any element of the array is satisfied.  This is currently not the case in Oak. Instead only the first value of the array is used in the comparison.
OAK-591e4d4a$$AsyncIndexUpdate may resurrect nodes$$There is a race condition in the AsyncIndexUpdate.run() method. The implementation creates a checkpoint used as the after node state for the comparison with the previous checkpoint. In a next step a builder is created from the current root state of the node store. Node removed between the checkpoint call and retrieving the root state may get resurrected by the AsyncIndexUpdate.
OAK-f37ce716$$DocumentNodeStore does not make use of References while serializing Blob$$The BlobSerializer in DocumentNodeStore does not make use of Blob references which results in copying the blobs by value hence significantly slowing down any migration
OAK-192ee9e4$$Document split suppressed with steady load on many cluster nodes$$Document split is suppressed when there is a steady write load on many cluster nodes. The document grows bigger over time and leads to poor performance.
OAK-9d36bede$$Stale cache after MongoMK GC$$After a MongoMK revision GC the docChildrenCache may be stale and lead to a NPE when reading children with deleted and GC'ed siblings.
OAK-2426deae$$Async index update persists conflict markers$$A long running test I performed yesterday failed with a FileNotFoundException in the lucene index. After analyzing the issue it turned  out the async index update persisted a conflict markers introduced by a rebase call. So far I'm not able to reproduce it with a more simple test setup and after a shorter time (the initial test failed after 10 hours). Given the way the async index update work, there shouldn't be any conflicts, because it's the only component writing into this location of the repository.   As an immediate workaround, I'd like to add the AnnotatingConflictHandler & ConflictValidator combo to the merge call to make sure a commit with conflict markers does not get persisted.
OAK-dd3437d4$$ConcurrentConflictTest fails occasionally$$Occurs every now and then on buildbot. E.g.: http://ci.apache.org/builders/oak-trunk-win7/builds/16
OAK-07646fba$$Upgraded version history has UUIDs as jcr:frozenUuid of non-referenceable nodes$$In Jackrabbit Classic each node, even non-referenceable ones, has a UUID as its identifier, and thus the {{jcr:frozenUuid}} properties of frozen nodes are always UUIDs. In contrast Oak uses path identifiers for non-referenceable frozen nodes (see OAK-1009), which presents a problem when dealing with version histories migrated from Jackrabbit Classic.  To avoid this mismatch, the upgrade code should check each frozen node for referenceability and replace the frozen UUID with a path identifier if needed.
OAK-08ba79d4$$Upgraded version history has UUIDs as jcr:frozenUuid of non-referenceable nodes$$In Jackrabbit Classic each node, even non-referenceable ones, has a UUID as its identifier, and thus the {{jcr:frozenUuid}} properties of frozen nodes are always UUIDs. In contrast Oak uses path identifiers for non-referenceable frozen nodes (see OAK-1009), which presents a problem when dealing with version histories migrated from Jackrabbit Classic.  To avoid this mismatch, the upgrade code should check each frozen node for referenceability and replace the frozen UUID with a path identifier if needed.
OAK-9f7c1df0$$Upgraded version history has UUIDs as jcr:frozenUuid of non-referenceable nodes$$In Jackrabbit Classic each node, even non-referenceable ones, has a UUID as its identifier, and thus the {{jcr:frozenUuid}} properties of frozen nodes are always UUIDs. In contrast Oak uses path identifiers for non-referenceable frozen nodes (see OAK-1009), which presents a problem when dealing with version histories migrated from Jackrabbit Classic.  To avoid this mismatch, the upgrade code should check each frozen node for referenceability and replace the frozen UUID with a path identifier if needed.
OAK-16225d51$$MongoMK GC removes documents with data still in use$$The version garbage collector may delete previous documents that contain commit root information still in use by the main document.
OAK-077efee5$$ConstraintViolationException seen with multiple Oak/Mongo with ConcurrentCreateNodesTest$$While running ConcurrentCreateNodesTest with 5 instances writing to same Mongo instance following exception is seen  {noformat} Exception in thread "Background job org.apache.jackrabbit.oak.benchmark.ConcurrentCreateNodesTest Writer@3f56e5ed" java.lang.RuntimeException: javax.jcr.nodetype.ConstraintViolationException: OakConstraint0001: /: The primary type rep:root does not exist     at org.apache.jackrabbit.oak.benchmark.ConcurrentCreateNodesTest Writer.run(ConcurrentCreateNodesTest.java:111)     at org.apache.jackrabbit.oak.benchmark.AbstractTest 1.run(AbstractTest.java:481) Caused by: javax.jcr.nodetype.ConstraintViolationException: OakConstraint0001: /: The primary type rep:root does not exist     at org.apache.jackrabbit.oak.api.CommitFailedException.asRepositoryException(CommitFailedException.java:225)     at org.apache.jackrabbit.oak.api.CommitFailedException.asRepositoryException(CommitFailedException.java:212)     at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.newRepositoryException(SessionDelegate.java:679)     at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.save(SessionDelegate.java:553)     at org.apache.jackrabbit.oak.jcr.session.SessionImpl 8.perform(SessionImpl.java:417)     at org.apache.jackrabbit.oak.jcr.session.SessionImpl 8.perform(SessionImpl.java:414)     at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.perform(SessionDelegate.java:308)     at org.apache.jackrabbit.oak.jcr.session.SessionImpl.perform(SessionImpl.java:127)     at org.apache.jackrabbit.oak.jcr.session.SessionImpl.save(SessionImpl.java:414)     at org.apache.jackrabbit.oak.benchmark.ConcurrentCreateNodesTest Writer.run(ConcurrentCreateNodesTest.java:100)     ... 1 more Caused by: org.apache.jackrabbit.oak.api.CommitFailedException: OakConstraint0001: /: The primary type rep:root does not exist     at org.apache.jackrabbit.oak.plugins.nodetype.TypeEditor.constraintViolation(TypeEditor.java:150)     at org.apache.jackrabbit.oak.plugins.nodetype.TypeEditor.getEffectiveType(TypeEditor.java:286)     at org.apache.jackrabbit.oak.plugins.nodetype.TypeEditor.<init>(TypeEditor.java:101)     at org.apache.jackrabbit.oak.plugins.nodetype.TypeEditorProvider.getRootEditor(TypeEditorProvider.java:85)     at org.apache.jackrabbit.oak.spi.commit.CompositeEditorProvider.getRootEditor(CompositeEditorProvider.java:80)     at org.apache.jackrabbit.oak.spi.commit.EditorHook.processCommit(EditorHook.java:53)     at org.apache.jackrabbit.oak.spi.commit.CompositeHook.processCommit(CompositeHook.java:60)     at org.apache.jackrabbit.oak.spi.commit.CompositeHook.processCommit(CompositeHook.java:60)     at org.apache.jackrabbit.oak.spi.state.AbstractNodeStoreBranch InMemory.merge(AbstractNodeStoreBranch.java:498)     at org.apache.jackrabbit.oak.spi.state.AbstractNodeStoreBranch.merge(AbstractNodeStoreBranch.java:300)     at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.merge(DocumentNodeStoreBranch.java:129)     at org.apache.jackrabbit.oak.plugins.document.DocumentRootBuilder.merge(DocumentRootBuilder.java:159)     at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.merge(DocumentNodeStore.java:1275)     at org.apache.jackrabbit.oak.core.MutableRoot.commit(MutableRoot.java:247)     at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.commit(SessionDelegate.java:405)     at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.save(SessionDelegate.java:551)     ... 7 more {noformat}  This has been reported by [~rogoz]
OAK-78c37386$$NPE in MarkSweepGarbageCollector.saveBatchToFile during Datastore GC with FileDataStore$$During running a datastore garbage collection on a Jackrabbit 2 FileDataStore (org.apache.jackrabbit.oak.plugins.blob.datastore.FileDataStore, see http://jackrabbit.apache.org/oak/docs/osgi_config.html) an NPE is thrown {code} 13.05.2014 17:50:16.944 *ERROR* [qtp1416657193-147] org.apache.jackrabbit.oak.management.ManagementOperation Blob garbage collection failed java.lang.RuntimeException: Error in retrieving references 	at org.apache.jackrabbit.oak.plugins.blob.MarkSweepGarbageCollector 1.addReference(MarkSweepGarbageCollector.java:395) 	at org.apache.jackrabbit.oak.plugins.segment.Segment.collectBlobReferences(Segment.java:248) 	at org.apache.jackrabbit.oak.plugins.segment.SegmentTracker.collectBlobReferences(SegmentTracker.java:178) 	at org.apache.jackrabbit.oak.plugins.segment.SegmentBlobReferenceRetriever.collectReferences(SegmentBlobReferenceRetriever.java:38) 	at org.apache.jackrabbit.oak.plugins.blob.MarkSweepGarbageCollector.iterateNodeTree(MarkSweepGarbageCollector.java:361) 	at org.apache.jackrabbit.oak.plugins.blob.MarkSweepGarbageCollector.mark(MarkSweepGarbageCollector.java:201) 	at org.apache.jackrabbit.oak.plugins.blob.MarkSweepGarbageCollector.markAndSweep(MarkSweepGarbageCollector.java:173) 	at org.apache.jackrabbit.oak.plugins.blob.MarkSweepGarbageCollector.collectGarbage(MarkSweepGarbageCollector.java:149) 	at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService 2.collectGarbage(SegmentNodeStoreService.java:185) 	at org.apache.jackrabbit.oak.plugins.blob.BlobGC 1.call(BlobGC.java:68) 	at org.apache.jackrabbit.oak.plugins.blob.BlobGC 1.call(BlobGC.java:64) 	at java.util.concurrent.FutureTask.run(FutureTask.java:262) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) 	at java.util.concurrent.ThreadPoolExecutor Worker.run(ThreadPoolExecutor.java:615) 	at java.lang.Thread.run(Thread.java:745) Caused by: java.lang.NullPointerException: null 	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:192) 	at com.google.common.base.Joiner.toString(Joiner.java:436) 	at com.google.common.base.Joiner.appendTo(Joiner.java:108) 	at com.google.common.base.Joiner.appendTo(Joiner.java:152) 	at com.google.common.base.Joiner.join(Joiner.java:193) 	at com.google.common.base.Joiner.join(Joiner.java:183) 	at org.apache.jackrabbit.oak.plugins.blob.MarkSweepGarbageCollector.saveBatchToFile(MarkSweepGarbageCollector.java:317) 	at org.apache.jackrabbit.oak.plugins.blob.MarkSweepGarbageCollector 1.addReference(MarkSweepGarbageCollector.java:391) 	... 14 common frames omitted {code}  Attached you find the OSGi config for both the nodestore and the datastore.
OAK-016df669$$NodeDocument _modified may go back in time$$In a cluster with multiple DocumentMK instances the _modified field of a NodeDocument may go back in time. This will result in incorrect diff calculations when the DocumentNodeStore uses the _modified field to find changed nodes for a given revision range.
OAK-3e83a4c1$$NodeDocument _modified may go back in time$$In a cluster with multiple DocumentMK instances the _modified field of a NodeDocument may go back in time. This will result in incorrect diff calculations when the DocumentNodeStore uses the _modified field to find changed nodes for a given revision range.
OAK-ca36450e$$IllegalStateException when using "lowerCase"/"lower" on a array property$$if query contain lowerCase on array property then QueryResult.getRows() throwing  IllegalStateException.  Query which causing issue   select [selector_1].* from [nt:unstructured] AS [selector_1] where (([selector_1].[lcc:className] = 'com.adobe.icc.dbforms.obj.ConditionalDataModule')) AND (LOWER([selector_1].[dataDictionaryRefs]) = 'employeedd')  If we remove LOWER function then it is working    select [selector_1].* from [nt:unstructured] AS [selector_1] where (([selector_1].[lcc:className] = 'com.adobe.icc.dbforms.obj.ConditionalDataModule')) AND ([selector_1].[dataDictionaryRefs] = 'EmployeeDD')
OAK-093b9128$$Default sync handler property mapping does not allow constant properties$$it would be useful, if the default sync handler user (and group) mapping could also handle constant properties and use given primary type and mixin type information. eg:  {noformat} profile/nt:primaryType="sling:Folder" profile/sling:resourceType="sling/security/profile" {noformat}
OAK-7fe28a0e$$Trying to remove a missing property throws PathNotFoundException$$The following code snippet throws a {{PathNotFoundException}} if the "missing" property is not present.  {code:java} node.setProperty("missing", (String) null); {code}  A better way to handle such a case would be for the above statement to simply do nothing.
OAK-3ae276c1$$Indexes: re-index automatically when adding an index$$When adding an index via import of content, the index is not automatically re-built. This is problematic, because subsequent queries will return no data because of that. Currently, the only way to re-index is to set the "reindex" property to "true".  I suggest that indexes are automatically re-indexes if the hidden child node (":data" I believe) is missing. This is in addition to the "reindex" property.
OAK-716e1237$$Hourly async reindexing on an idle instance$$OAK-1292 introduced the following interesting but not very nice behavior:  On an idle system with no changes for an extended amount of time, the OAK-1292 change blocks the async indexer from updating the reference to the last indexed checkpoint. After one hour (the default checkpoint lifetime), the referenced checkpoint will expire, and the indexer will fall back to full reindexing.  The result of this behavior is that once every hour, the size of an idle instance will grow with dozens or hundreds of megabytes of new index data generated by reindexing. Older index data becomes garbage, but the compaction code from OAK-1804 is needed to make it collectable. A better solution would be to prevent the reindexing from happening in the first place.
OAK-9c2421ed$$Unnecessary invocations of LastRevRecovery when recovery already done.$$Even after _lastRev recovery executed on a cluster node, there are unnecessary  invocations of recovery happening on that cluster node, till that cluster node comes online again.
OAK-35562cce$$PropertyIndex only considers the cost of a single indexed property$$The existing PropertyIndex loops through the PropertyRestriction objects in the Filter and essentially only calculates the cost of the first indexed property. This isn't actually the first property in the query and Filter.propertyRestrictions is a HashMap.  More confusingly, the plan for a query with multiple indexed properties outputs *all* indexed properties, even though only the first one is used.  For queries with multiple indexed properties, the cheapest property index should be used in all three relevant places: when calculating the cost, when executing the query, and when producing the plan.
OAK-b6f89048$$Ordered index fails with old index content$$With the latest changes, the ordered index no longer works with old index data. When running the latest Oak 1.0.2 snapshot run against an Oak 1.0.0 repository with an existing ordered index, the index fails with the exception below.  As a workaround, the ordered index can be manually re-built. Either the index re-build needs to be automatic, or the ordered index needs to work with the old index content.  {noformat} java.lang.IndexOutOfBoundsException: index (3) must be less than size (1)     at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:306)     at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:285)     at org.apache.jackrabbit.oak.plugins.segment.SegmentPropertyState.getValue(SegmentPropertyState.java:157)     at org.apache.jackrabbit.oak.plugins.index.property.strategy.OrderedContentMirrorStoreStrategy.getPropertyNext(OrderedContentMirrorStoreStrategy.java:1024) {noformat}
OAK-df59fb45$$Ordered index fails with old index content$$With the latest changes, the ordered index no longer works with old index data. When running the latest Oak 1.0.2 snapshot run against an Oak 1.0.0 repository with an existing ordered index, the index fails with the exception below.  As a workaround, the ordered index can be manually re-built. Either the index re-build needs to be automatic, or the ordered index needs to work with the old index content.  {noformat} java.lang.IndexOutOfBoundsException: index (3) must be less than size (1)     at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:306)     at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:285)     at org.apache.jackrabbit.oak.plugins.segment.SegmentPropertyState.getValue(SegmentPropertyState.java:157)     at org.apache.jackrabbit.oak.plugins.index.property.strategy.OrderedContentMirrorStoreStrategy.getPropertyNext(OrderedContentMirrorStoreStrategy.java:1024) {noformat}
OAK-705ce1d1$$NodeStoreKernel doesn't handle array properties correctly$${{NodeStoreKernel}} currently only supports array properties of type long. For other types it will fail with an {{IllegalStateException}}. See also the FIXME in the code.
OAK-9225a3e2$$UnmergedBranch state growing with empty BranchCommit leading to performance degradation$$In some cluster deployment cases it has been seen that in memory state of UnmergedBranches contains large number of empty commits. For e.g. in  one of of the runs there were 750 entries in the UnmergedBranches and each Branch had empty branch commits.  If there are large number of UnmergedBranches then read performance would degrade as for determining revision validity currently logic scans all branches  Below is some part of UnmergedBranch state  {noformat} Branch 1 1 -> br146d2edb7a7-0-1 (true) (revision: "br146d2edb7a7-0-1", clusterId: 1, time: "2014-06-25 05:08:52.903", branch: true) 2 -> br146d2f0450b-0-1 (true) (revision: "br146d2f0450b-0-1", clusterId: 1, time: "2014-06-25 05:11:40.171", branch: true) Branch 2 1 -> br146d2ef1d08-0-1 (true) (revision: "br146d2ef1d08-0-1", clusterId: 1, time: "2014-06-25 05:10:24.392", branch: true) Branch 3 1 -> br146d2ed26ca-0-1 (true) (revision: "br146d2ed26ca-0-1", clusterId: 1, time: "2014-06-25 05:08:15.818", branch: true) 2 -> br146d2edfd0e-0-1 (true) (revision: "br146d2edfd0e-0-1", clusterId: 1, time: "2014-06-25 05:09:10.670", branch: true) Branch 4 1 -> br146d2ecd85b-0-1 (true) (revision: "br146d2ecd85b-0-1", clusterId: 1, time: "2014-06-25 05:07:55.739", branch: true) Branch 5 1 -> br146d2ec21a0-0-1 (true) (revision: "br146d2ec21a0-0-1", clusterId: 1, time: "2014-06-25 05:07:08.960", branch: true) 2 -> br146d2ec8eca-0-1 (true) (revision: "br146d2ec8eca-0-1", clusterId: 1, time: "2014-06-25 05:07:36.906", branch: true) Branch 6 1 -> br146d2eaf159-1-1 (true) (revision: "br146d2eaf159-1-1", clusterId: 1, time: "2014-06-25 05:05:51.065", counter: 1, branch: true) Branch 7 1 -> br146d2e9a513-0-1 (true) (revision: "br146d2e9a513-0-1", clusterId: 1, time: "2014-06-25 05:04:26.003", branch: true) {noformat}  [~mreutegg] Suggested that these branch might be for those revision which have resulted in a collision and upon checking it indeed appears to be the case  (value true in brackets above indicate that). Further given the age of such revision it looks like they get populated upon startup itself  *Fix* * Need to check why we need to populate the UnermgedBranch * Possibly implement some purge job which would remove such stale entries
OAK-913c2f53$$TarMK compaction can create mixed segments$$As described in http://markmail.org/message/ujkqdlthudaortxf, commits that occur while the compaction operation is running can make the compacted segments contain references to older data segments, which prevents old data from being reclaimed during cleanup.
OAK-c215b267$$TarMK compaction can create mixed segments$$As described in http://markmail.org/message/ujkqdlthudaortxf, commits that occur while the compaction operation is running can make the compacted segments contain references to older data segments, which prevents old data from being reclaimed during cleanup.
OAK-2e16a983$$Query: UnsupportedOperationException for some combinations of "or" and "and" conditions$$The following query throws an UnsupportedOperationException:  {noformat} select * from [nt:base]    where [a] = 1 and [b] = 2 and [b] = 3 or [c] = 4 {noformat}
OAK-93c1aa40$$AsyncIndexUpdate unable to cope with missing checkpoint ref$$The async index uses a checkpoint reference stored under the _:async_ hidden node as a base for running the index diff. It might happen that this reference is stale (pointing to checkpoints that no longer exist) so the async indexer logs a warning that it will reindex everything and will start its work. The trouble is with the #mergeWithConcurrencyCheck which does not cope well with this scenario. Even if the ref checkpoint is null, it will throw a concurrent update exception which will be logged as a misleading debug log _Concurrent update detected in the async index update_.  Overall the code looks stuck in an endless reindexing loop.  {code} *WARN* [pool-9-thread-1] org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate Failed to retrieve previously indexed checkpoint 569d8847-ebb6-4832-a55f-2b0b1a32ae71; re-running the initial async index update *DEBUG* [pool-9-thread-1] org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate Concurrent update detected in the async index update {code}
OAK-4bfbfcdd$$ContentMirrorStoreStrategy should utilize path restriction when available$$Currently {{ContentStoreMirrorStrategy}} has a mirror of content path under {{:index}}. Yet, while {{query}} (and {{count}}) methods doesn't jump directly into restricted path.  This would be very useful for {{PropertyIndex}} where the queries can be optimized by supplying a path restriction along with an indexed property restriction (I don't know if queries with references would use paths so much though)
OAK-f620b79b$$TokenLoginModule can't handle case insensitive userids$$Login against TokenLoginModule with an userid different in case throws:   javax.security.auth.login.LoginException: Invalid token credentials.
OAK-004db804$$XPath queries with certain combinations of "or" conditions don't use an index$$XPath queries with the following conditions are not converted to "union" SQL-2 queries and therefore don't use an index:  {noformat} /jcr:root/content//*[((@i = '1' or @i = '2') or (@s = 'x')) and (@t = 'a' or @t = 'b')] {noformat}
OAK-e30023ba$$Oak Lucene index doesn't get notified about updates when index is stored on the file system$$It looks like the the lucene IndexTracked class responsible for refreshing the in-memory cache of the lucene index doesn't get the update notification when the index is stored on the file system. This results in searches not working until the next restart
OAK-a0a495f0$$Missing privileges after repository upgrade$$After upgrading from Jackrabbit classic all Oak specific privileges are missing (rep:userManagement, rep:readNodes, rep:readProperties, rep:addProperties, rep:alterProperties, rep:removeProperties, rep:indexDefinitionManagement).  The reason seems to be that the {{PrivilegeInitializer}} is not run during upgrade.
OAK-ca63fdf3$$Missing privileges after repository upgrade$$After upgrading from Jackrabbit classic all Oak specific privileges are missing (rep:userManagement, rep:readNodes, rep:readProperties, rep:addProperties, rep:alterProperties, rep:removeProperties, rep:indexDefinitionManagement).  The reason seems to be that the {{PrivilegeInitializer}} is not run during upgrade.
OAK-4af0d4ee$$ArrayIndexOutOfBoundsException in Segment.getRefId()$$It looks like there is some SegmentMK bug that causes the {{Segment.getRefId()}} to throw an {{ArrayIndexOutOfBoundsException}} in some fairly rare corner cases. The data was originally migrated into oak via the crx2oak tool mentioned here: http://docs.adobe.com/docs/en/aem/6-0/deploy/upgrade.html That tool uses *oak-core-1.0.0* creating an oak instance.  Similar to OAK-1566 this system was using FileDataStore with SegmentNodeStore.  In this case the error is seen when running offline compaction using oak-run-1.1-SNAPSHOT.jar (latest).  {code:none} > java -Xmx4096m -jar oak-run-1.1-SNAPSHOT.jar compact /oak/crx-quickstart/repository/segmentstore Apache Jackrabbit Oak 1.1-SNAPSHOT Compacting /wcm/cq-author/crx-quickstart/repository/segmentstore before [data00055a.tar, data00064a.tar, data00045b.tar, data00005a.tar, data00018a.tar, data00022a.tar, data00047a.tar, data00037a.tar, data00049a.tar, data00014a.tar, data00066a.tar, data00020a.tar, data00058a.tar, data00065a.tar, data00069a.tar, data00012a.tar, data00009a.tar, data00060a.tar, data00041a.tar, data00016a.tar, data00072a.tar, data00048a.tar, data00061a.tar, data00053a.tar, data00038a.tar, data00001a.tar, data00034a.tar, data00003a.tar, data00052a.tar, data00006a.tar, data00027a.tar, data00031a.tar, data00056a.tar, data00035a.tar, data00063a.tar, data00068a.tar, data00008v.tar, data00010a.tar, data00043b.tar, data00021a.tar, data00017a.tar, data00024a.tar, data00054a.tar, data00051a.tar, data00057a.tar, data00059a.tar, data00036a.tar, data00033a.tar, data00019a.tar, data00046a.tar, data00067a.tar, data00004a.tar, data00044a.tar, data00013a.tar, data00070a.tar, data00026a.tar, data00002a.tar, data00011a.tar, journal.log, data00030a.tar, data00042a.tar, data00025a.tar, data00062a.tar, data00023a.tar, data00071a.tar, data00032b.tar, data00040a.tar, data00015a.tar, data00029a.tar, data00050a.tar, data00000a.tar, data00007a.tar, data00028a.tar, data00039a.tar] -> compacting Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: 206 at org.apache.jackrabbit.oak.plugins.segment.Segment.getRefId(Segment.java:191) at org.apache.jackrabbit.oak.plugins.segment.Segment.internalReadRecordId(Segment.java:299) at org.apache.jackrabbit.oak.plugins.segment.Segment.readRecordId(Segment.java:295) at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getTemplateId(SegmentNodeState.java:69) at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getTemplate(SegmentNodeState.java:78) at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getProperties(SegmentNodeState.java:150) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:154) at org.apache.jackrabbit.oak.plugins.segment.Compactor CompactDiff.childNodeAdded(Compactor.java:124) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160) at org.apache.jackrabbit.oak.plugins.segment.Compactor CompactDiff.childNodeAdded(Compactor.java:124) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160) at org.apache.jackrabbit.oak.plugins.segment.Compactor CompactDiff.childNodeAdded(Compactor.java:124) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160) at org.apache.jackrabbit.oak.plugins.segment.Compactor CompactDiff.childNodeAdded(Compactor.java:124) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160) at org.apache.jackrabbit.oak.plugins.segment.Compactor CompactDiff.childNodeAdded(Compactor.java:124) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160) at org.apache.jackrabbit.oak.plugins.segment.Compactor CompactDiff.childNodeAdded(Compactor.java:124) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160) at org.apache.jackrabbit.oak.plugins.segment.Compactor CompactDiff.childNodeAdded(Compactor.java:124) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160) at org.apache.jackrabbit.oak.plugins.segment.Compactor CompactDiff.childNodeAdded(Compactor.java:124) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160) at org.apache.jackrabbit.oak.plugins.segment.Compactor CompactDiff.childNodeAdded(Compactor.java:124) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160) at org.apache.jackrabbit.oak.plugins.segment.Compactor CompactDiff.childNodeAdded(Compactor.java:124) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160) at org.apache.jackrabbit.oak.plugins.segment.Compactor CompactDiff.childNodeAdded(Compactor.java:124) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160) at org.apache.jackrabbit.oak.plugins.segment.Compactor CompactDiff.childNodeAdded(Compactor.java:124) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160) at org.apache.jackrabbit.oak.plugins.segment.Compactor CompactDiff.childNodeAdded(Compactor.java:124) at org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160) at org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:395) at org.apache.jackrabbit.oak.plugins.segment.Compactor.process(Compactor.java:80) at org.apache.jackrabbit.oak.plugins.segment.Compactor.compact(Compactor.java:85) at org.apache.jackrabbit.oak.plugins.segment.file.FileStore.compact(FileStore.java:438) at org.apache.jackrabbit.oak.run.Main.compact(Main.java:311) at org.apache.jackrabbit.oak.run.Main.main(Main.java:133) {code}
OAK-5c4589bd$$Range queries and relative properties resultset should be consistent with JR2$$When running a range query like {{/jcr:root/content/nodes//*[(*/*/*/@prop >= 9)]}} the resultset is not consistent for the same use-case when running in jacrabbit 2.
OAK-daf9a4ef$$RootImplFuzzIT test failures$$As seen in the CI build, {{RootImplFuzzIT}} fails every now and then. This might be because of OAK-174, but there's been quite a bit of other work on the same area, so this could be caused also by something else.  The troublesome seeds as seen in failing CI builds are 1437930918, 206057576, 1638075186, 1705736349, -1856261793 and 569172885.
OAK-c7669f31$$Reindex removes all nodes under index definition node$$Reindex logic in {{IndexUpdate}} removes all child node from index definition node thus removing valid nodes which might be part of index defintion. It should only remove hidden nodes
OAK-a1556c30$$[Ordered Index] Indexing on large content is slow$$Indexing large number of ordered properties is quite slow.  Explore ways of making it faster. The current skip list implementation uses 4 lanes with a probability of 10%. It should be made configurable and the defaults changed.
OAK-5931a4a7$$Non-blocking reindexing doesn't finish properly$$The non blocking reindexer needs to run at least 2 cycles before setting the index definition back to synchronous mode. Currently it is too eager to mark the status as 'done' which confuses the _PropertyIndexAsyncReindex_ mbean into thinking the indexing is over and so skipping the final round that is supposed to do the switch back to sync mode.
OAK-f2740ce1$$Ordered index does not return relative properties for un-restricted indexes$$Even if we specify an index without any restriction to node type; the ordered index does not return any result for relative properties
OAK-29d3d8f1$$Lucene index not created if no node is indexed$$If a Lucene property index is defined for a property which is not present in any of the nodes then {{LuceneIndexWriter}} would create any lucene index for that.  For eg if we have an index of {{foo}} and none of the node has property {{foo}} set in that case {{LuceneIndexWriter}} would not create an {{IndexWriter}} and hence no directory would be created. Later when system performs a query like {{select jcr:path from nt:base where foo = 'bar'}} then {{LucenePropertyIndex}} would not participate in the query as no Lucene index would be found and system would revert to traversal.  As a fix Lucene index should still be created even if it does not contain any document
OAK-a28098fd$$Session.getItem violates JCR Spec$$Session.getItem(path) is supposed to first return a node for the given path, and if no node is found return a property.  The oak implementation returns this in the opposite order.  see attached patch for a possible fix.
OAK-dcadb0e1$$UUID collision check is not does not work in transient space$$I think OAK-1037 broke the system view import.  test case: 1. create a new node with a uuid (referenceable, or new user) 2. import systemview with IMPORT_UUID_COLLISION_REPLACE_EXISTING 3. save()  result: {noformat} javax.jcr.nodetype.ConstraintViolationException: OakConstraint0030: Uniqueness constraint violated at path [/] for one of the property in [jcr:uuid] having value e358efa4-89f5-3062-b10d-d7316b65649e {noformat}  expected: * imported content should replace the existing node - even in transient space.  note: * if you perform a save() after step 1, everything works.
OAK-6dde8e9d$$XPath: Query with mixed full-text, "and", "or" conditions fails$$When performing a query like   {noformat}         //element(*, test:Asset)[             (                 jcr:contains(., 'summer')                 or                 jcr:content/metadata/@tags = 'namespace:season/summer'             ) and                 jcr:contains(jcr:content/metadata/@format, 'image')             ]  {noformat}  The Lucene/Aggregate returns as well nodes that does not match all the criterias.
OAK-08b25cb0$$Lucene Index property definition is ignored if its not in includePropertyNames config$$Lucene index property definition will not be used unless that property is in includePropertyNames config. This enforces including that property in includePropertyNames. includePropertyNames restricts all properties from getting indexed, so user is now enforced to include all properties in includePropertyNames to be indexed.
OAK-e33328e0$$Sling I18N queries not supported by Oak$$The Sling I18N component issues XPath queries like the following:  {code:none} //element(*,mix:language)[fn:lower-case(@jcr:language)='en']//element(*,sling:Message)[@sling:message]/(@sling:key|@sling:message) {code}  Such queries currently fail with the following exception:  {code:none} javax.jcr.query.InvalidQueryException: java.text.ParseException: Query: //element(*,mix:language)[fn:lower-(*)case(@jcr:language)='en']//element(*,sling:Message)[@sling:message]/(@sling:key|@sling:message); expected: (         at org.apache.jackrabbit.oak.jcr.query.QueryManagerImpl.executeQuery(QueryManagerImpl.java:115)         at org.apache.jackrabbit.oak.jcr.query.QueryImpl.execute(QueryImpl.java:85)         at org.apache.sling.jcr.resource.JcrResourceUtil.query(JcrResourceUtil.java:52)         at org.apache.sling.jcr.resource.internal.helper.jcr.JcrResourceProvider.queryResources(JcrResourceProvider.java:262)         ... 54 more Caused by: java.text.ParseException: Query: //element(*,mix:language)[fn:lower-(*)case(@jcr:language)='en']//element(*,sling:Message)[@sling:message]/(@sling:key|@sling:message); expected: (         at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.getSyntaxError(XPathToSQL2Converter.java:704)         at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.read(XPathToSQL2Converter.java:410)         at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.parseExpression(XPathToSQL2Converter.java:336)         at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.parseCondition(XPathToSQL2Converter.java:279)         at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.parseAnd(XPathToSQL2Converter.java:252)         at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.parseConstraint(XPathToSQL2Converter.java:244)         at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.convert(XPathToSQL2Converter.java:153)         at org.apache.jackrabbit.oak.query.QueryEngineImpl.parseQuery(QueryEngineImpl.java:86)         at org.apache.jackrabbit.oak.query.QueryEngineImpl.executeQuery(QueryEngineImpl.java:99)         at org.apache.jackrabbit.oak.query.QueryEngineImpl.executeQuery(QueryEngineImpl.java:39)         at org.apache.jackrabbit.oak.jcr.query.QueryManagerImpl.executeQuery(QueryManagerImpl.java:110) {code}
OAK-0ac7ff20$$TarMK Cold Standby can corrupt bulk segments$$There's a race condition on the segment transfer code that may introduce corrupted binary segments on the secondary instance. What can happen during the head sync phase is that the master may send the head segment twice which will make the client receive&store the second segment thinking it's a different one.
OAK-57bd2dc5$$DocumentNS may expose branch commit on earlier revision$$The DocumentNodeStore may expose the changes of a branch on a revision earlier than it's commit revision. This only happens when the read revision equals the revision of the not yet merged changes on the branch.
OAK-f4d5bbe1$$Incorrect recovery of _lastRev for branch commit$$The recovery process for _lastRevs is incorrect for branch commits. It propagates the revision of the commit to the branch up to the root node instead of the revision of the merge for the changes.
OAK-ca85ecce$$Released checkpoint can still be retrieved$$The following fails on the 2nd assertion on the MongoMK  {code} assertTrue(store.release(cp)); assertNull(store.retrieve(cp)); {code}  The JavaDoc on the {{release}} method is a bit vague, but I assume it is safe to assume that when it returns {{true}} the checkpoint should be gone. If not, we should update the JavaDoc.
OAK-1d08cbd3$$DocumentNodeStore.diffManyChildren() reads too many nodes$$DocumentNodeStore.diffManyChildren() compares too many nodes when running in a non-clustered setup and there are many changes below a location with 'many' children.  This is a regression introduced by OAK-2232. The fix changed the way how the minimum revision is calculated based on the two revisions to compare. The seen-at revision of the RevisionComparator is taken into account. However, in a single cluster node setup, the revision range for the current clusterId is never updated. This means the minimum revision is calculated too far back and causes queries with too many nodes than necessary.
OAK-408a566e$$Field boost not working if the property for indexing is picked using aggregate index rules$$For below index definition -  {code} {      jcr:primaryType:"oak:QueryIndexDefinition",    compatVersion:2,    type:"lucene",    async:"async",    reindex:false,    reindexCount:12,    aggregates:{         jcr:primaryType:"oak:Unstructured",       app:Asset:{            jcr:primaryType:"oak:Unstructured",          include0:{               jcr:primaryType:"oak:Unstructured",             path:"jcr:content/metadata/*"          }       }    },    indexRules:{         jcr:primaryType:"nt:unstructured",       app:Asset:{            jcr:primaryType:"nt:unstructured",          properties:{               jcr:primaryType:"nt:unstructured",             foo:{                  jcr:primaryType:"nt:unstructured",                nodeScopeIndex:true,                ordered:true,                propertyIndex:true,                name:"jcr:content/metadata/foo",                type:"Long",                boost:3,                nodeName:"foo"             }          }       }    } } {code}  On executing query of form -   {code} //element(*, app:Asset)  [     jcr:contains(., 'bar' ) ] {code}  should boost the results containing property - 'jcr:content/metadata/foo', but its ignoring index time boosting for it.
OAK-d0f6715d$$NodeDocument.getNodeAtRevision() may read too many revisions$$This is a regression introduced by OAK-1972.  The revision returned with the value may be different from the revision of the change when the change was first committed to a branch and later merged. In this case the value will return the merge revision. The check in getNodeAtRevision() introduced with OAK-1972 then assumes there may be more recent changes in a previous document and starts to scan the revision history. This scan depends on the number of changes that have been applied on the document since the most recent change on the property in question.
OAK-a0dc4c89$$Diff reads too many nodes$$DocumentNodeStore.diffManyChildren() may read too many nodes when there is an inactive cluster node with an old _lastRev on the root document. This is a regression introduced with the fix for OAK-2232.  The fix assumes an inactive cluster node does not have a revision range with an old revision seen at a current timestamp. The DocumentNodeStore will in fact purge revisions from the range in the RevisionComparator after an hour. But on startup the first background read may populate the RevisionComparator with a revision, which is potentially very old (e.g. if the clusterId is not used anymore).
OAK-74f22886$$TarMK Cold Standby expose standby read timeout value$$Running into a read timeout on the standby instance logs some uncaught error: {code} org.apache.jackrabbit.oak.plugins.segment.standby.client.SegmentLoaderHandler Exception caught, closing channel. io.netty.handler.timeout.ReadTimeoutException: null {code}  I'm not sure how/if I need to fix this, the sync process will pickup again, but we can expose the timeout value, so if the network connection is known to be poor, a client can increase the timeout to work around this issue.
OAK-b3071839$$read is inefficient when there are many split documents$$As reported in OAK-2358 there is a potential problem with revisionGC not cleaning up split documents properly (in 1.0.8.r1644758 at least).   As a side-effect, having many garbage-revisions renders the diffImpl algorithm to become very slow - normally it would take only a few millis, but with nodes that have many split-documents I can see diffImpl take hundres of millis, sometimes up to a few seconds. Which causes the observation dequeuing to be slower than the rate in which observation events are enqueued, which results in observation queue never being cleaned up and event handling being delayed more and more.  Adding some logging showed that diffImpl would often read many split-documents, which supports the assumption that the revisionGC not cleaning up revisions has the diffImpl-slowness as a side-effect. Having said that - diffImpl should probably still be able to run fast, since all the revisions it should look at should be in the main document, not in split documents.  I dont have a test case handy for this at the moment unfortunately - if more is coming up, I'll add more details here.
OAK-90ea7aa5$$NPE in DocumentNodeStore#retrieve for non existing checkpoint$$Said method throws a NPE when passing it a valid revision identifier from a non existing checkpoint.
OAK-487de751$$Possibility of overflow in file length calculation$$In OakDirectory the length of a file is calculated in following way  {code:title=OakDirectory|linenumbers=true}         public OakIndexFile(String name, NodeBuilder file) {             ...             this.blobSize = determineBlobSize(file);             this.blob = new byte[blobSize];              PropertyState property = file.getProperty(JCR_DATA);             if (property != null && property.getType() == BINARIES) {                 this.data = newArrayList(property.getValue(BINARIES));             } else {                 this.data = newArrayList();             }              this.length = data.size() * blobSize;             if (!data.isEmpty()) {                 Blob last = data.get(data.size() - 1);                 this.length -= blobSize - last.length();             } {code}  In above calculation its possible to have an overflow in  bq. this.length = data.size() * blobSize;  As multiplication of two integers result in an integer [1]  [1] http://stackoverflow.com/questions/12861893/casting-result-of-multiplication-two-positive-integers-to-long-is-negative-value
OAK-0fa892b3$$issues with JsopBuilder.encode and .escape$$1) escape() escapes many characters that do not need to be escaped (>127)  2) encode() does not encode many control characters that would need to be escaped when read through a JSON parser.
OAK-7c320b1e$$issues with JsopBuilder.encode and .escape$$1) escape() escapes many characters that do not need to be escaped (>127)  2) encode() does not encode many control characters that would need to be escaped when read through a JSON parser.
OAK-8079f7b5$$issues with JsopBuilder.encode and .escape$$1) escape() escapes many characters that do not need to be escaped (>127)  2) encode() does not encode many control characters that would need to be escaped when read through a JSON parser.
OAK-24ce6788$$ValueFactory: Missing identifier validation when creating (weak)reference value from String$$the JCR specification mandates validation of the identifier during value conversion from STRING to REFERENCE or WEAK_REFERENCE:  <quote from 3.6.4.1 From STRING To)> REFERENCE or WEAKREFERENCE: If the string is a syntactically valid  identifier, according to the implementation, it is converted directly, otherwise a  ValueFormatException is thrown. The identifier is not required to be that of an  existing node in the current workspace.  <end_quote>  the current ValueFactory implementation in oak-jcr lacks that validation: creating a REFERENCE or WEAKREFERENCE value using ValueFactory#createValue(String, int) succeeds even if the specified string isn't a valid referenceable node identifier.
OAK-039f892d$$int overflow with orderby causing huge slowdown$$Consider the following query: {code} //element(*,slingevent:Job) order by @slingevent:created ascending {code} this query - when running with a large number of slingevent:Job around - will take a very long time due to the fact, that FilterIterators.SortIterator.init() in the following loop: {code} if (list.size() > max * 2) {   // remove tail entries right now, to save memory   Collections.sort(list, orderBy);   keepFirst(list, max); } {code} does a multiplication with 'max', which is by default set to Integer.MAX_VALUE (see FilterIterators.newCombinedFilter). This results in max *2 to overflow (result is -2) - thus that init-loop will sort the list for every additional entry. Which is definitely not the intention.
OAK-24cb1908$$DocumentNodeStore revision GC may lead to NPE$$The DocumentNodeStore revision GC may cause a NPE in a reader thread when the GC deletes documents currently accessed by the reader. The {{docChildrenCache}} is invalidated in {{VersionGarbageCollector.collectDeletedDocuments()}} after documents are removed in the DocumentStore. The NPE may occur if removed documents are access in between.
OAK-920f32d0$$[LucenePropertyIndex] full-text search on first level relative node returns no result$$Following query does not return any result even with a proper index defined [1]. {noformat}//element(*, test:Page)[ " +             "jcr:contains(jcr:content, 'summer') ] {noformat}  [1] {code} {   "jcr:primaryType": "oak:QueryIndexDefinition",   "compatVersion": 2,   "name": "pageIndex",   "type": "lucene",   "async": "async",   "reindex": true,   "aggregates": {     "jcr:primaryType": "nt:unstructured",     "test:Page": {       "jcr:primaryType": "nt:unstructured",       "include0": {         "jcr:primaryType": "nt:unstructured",         "relativeNode": true,         "path": "jcr:content"       }     }   },   "indexRules": {     "jcr:primaryType": "nt:unstructured",     "test:Page": {       "jcr:primaryType": "nt:unstructured",       "properties": {         "jcr:primaryType": "nt:unstructured",         "jcr:lastModified": {           "jcr:primaryType": "nt:unstructured",           "ordered": true,           "propertyIndex": true,           "name": "jcr:content/jcr:lastModified",           "type": "Date"         }       }     }   } } {code}
OAK-e6d4f9a6$$XPath to SQL-2 conversion fails due to escaping error$$The problem is that the comment is not properly escaped (a C-style comment), so that "*/" in the XPath query accidentally ends the comment in the SQL-2 query.  The following query can't be converted to SQL-2, because it contains "*/":  {noformat} /jcr:root/etc//*[@type = 'product'  and ((@size = 'M' or */@size= 'M' or */*/@size = 'M'  or */*/*/@size = 'M' or */*/*/*/@size = 'M' or */*/*/*/*/@size = 'M'))] {noformat}  I think this was introduced by OAK-2354  http://svn.apache.org/viewvc?view=revision&amp;revision=1645616
OAK-be3a9114$$TARMK Cold Standby size increase due to checkpoints copy$$The current sync design gets confused by existing checkpoints and tries to copy them by value, bypassing the current storage optimization where there are a lot of references to existing content. this can result in a considerable size increase on the standby.
OAK-7fca85bf$$IllegalStateException for ValueMap on _revisions$$An IllegalStateException may be thrown by the MergeSortedIterator when _revisions on the root document are read with the ValueMap implementation. It only happens when the local _revisions map has entries that are lower than the most recent split document.
OAK-8159fc21$$Lucene AND query with a complex OR phrase returns incorrect result$$Queries like this {noformat}/jcr:root/content//element(*, test:Asset)[(jcr:contains(., 'cube')) and (jcr:contains(jcr:content/@foo, '"a" OR "b"'))] {noformat} returns wrong results.  This get converted to {noformat}+:fulltext:cube full:jcr:content/foo:"a" full:jcr:content/foo:"b" {noformat}
OAK-7e250001$$UpdateOp.Key.equals() incorrect$$As reported on the dev list [0], the equals implementation of UpdateOp.Key is incorrect.  [0] http://markmail.org/message/acpg2mhbxjn4lglu
OAK-beaca1a4$$IndexPlanner returning plan for queries involving jcr:score$$Consider a query like   {noformat} /jcr:root//element(*, cq:Taggable)[ (@cq:tags = 'geometrixx-outdoors:activity/biking' or @cq:tags = '/etc/tags/geometrixx-outdoors/activity/biking') ] order by @jcr:score descending  {noformat}  And a seemingly non related index like  {noformat} /oak:index/assetType   ...   - type = "lucene"   + indexRules     + nt:base       + properties         + assetType           - propertyIndex = true           - name = "assetType" {noformat}  Then currently {{IndexPlanner}} would return a plan because even when it cannot evaluate any of property restrictions because it thinks it can sort on {{jcr:score}}. This later results in an exception like  {noformat} 14.01.2015 16:16:35.866 *ERROR* [0:0:0:0:0:0:0:1 [1421248595863] POST /bin/tagcommand HTTP/1.1] org.apache.sling.engine.impl.SlingRequestProcessorImpl service: Uncaught Throwable java.lang.IllegalStateException: No query created for filter Filter(query=select [jcr:path], [jcr:score], * from [cq:Taggable] as a where [cq:tags] in('geometrixx-outdoors:activity/swimming', '/etc/tags/geometrixx-outdoors/activity/swimming') and isdescendantnode(a, '/') order by [jcr:score] desc /* xpath: /jcr:root//element(*, cq:Taggable)[ (@cq:tags = 'geometrixx-outdoors:activity/swimming' or @cq:tags = '/etc/tags/geometrixx-outdoors/activity/swimming') ] order by @jcr:score descending */, path=//*, property=[cq:tags=in(geometrixx-outdoors:activity/swimming, /etc/tags/geometrixx-outdoors/activity/swimming)]) 	at org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex.getQuery(LucenePropertyIndex.java:505) 	at org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex.access 200(LucenePropertyIndex.java:158) 	at org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex 1.loadDocs(LucenePropertyIndex.java:303) 	at org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex 1.computeNext(LucenePropertyIndex.java:261) 	at org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex 1.computeNext(LucenePropertyIndex.java:253) 	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143) {noformat}
OAK-ea7a6199$$NoSuchElementException thrown by NodeDocument$$Following error is seen with latest 1.0.9-SNAPSHOT builds on some system  {noformat} Caused by: java.util.NoSuchElementException: null         at java.util.TreeMap.key(TreeMap.java:1221)         at java.util.TreeMap.firstKey(TreeMap.java:285)         at java.util.Collections UnmodifiableSortedMap.firstKey(Collections.java:1549)         at com.google.common.collect.ForwardingSortedMap.firstKey(ForwardingSortedMap.java:73)         at org.apache.jackrabbit.oak.plugins.document.NodeDocument.getNodeAtRevision(NodeDocument.java:819)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.readNode(DocumentNodeStore.java:930) {noformat}  Most likely the above occurs because a {{TreeMap}} associated with some key in NodeDocument is empty.  {noformat} 23.01.2015 01:57:23.308 *WARN* [pool-11-thread-5]org.apache.jackrabbit.oak.plugins.observation.NodeObserver Error whiledispatching observation eventscom.google.common.util.concurrent.UncheckedExecutionException:com.google.common.util.concurrent.UncheckedExecutionException:java.util.NoSuchElementException         at com.google.common.cache.LocalCache Segment.get(LocalCache.java:2199)         at com.google.common.cache.LocalCache.get(LocalCache.java:3932)         at com.google.common.cache.LocalCache LocalManualCache.get(LocalCache.java:4721)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.getChildren(DocumentNodeStore.java:731)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.diffImpl(DocumentNodeStore.java:1666)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.access 200(DocumentNodeStore.java:105)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore 7.call(DocumentNodeStore.java:1260)         at org.apache.jackrabbit.oak.plugins.document.MongoDiffCache.getChanges(MongoDiffCache.java:88)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.diffChildren(DocumentNodeStore.java:1255)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeState.compareAgainstBaseState(DocumentNodeState.java:260)         at org.apache.jackrabbit.oak.plugins.observation.EventGenerator Continuation.run(EventGenerator.java:172)         at org.apache.jackrabbit.oak.plugins.observation.EventGenerator.generate(EventGenerator.java:118)         at org.apache.jackrabbit.oak.plugins.observation.NodeObserver.contentChanged(NodeObserver.java:156)         at org.apache.jackrabbit.oak.spi.commit.BackgroundObserver 1 1.call(BackgroundObserver.java:117)         at org.apache.jackrabbit.oak.spi.commit.BackgroundObserver 1 1.call(BackgroundObserver.java:111)         at java.util.concurrent.FutureTask.run(FutureTask.java:262)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)         at java.util.concurrent.ThreadPoolExecutor Worker.run(ThreadPoolExecutor.java:615)         at java.lang.Thread.run(Thread.java:744) Caused by: com.google.common.util.concurrent.UncheckedExecutionException:java.util.NoSuchElementException         at com.google.common.cache.LocalCache Segment.get(LocalCache.java:2199)         at com.google.common.cache.LocalCache.get(LocalCache.java:3932)         at com.google.common.cache.LocalCache LocalManualCache.get(LocalCache.java:4721)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.getNode(DocumentNodeStore.java:704)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.readChildren(DocumentNodeStore.java:786)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore 4.call(DocumentNodeStore.java:734)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore 4.call(DocumentNodeStore.java:731)         at com.google.common.cache.LocalCache LocalManualCache 1.load(LocalCache.java:4724)         at com.google.common.cache.LocalCache LoadingValueReference.loadFuture(LocalCache.java:3522)         at com.google.common.cache.LocalCache Segment.loadSync(LocalCache.java:2315)         at com.google.common.cache.LocalCache Segment.lockedGetOrLoad(LocalCache.java:2278)         at com.google.common.cache.LocalCache Segment.get(LocalCache.java:2193)        ... 18 common frames omitted Caused by: java.util.NoSuchElementException: null         at java.util.TreeMap.key(TreeMap.java:1221)         at java.util.TreeMap.firstKey(TreeMap.java:285)         at java.util.Collections UnmodifiableSortedMap.firstKey(Collections.java:1549)         at com.google.common.collect.ForwardingSortedMap.firstKey(ForwardingSortedMap.java:73)         at org.apache.jackrabbit.oak.plugins.document.NodeDocument.getNodeAtRevision(NodeDocument.java:819)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.readNode(DocumentNodeStore.java:930)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore 3.call(DocumentNodeStore.java:707)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore 3.call(DocumentNodeStore.java:704)         at com.google.common.cache.LocalCache LocalManualCache 1.load(LocalCache.java:4724)         at com.google.common.cache.LocalCache LoadingValueReference.loadFuture(LocalCache.java:3522)         at com.google.common.cache.LocalCache Segment.loadSync(LocalCache.java:2315)         at com.google.common.cache.LocalCache Segment.lockedGetOrLoad(LocalCache.java:2278)         at com.google.common.cache.LocalCache Segment.get(LocalCache.java:2193) {noformat}
OAK-60186813$$Long overflow in PermissionEntryProviderImpl$$PermissionEntryProviderImpl#init can end up in a Long overflow if the underlying implementation does not know the exact value of the number children, and the child node count is higher than maxSize.  I will attach a patch with a test case
OAK-977a31d8$$Error while configuring analyzer by composition$$Error while creating analyzer by composition from osgi due to an illegal argument {{jcr:primaryType}} passed to {{TokenizerFactory.forName(clazz, args)}} in {{NodeStateAnalyzerFactory.loadTokenizer()}}  {noformat} Caused by: java.lang.IllegalArgumentException: Unknown parameters: {jcr:primaryType=nt:unstructured} 	at org.apache.lucene.analysis.core.LowerCaseFilterFactory.<init>(LowerCaseFilterFactory.java:45) {noformat}
OAK-239de7b8$$Entries in _commitRoot not purged$$Entries in _commitRoot are not purged or moved to previous documents if there are no changes with those revisions. Usually there is always a change associated with a _commitRoot, but in some cases it may happen that the only update on the document is for non-revisioned data like the _children flag.
OAK-dfa87520$$Lucene index rules should be case insensitive$$Following the lucene index definitions update, the ignored properties are upgraded as a lower case version, but the rest of the lucene bits (indexing) still take the case into account, resulting in the exclude rules being ignored, and properties being indexed.
OAK-77d2d3b0$$Failure in one of the batch in VersionGC might lead to orphaned nodes$$VersionGC logic currently performs deletion of nodes in batches. For GC to work properly NodeDocument should always be removed in bottom-up mode i.e. parent node should be removed *after* child has been removed  Currently the GC logic deletes the NodeDocument in undefined order. In such mode if one of the batch fails then its possible that parent might have got deleted but the child was not deleted.   Now in next run the child node would not be recognized as a deleted node because the commit root would not be found.
OAK-36fe017c$$DocumentNodeStore.dispose() may leave repository in an inconsistent state$$The repository may become inconsistent when a commit happens while the DocumentNodeStore is disposed.   The node store writes back pending _lastRevs and then unset the active flag in the clusterNodes collection. It is possible a commit gets through even after the _lastRevs had been updated and the active flag is cleared. This means the missing _lastRev updates will not be recovered on a restart or by another cluster node.
OAK-72d24f4b$$IndexCopier might create empty files in case of error occuring while copying$$On some of the setups following logs are seen {noformat} error.log:12.03.2015 03:53:59.785 *WARN* [pool-5-thread-90] org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier Found local copy for _2uv.cfs in MMapDirectory@/mnt/installation/crx-quickstart/repository/index/e5a943cdec3000bd8ce54924fd2070ab5d1d35b9ecf530963a3583d43bf28293/1 lockFactory=NativeFSLockFactory@/mnt/installation/crx-quickstart/repository/index/e5a943cdec3000bd8ce54924fd2070ab5d1d35b9ecf530963a3583d43bf28293/1 but size of local 0 differs from remote 1070972. Content would be read from remote file only error.log:12.03.2015 03:54:02.883 *WARN* [pool-5-thread-125] org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier Found local copy for _2rr.si in MMapDirectory@/mnt/installation/crx-quickstart/repository/index/43b36b107f8ce7e162c15b22508aa457ff6ae0083ed3e12d14a7dab67f886def/1 lockFactory=NativeFSLockFactory@/mnt/installation/crx-quickstart/repository/index/43b36b107f8ce7e162c15b22508aa457ff6ae0083ed3e12d14a7dab67f886def/1 but size of local 0 differs from remote 240. Content would be read from remote file only error.log:12.03.2015 03:54:03.467 *WARN* [pool-5-thread-132] org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier Found local copy for _2ro_3.del in MMapDirectory@/mnt/installation/crx-quickstart/repository/index/43b36b107f8ce7e162c15b22508aa457ff6ae0083ed3e12d14a7dab67f886def/1 lockFactory=NativeFSLockFactory@/mnt/installation/crx-quickstart/repository/index/43b36b107f8ce7e162c15b22508aa457ff6ae0083ed3e12d14a7dab67f886def/1 but size of local 0 differs from remote 42. Content would be read from remote file only error.log:12.03.2015 03:54:03.737 *WARN* [pool-5-thread-135] org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier Found local copy for _2rm_2.del in MMapDirectory@/mnt/installation/crx-quickstart/repository/index/43b36b107f8ce7e162c15b22508aa457ff6ae0083ed3e12d14a7dab67f886def/1 lockFactory=NativeFSLockFactory@/mnt/installation/crx-quickstart/repository/index/43b36b107f8ce7e162c15b22508aa457ff6ae0083ed3e12d14a7dab67f886def/1 but size of local 0 differs from remote 35. Content would be read from remote file only {noformat}  They indicate that copier has created files of size 0. Looking at the code flow this can happen in case while starting copying some error occurs in between. {{org.apache.lucene.store.Directory#copy}} do take care of removing the file in case of error but that is done only for IOException and not for other cases.  As a fix the logic should ensure that local file gets deleted if the copy was not successful
OAK-d2da7499$$Blob GC throws NPE$$Blob GC when registered without a shared data store throws NPE. The {{ClusterRepositoryInfo#getId}} method should check if clusterId is registered or not.
OAK-0598498e$$DocumentNodeStore.dispatch() may pass null to NodeStateDiff$$This is a regression introduced by OAK-2562. The dispatch method passes a null state if the node does not exist at a given revision.
OAK-429baf4d$$TreeTypeProvider treats optimized node type definition info as Ac-Content$$while investigating a bug reported by [~teofili] and [~mpetria] that cause group-import with policy node to fail when run with non-administrative session, i found that the {{TreeTypeProvider}} wrongly identifies the optimized item definition information stored with the node types (e.g. {{/jcr:system/jcr:nodeTypes/rep:AccessControllable/rep:namedChildNodeDefinitions/rep:policy}} ) as access control content and thus doesn't read it properly when using a session that doesn't have jcr:readAccessControl privilege at /jcr:system/jcr:nodeTypes.  the effect of this bug is as follows: the internal calculation of the effective node type and thus item definitions will not work properly for {{rep:policy}} nodes (and similar) as the editing session cannot read the full (oak internal) node type definition as stored below {{/jcr:system/jcr:nodeTypes}}.
OAK-1bf5c550$$potential clash of commit id's after restart$$the commit id's in the current implementation are counter-based, i.e. every commit (on HEAD or on a branch) gets its id by incrementing counter.  only the current HEAD id is recorded/persisted. on startup the counter is initialized with the current HEAD id.   assume the following sequence:  - ...startup... - counter == HEAD == 99 - commit on HEAD -> new HEAD rev: ++counter == 100 - create branch -> new branch rev: ++counter == 101 - ...restart... - counter == HEAD == 100 - commit on HEAD -> new HEAD rev: ++counter == 101 => clashes with older branch rev!   since a commit is never overwritten the above scenario results in a private branch revision marked as HEAD, i.e. the revision history is corrupted.
OAK-db19e70f$$Tree.getStatus() and Tree.getPropertyStatus() fail for items whose parent has been removed$$None
OAK-3979fa8d$$OakIndexInput cloned instances are not closed$$Related to the inspections I was doing for OAK-2798 I also noticed that we don't fully comply with the {{IndexInput}} javadoc [1] as the cloned instances should throw the given exception if original is closed, but I also think that the original instance should close the cloned instances, see also [ByteBufferIndexInput#close|https://github.com/apache/lucene-solr/blob/lucene_solr_4_7_1/lucene/core/src/java/org/apache/lucene/store/ByteBufferIndexInput.java#L271].  [1] : {code} /** Abstract base class for input from a file in a {@link Directory}.  A  * random-access input stream.  Used for all Lucene index input operations.  *  * <p>{@code IndexInput} may only be used from one thread, because it is not  * thread safe (it keeps internal state like file position). To allow  * multithreaded use, every {@code IndexInput} instance must be cloned before  * used in another thread. Subclasses must therefore implement {@link #clone()},  * returning a new {@code IndexInput} which operates on the same underlying  * resource, but positioned independently. Lucene never closes cloned  * {@code IndexInput}s, it will only do this on the original one.  * The original instance must take care that cloned instances throw  * {@link AlreadyClosedException} when the original one is closed. {code}
OAK-f51ea2a2$$XPath backwards compatibility issue with false() and true()$$In JR2 (actually CRX 2) both of the following queries for nodes with a boolean property can be parsed, however only query (a) returns search results. {noformat}     (a) /jcr:root/test//*[@foo = true()]     (b) /jcr:root/test//*[@foo = true] {noformat}  On Oak 1.2, query (a) results in an exception\[0\] and query (b) returns search results.  See discussion at http://markmail.org/thread/kpews55jpdwm62ds
OAK-a2950285$$Parent of unseen children must not be removable$$With OAK-2673, it's now possible to have hidden intermediate nodes created concurrently. So, a scenario like: {noformat} start -> /:hidden N1 creates /:hiddent/parent/node1 N2 creates /:hidden/parent/node2 {noformat} is allowed.  But, if N2's creation of {{parent}} got persisted later than that on N1, then N2 is currently able to delete {{parent}} even though there's {{node1}}.
OAK-44585b0c$$AccessDenied when modifying transiently moved item with too many ACEs$$If at least the following preconditions are fulfilled, saving a moved item fails with access denied:  1. there are more PermissionEntries in the PermissionEntryCache than the configured EagerCacheSize 2. an node is moved to a location where the user has write access through a group membership 3. a property is added to the transiently moved item  For example: 1. set the *eagerCacheSize* to '0' 2. create new group *testgroup* and user *testuser* 3. make *testuser* member of *testgroup* 4. create nodes {{/testroot/a}} and {{/testroot/a/b}} and {{/testroot/a/c}} 5. allow *testgroup* {{rep:write}} on {{/testroot/a}} 6. as *testuser* create {{/testroot/a/b/item}} (to verify that the user has write access) 7. as *testuser* move {{/testroot/a/b/item}} to {{/testroot/a/c/item}} 8. {{save()}} -> works 9. as *testuser* move {{/testroot/a/c/item}} back to {{/testroot/a/b/item}} AND add new property to the transient {{/testroot/a/b/item}} 10. {{save()}} -> access denied
OAK-5449bf39$$PathUtils.isAncestor("/", "/") should return false but returns true$$None
OAK-3bf07779$$Index updation fails on updating multivalued property$$On emptying a multivalued property, fulltext index updation fails and one can search on old values. Following test demonstrates the issue. Added below test in [LuceneIndexQueryTest.java|https://github.com/apache/jackrabbit-oak/blob/trunk/oak-lucene/src/test/java/org/apache/jackrabbit/oak/plugins/index/lucene/LuceneIndexQueryTest.java] which should pass -  {code}     @Test     public void testMultiValuedPropUpdate() throws Exception {         Tree test = root.getTree("/").addChild("test");         String child = "child";         String mulValuedProp = "prop";         test.addChild(child).setProperty(mulValuedProp, of("foo","bar"), Type.STRINGS);         root.commit();         assertQuery(                 "/jcr:root//*[jcr:contains(@" + mulValuedProp + ", 'foo')]",                 "xpath", ImmutableList.of("/test/" + child));         test.getChild(child).setProperty(mulValuedProp, new ArrayList<String>(), Type.STRINGS);         root.commit();         assertQuery(                 "/jcr:root//*[jcr:contains(@" + mulValuedProp + ", 'foo')]",                 "xpath", new ArrayList<String>());          test.getChild(child).setProperty(mulValuedProp, of("bar"), Type.STRINGS);         root.commit();         assertQuery(                 "/jcr:root//*[jcr:contains(@" + mulValuedProp + ", 'foo')]",                 "xpath", new ArrayList<String>());      } {code}
OAK-eabb4066$$SQL2 query with union, limit and offset can return invalid results$$when using order, limit and offset and a SQL2 query that contains an union of two subqueries that have common results can return invalid results  Example: assuming content tree /test/a/b/c/d/e exists {code:sql} SELECT [jcr:path] FROM [nt:base] AS a WHERE ISDESCENDANTNODE(a, '/test') UNION SELECT [jcr:path] FROM [nt:base] AS a WHERE ISDESCENDANTNODE(a, '/test')" ORDER BY [jcr:path] {code} with limit=3 and offset 2 returns only one row ( instead of 3 )  the correct result set is {noformat} /test/a/b/c /test/a/b/c/d /test/a/b/c/d/e {noformat}
OAK-5135cf4b$$VersionablePathHook must not process hidden nodes$$The VersionablePathHook also processes hidden nodes, e.g. index data, which adds considerable overhead to the merge phase.
OAK-147515ae$$Async Update fails after IllegalArgumentException$$The async index update can fail due to a mismatch between an index definition and the actual content. If that is the case, it seems that it can no longer make any progress. Instead it re-indexes the latest changes over and over again until it hits the problematic property.  Discussion at http://markmail.org/thread/42bixzkrkwv4s6tq  Stacktrace attached.
OAK-494da6de$$UserValidator and AccessControlValidator must not process hidden nodes$$This is similar to OAK-3019 but for {{UserValidator}} and {{AccessControlValidator}}.
OAK-89317b28$$Hierarchy conflict detection broken$$Hierarchy conflict detection is broken in 1.0.14. It may happen that a child document is created even though the parent is considered deleted.
OAK-7552a10b$$Locking issues seen with CopyOnWrite mode enabled$$When CopyOnWrite mode is enabled and incremental mode is enabled i.e. {{indexPath}} property set then failure in any indexing cycle would prevent further indexing from progressing. For e.g. if any indexing cycle fails then subsequent indexing cycle would fail with Lucene locking related exception  {noformat} Caused by: org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/tmp/junit8067118705344013640/2c26b46b68ffc68ff99b453c1d30413413422d706483bfa0f98a5e886266e7ae/1/write.lock 	at org.apache.lucene.store.Lock.obtain(Lock.java:89) 	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:707) 	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditorContext.getWriter(LuceneIndexEditorContext.java:169) 	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditor.addOrUpdate(LuceneIndexEditor.java:293) 	... 37 more {noformat}  Any further indexing would continue to fail with this exception
OAK-33c18762$$LastRevRecoveryAgent can update _lastRev of children but not the root$$As mentioned in [OAK-2131|https://issues.apache.org/jira/browse/OAK-2131?focusedCommentId=14616391&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14616391] there can be a situation wherein the LastRevRecoveryAgent updates some nodes in the tree but not the root. This seems to happen due to OAK-2131's change in the Commit.applyToCache (where paths to update are collected via tracker.track): in that code, paths which are non-root and for which no content has changed (and mind you, a content change includes adding _deleted, which happens by default for nodes with children) are not 'tracked', ie for those the _lastRev is not update by subsequent backgroundUpdate operations - leaving them 'old/out-of-date'. This seems correct as per description/intention of OAK-2131 where the last revision can be determined via the commitRoot of the parent. But it has the effect that the LastRevRecoveryAgent then finds those intermittent nodes to be updated while as the root has already been updated (which is at first glance non-intuitive).  I'll attach a test case to reproduce this.  Perhaps this is a bug, perhaps it's ok. [~mreutegg] wdyt?
OAK-29e5b734$$Redundent entries in effective policies per principal-set$$when retrieving the effective policies for a given set of principals the resulting array of policies contains redundant entries if a given policy contains multiple ACEs for the given set of principals.
OAK-ba38c380$$LIRS cache: zero size cache causes IllegalArgumentException$$The LIRS cache does not support a zero size cache currently. Such a configuration causes an IllegalArgumentException.  Instead, no exception should be thrown, and no or a minimum size cache should be used.
OAK-25850476$$Revision GC fails when split documents with very long paths are present$$My company is using the MongoDB microkernel with Oak, and we've noticed that the daily revision GC is failing with errors like this: {code} 13.07.2015 13:06:16.261 *ERROR* [pool-7-thread-1-Maintenance Queue(com/adobe/granite/maintenance/job/RevisionCleanupTask)] org.apache.jackrabbit.oak.management.ManagementOperation Revision garbage collection failed java.lang.IllegalArgumentException: 13:h113f9d0fe7ac0f87fa06397c37b9ffd4b372eeb1ec93e0818bb4024a32587820 at org.apache.jackrabbit.oak.plugins.document.Revision.fromString(Revision.java:236) at org.apache.jackrabbit.oak.plugins.document.SplitDocumentCleanUp.disconnect(SplitDocumentCleanUp.java:84) at org.apache.jackrabbit.oak.plugins.document.SplitDocumentCleanUp.disconnect(SplitDocumentCleanUp.java:56) at org.apache.jackrabbit.oak.plugins.document.VersionGCSupport.deleteSplitDocuments(VersionGCSupport.java:53) at org.apache.jackrabbit.oak.plugins.document.VersionGarbageCollector.collectSplitDocuments(VersionGarbageCollector.java:117) at org.apache.jackrabbit.oak.plugins.document.VersionGarbageCollector.gc(VersionGarbageCollector.java:105) at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreService 2.run(DocumentNodeStoreService.java:511) at org.apache.jackrabbit.oak.spi.state.RevisionGC 1.call(RevisionGC.java:68) at org.apache.jackrabbit.oak.spi.state.RevisionGC 1.call(RevisionGC.java:64) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) {code}  I've narrowed the issue down to the disconnect(NodeDocument) method of the [SplitDocumentCleanUp class|https://svn.apache.org/repos/asf/jackrabbit/oak/trunk/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/document/SplitDocumentCleanUp.java]. The method always tries to extract the path of the node from its ID, but this won't work for documents whose path is very long because those documents will have the hash of their path in the ID.  I believe this code should fix the issue, but I haven't had a chance to actually try it: {code}     private void disconnect(NodeDocument splitDoc) {         String mainId = Utils.getIdFromPath(splitDoc.getMainPath());         NodeDocument doc = store.find(NODES, mainId);         if (doc == null) {             LOG.warn("Main document {} already removed. Split document is {}",                     mainId, splitId);             return;         }         String path = splitDoc.getPath();         int slashIdx = path.lastIndexOf('/');         int height = Integer.parseInt(path.substring(slashIdx + 1));         Revision rev = Revision.fromString(                 path.substring(path.lastIndexOf('/', slashIdx - 1) + 1, slashIdx));         doc = doc.findPrevReferencingDoc(rev, height);         if (doc == null) {             LOG.warn("Split document {} not referenced anymore. Main document is {}",                     splitId, mainId);             return;         }         // remove reference         if (doc.getSplitDocType() == INTERMEDIATE) {             disconnectFromIntermediate(doc, rev);         } else {             markStaleOnMain(doc, rev, height);         }     } {code} By using getPath(), the code should automatically use either the ID or the _path property, whichever is right for the document.
OAK-38f5ef13$$Version garbage collector doesn't collect a rolled back document if it was never deleted$$If a commit gets rolled back it can leave (in case the document was never deleted explicitly) a document in a state like: {noformat} {        "_id" : "7:/etc/workflow/packages/2014/10/12/rep:ours",        "_deleted" : {         },        "_commitRoot" : {         },        "jcr:primaryType" : {         },        "_modified" : NumberLong(1413126245),        "_children" : true,        "_modCount" : NumberLong(2) } {noformat}  If the path is fairly busy, the document can get created naturally later and then follow the usual cycle. But, at times, such documents are ephemeral in nature and never re-used. In those cases, such documents can remain silently without getting collected.
OAK-311e8b33$$SegmentWriter doesn't properly check the length of external blob IDs$$To store the length field of an external binary ID, the following encoding is used:  {noformat} 1110 + 4bit + 8bit {noformat}  which allows to store numbers between 0 and 2{^}12^ - 1.   The current implementation of {{SegmentWriter}} allows the length of binary IDs to range between 0 and 2{^}13^ - 1, writing incorrect data when the length of the binary ID ranges from 2{^}12^ to 2{^}13^ - 1.  When reading this incorrect data back, an {{IllegalStateException}} is thrown complaining that the first byte of the length fields has an unexpected value record type. See OAK-1842 for an example.
OAK-d10362c0$$AsyncIndexer fails due to FileNotFoundException thrown by CopyOnWrite logic$$At times the CopyOnWrite reports following exception  {noformat} 15.07.2015 14:20:35.930 *WARN* [pool-58-thread-1] org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate The async index update failed org.apache.jackrabbit.oak.api.CommitFailedException: OakLucene0004: Failed to close the Lucene index 	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditor.leave(LuceneIndexEditor.java:204) 	at org.apache.jackrabbit.oak.plugins.index.IndexUpdate.leave(IndexUpdate.java:219) 	at org.apache.jackrabbit.oak.spi.commit.VisibleEditor.leave(VisibleEditor.java:63) 	at org.apache.jackrabbit.oak.spi.commit.EditorDiff.process(EditorDiff.java:56) 	at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.updateIndex(AsyncIndexUpdate.java:366) 	at org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.run(AsyncIndexUpdate.java:311) 	at org.apache.sling.commons.scheduler.impl.QuartzJobExecutor.execute(QuartzJobExecutor.java:105) 	at org.quartz.core.JobRunShell.run(JobRunShell.java:207) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) 	at java.util.concurrent.ThreadPoolExecutor Worker.run(ThreadPoolExecutor.java:615) 	at java.lang.Thread.run(Thread.java:745) Caused by: java.io.FileNotFoundException: _2s7.fdt 	at org.apache.lucene.store.FSDirectory.fileLength(FSDirectory.java:261) 	at org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier CopyOnWriteDirectory COWLocalFileReference.fileLength(IndexCopier.java:837) 	at org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier CopyOnWriteDirectory.fileLength(IndexCopier.java:607) 	at org.apache.lucene.index.SegmentCommitInfo.sizeInBytes(SegmentCommitInfo.java:141) 	at org.apache.lucene.index.DocumentsWriterPerThread.sealFlushedSegment(DocumentsWriterPerThread.java:529) 	at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:502) 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:508) 	at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:618) 	at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3147) 	at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3123) 	at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:988) 	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:932) 	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:894) 	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditorContext.closeWriter(LuceneIndexEditorContext.java:192) 	at org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditor.leave(LuceneIndexEditor.java:202) 	... 10 common frames omitted {noformat}
OAK-f3c9c818$$NPE in RecordIdMap$${{RecordIdMap}} is not properly guarded against NPEs when calling accessors on an empty map (which is represented by {{keys == null}}.   {noformat} testRecordIdMap(org.apache.jackrabbit.oak.plugins.segment.RecordIdMapTest)  Time elapsed: 0.019 sec  <<< ERROR! java.lang.NullPointerException at org.apache.jackrabbit.oak.plugins.segment.RecordIdMap.size(RecordIdMap.java:100) at org.apache.jackrabbit.oak.plugins.segment.RecordIdMapTest.testRecordIdMap(RecordIdMapTest.java:64) {noformat}
OAK-c65b07c3$$Global fulltext index returning plan for pure NodeType queries$$On a system having  # Global fulltext index enabled with version V2 and {{evaluatePathRestriction}} enabled # NodeType index having indexing enabled for specific nodetype like cq:ClientLibraryFolder  A query like  {noformat} /jcr:root//element(*, cq:ClientLibraryFolder) {noformat}  Ends up getting evaluated by fulltext index as it return plan with include all query  *Expected* For such query global fulltext index should not return any plan if the path restriction is on root path with include all children
OAK-e115fd90$$Trailing slash not removed for simple path in JCR to Oak path conversion$$While converting from JCR path to Oak path the trailing slashes are not removed for simple paths. They are removed for complex path  {code} assertEquals("/oak-foo:bar/oak-quu:qux",npMapper.getOakPath("/foo:bar/quu:qux/")); assertEquals("/a/b/c",npMapper.getOakPath("/a/b/c/"));     } {code}  Of the two cases above the first one passes while the second one fails
OAK-786b3d76$$Lucene suggestions index definition can't be restricted to a specific type of node$$While performing a suggestor query like   {code} SELECT [rep:suggest()] as suggestion  FROM [nt:unstructured] WHERE suggest('foo') {code}  Suggestor does not provide any result. In current implementation, [suggestions|http://jackrabbit.apache.org/oak/docs/query/lucene.html#Suggestions] in Oak work only for index definitions for {{nt:base}} nodetype. So, an index definition like: {code:xml}     <lucene-suggest         jcr:primaryType="oak:QueryIndexDefinition"         async="async"         compatVersion="{Long}2"         type="lucene">         <indexRules jcr:primaryType="nt:unstructured">             <nt:base jcr:primaryType="nt:unstructured">                 <properties jcr:primaryType="nt:unstructured">                     <description                         jcr:primaryType="nt:unstructured"                         analyzed="{Boolean}true"                         name="description"                         propertyIndex="{Boolean}true"                         useInSuggest="{Boolean}true"/>                 </properties>             </nt:base>         </indexRules>     </lucene-suggest> {code} works, but if we change nodetype to {{nt:unstructured}} like: {code:xml}     <lucene-suggest         jcr:primaryType="oak:QueryIndexDefinition"         async="async"         compatVersion="{Long}2"         type="lucene">         <indexRules jcr:primaryType="nt:unstructured">             <nt:unstructured jcr:primaryType="nt:unstructured">                 <properties jcr:primaryType="nt:unstructured">                     <description                         jcr:primaryType="nt:unstructured"                         analyzed="{Boolean}true"                         name="description"                         propertyIndex="{Boolean}true"                         useInSuggest="{Boolean}true"/>                 </properties>             </nt:base>         </indexRules>     </lucene-suggest> {code} , it won't work.  The issue is that suggestor implementation essentially is passing a pseudo row with path=/.: {code:title=LucenePropertyIndex.java}     private boolean loadDocs() { ...                         queue.add(new LuceneResultRow(suggestedWords)); ... {code} and {code:title=LucenePropertyIndex.java}         LuceneResultRow(Iterable<String> suggestWords) {             this.path = "/";             this.score = 1.0d;             this.suggestWords = suggestWords;         } {code} Due to path being set to "/", {{SelectorImpl}} later filters out the result as {{rep:root}} (primary type of "/") isn't a {{nt:unstructured}}.
OAK-64712735$$Some version copy settings conflicts with the earlyShutdown$$The {{RepositoryUpgrade#earlyShutdown}} property causes the source CRX2 repository to shutdown right after copying the content, before the first commit hook is launched. However, the {{VersionableEditor}} hook sometimes needs access to the source repository, to read the version histories that hasn't been copied yet (as the version histories are copied in two stages). As a result, the {{earlyShutdown}} may cause the upgrade process to fail.  {{earlyShutdown}} should be overriden for all cases in which the source repository is still needed in the commit hook phase. In particular, it should be set to {{false}} if:  * orphaned version histories are not copied, * orphaned version histories are copied, but the copyOrphanedVersion date is set after the copyVersion date.
OAK-4416a9f8$$Write operations on Property do not check checked-out state of Node$$Write operations on Property do not check the checked-out state. The same is true for Node.setProperty(..., null).
OAK-e12e2052$$IndexRule not respecting inheritence based on mixins$$IndexRule are meant to be applied based on both primaryType and minin type based inheritance. Currently it appears that only primaryType based inheritance is working
OAK-5f863af6$$Evaluation with restriction is not consistent with parent ACLs$$consider the following ACL setup:  {noformat} testuser allow rep:read,rep:write      /testroot testuser deny  jcr:removeNode /testroot/a  glob=*/c testuser allow jcr:removeNode /testroot/a  glob=*/b {noformat}  now: {{hasPermission(/tesroot/a/b/c, jcr:removeNode) == false}} but the user is still able to delete the node.  * if we change the order of the ACEs with the restriction, it works (i.e. the user can't delete) * if we use direct ACLs on the respective nodes, it works  I think this is a bug...but I'm not sure if {{hasPermission}} is wrong, or the check during node deletion.
OAK-194999ed$$SplitOperations purges _commitRoot entries too eagerly$$OAK-2528 introduced purging of _commitRoot entries without associated local changes on the document. Those _commitRoot entries are created when a child nodes is added and the _children flag is touched on the parent.  The purge operation is too eager and removes all such entries, which may result in an undetected hierarchy conflict.
OAK-06812d25$$Boosting fields not working as expected$$When the boost support was added the intention was to support a usecase like   {quote} For the fulltext search on a node where the fulltext content is derived from multiple field it should be possible to boost specific text contributed by individual field. Meaning that if a title field is boosted more than description, the title (part) in the fulltext field will mean more than the description (part) in the fulltext field. {quote}  This would enable a user to perform a search like _/jcr:root/content/geometrixx-outdoors/en//element(*, cq:Page)\[jcr:contains(., 'Keyword')\]_ and get a result where pages having 'Keyword' in title come above in search result compared to those where Keyword is found in description.  Current implementation just sets the boost while add the field value to fulltext field with the intention that Lucene would use the boost as explained above. However it does not work like that and boost value gets multiplies with other field and hence boosting does not work as expected
OAK-00b9bc52$$Two spaces in SQL2 fulltext search -> error$$Execute the following SQL2 query (eg, in crx/de's query tool) SELECT * FROM [nt:unstructured] AS c WHERE (CONTAINS(c.[jcr:title], 'a  b') AND ISDESCENDANTNODE(c, '/content')) (note there are 2 spaces between "a" and "b") Result: java.lang.IllegalArgumentException: Invalid expression: 'a b'  If there is only 1 space between a and b, there is no error.   Per jsr-283, fulltext expressions should be able to have strings of whitespace.
OAK-c83755c3$$NPE during syncAllExternalUsers in LdapIdentityProvider.createUser$$When executing the JMX method syncAllExternalUsers the following NPE has been encountered. This likely indicates that - for a particular user - there is no attribute '{{uid}}':  {code} java.lang.NullPointerException at org.apache.jackrabbit.oak.security.authentication.ldap.impl.LdapIdentityProvider.createUser(LdapIdentityProvider.java:667) at org.apache.jackrabbit.oak.security.authentication.ldap.impl.LdapIdentityProvider.access 000(LdapIdentityProvider.java:88) at org.apache.jackrabbit.oak.security.authentication.ldap.impl.LdapIdentityProvider 1.getNext(LdapIdentityProvider.java:281) at org.apache.jackrabbit.oak.security.authentication.ldap.impl.LdapIdentityProvider 1.getNext(LdapIdentityProvider.java:273) at org.apache.jackrabbit.commons.iterator.AbstractLazyIterator.hasNext(AbstractLazyIterator.java:39) at org.apache.jackrabbit.oak.spi.security.authentication.external.impl.jmx.SyncMBeanImpl Delegatee.syncAllExternalUsers(SyncMBeanImpl.java:245) at org.apache.jackrabbit.oak.spi.security.authentication.external.impl.jmx.SyncMBeanImpl.syncAllExternalUsers(SyncMBeanImpl.java:426) {code}
OAK-978c77ff$$Inconsistent read on DocumentNodeStore startup$$This is a regression introduced with OAK-2929. On DocumentNodeStore startup the RevisionComparator of the local instance is initialized with the current _lastRev entries from the other cluster nodes. The external _lastRev entries are 'seenAt' the same revision, which means for those revisions the RevisionComparator will use the clusterId to compare them. This is also described in OAK-3388.  OAK-2929 changed the sequence of revisions to check for conflicts from StableRevisionComparator to RevisionComparator. This makes the conflict check susceptible to the RevisionComparison behaviour described in OAK-3388. Commits may be rejected with a conflict, when there isn't really a conflict.
OAK-2f85bd78$$Node name having non space whitspace chars should not be allowed$$Due to the changes done in OAK-1174 node with non space whitespace chars like '\n', '\r' etc can be created. This is not desirable and also JR2 does not allow such node to be created so check must be added to prevent such a name from getting created.  As discussed in [1] this is regression due to usage of incorrect utility method as part of [2] the fix can be simply using a {{Character#isWhitespace}} instead of {{Character#isSpaceChar}}  [1] http://mail-archives.apache.org/mod_mbox/jackrabbit-oak-dev/201509.mbox/%3CCAHCW-mkkGtxkn%2B9xfXuvMTfgykewjMPsLwrVH%2B00%2BXaBQjA0sg%40mail.gmail.com%3E [2] https://github.com/apache/jackrabbit-oak/commit/342809f7f04221782ca6bbfbde9392ec4ff441c2
OAK-f4349a96$$ClusterNodeInfo does not pick an existing entry on startup$$When the {{DocumentNodeStore}} starts up, it attempts to find an entry that matches the current instance (which is defined by something based on network interface address and the current working directory).  However, an additional check is done when the cluster lease end time hasn't been reached, in which case the entry is skipped (assuming it belongs to a different instance), and the scan continues. When no other entry is found, a new one is created.  So why would we *ever* consider instances with matching instance information to be different? As far as I can tell the answer is: for unit testing.  But...  With the current assignment very weird things can happen, and I believe I see exactly this happening in a customer problem I'm investigating. The sequence is:  1) First system startup, cluster node id 1 is assigned  2) System crashes or was crashed  3) System restarts within the lease time (120s?), a new cluster node id is assigned  4) System shuts down, and gets restarted after a longer interval: cluster id 1 is used again, and system starts {{MissingLastRevRecovery}}, despite the previous shutdown having been clean  So what we see is that the system starts up with varying cluster node ids, and recovery processes may run with no correlation to what happened before.  Proposal:  a) Make {{ClusterNodeInfo.createInstance()}} much more verbose, so that the default system log contains sufficient information to understand why a certain cluster node id was picked.  b) Drop the logic that skips entries with non-expired leases, so that we get a one-to-one relation between instance ids and cluster node ids. For the unit tests that currently rely on this logic, switch to APIs where the test setup picks the cluster node id.
OAK-b76b31f7$$Background update may create journal entry with incorrect id$$The conflict check does not consider changes that are made visible between the rebase and the background read.
OAK-17032c50$$Intermittent IllegalMonitorStateException seen while releaseing IndexNode$$At times following exception seen. On this system the index got corrupted because backing index files got deleted from the system and hence index is not accessible.   {noformat} 21.09.2015 09:26:36.764 *ERROR* [FelixStartLevel] com.adobe.granite.repository.impl.SlingRepositoryManager start: Uncaught Throwable trying to access Repository, calling stopRepository() java.lang.IllegalMonitorStateException: attempt to unlock read lock, not locked by current thread         at java.util.concurrent.locks.ReentrantReadWriteLock Sync.unmatchedUnlockException(ReentrantReadWriteLock.java:444)         at java.util.concurrent.locks.ReentrantReadWriteLock Sync.tryReleaseShared(ReentrantReadWriteLock.java:428)         at java.util.concurrent.locks.AbstractQueuedSynchronizer.releaseShared(AbstractQueuedSynchronizer.java:1341)         at java.util.concurrent.locks.ReentrantReadWriteLock ReadLock.unlock(ReentrantReadWriteLock.java:881)         at org.apache.jackrabbit.oak.plugins.index.lucene.IndexNode.release(IndexNode.java:121)         at org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex.getPlans(LucenePropertyIndex.java:212)         at org.apache.jackrabbit.oak.query.QueryImpl.getBestSelectorExecutionPlan(QueryImpl.java:847)         at org.apache.jackrabbit.oak.query.QueryImpl.getBestSelectorExecutionPlan(QueryImpl.java:793)         at org.apache.jackrabbit.oak.query.ast.SelectorImpl.prepare(SelectorImpl.java:283)         at org.apache.jackrabbit.oak.query.QueryImpl.prepare(QueryImpl.java:568)         at org.apache.jackrabbit.oak.query.QueryEngineImpl.executeQuery(QueryEngineImpl.java:183)         at org.apache.jackrabbit.oak.security.user.UserProvider.getAuthorizableByPrincipal(UserProvider.java:234)         at org.apache.jackrabbit.oak.security.user.UserManagerImpl.getAuthorizable(UserManagerImpl.java:116)         at org.apache.jackrabbit.oak.security.principal.PrincipalProviderImpl.getAuthorizable(PrincipalProviderImpl.java:140)         at org.apache.jackrabbit.oak.security.principal.PrincipalProviderImpl.getPrincipal(PrincipalProviderImpl.java:69)         at org.apache.jackrabbit.oak.spi.security.principal.CompositePrincipalProvider.getPrincipal(CompositePrincipalProvider.java:50)         at org.apache.jackrabbit.oak.spi.security.principal.PrincipalManagerImpl.getPrincipal(PrincipalManagerImpl.java:47)         at com.adobe.granite.repository.impl.SlingRepositoryManager.setupPermissions(SlingRepositoryManager.java:997)         at com.adobe.granite.repository.impl.SlingRepositoryManager.createRepository(SlingRepositoryManager.java:420)         at com.adobe.granite.repository.impl.SlingRepositoryManager.acquireRepository(SlingRepositoryManager.java:290)         at org.apache.sling.jcr.base.AbstractSlingRepositoryManager.start(AbstractSlingRepositoryManager.java:304)         at com.adobe.granite.repository.impl.SlingRepositoryManager.activate(SlingRepositoryManager.java:267)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)         at java.lang.reflect.Method.invoke(Method.java:483)         at org.apache.felix.scr.impl.helper.BaseMethod.invokeMethod(BaseMethod.java:222)         at org.apache.felix.scr.impl.helper.BaseMethod.access 500(BaseMethod.java:37)         at org.apache.felix.scr.impl.helper.BaseMethod Resolved.invoke(BaseMethod.java:615)         at org.apache.felix.scr.impl.helper.BaseMethod.invoke(BaseMethod.java:499)         at org.apache.felix.scr.impl.helper.ActivateMethod.invoke(ActivateMethod.java:295)         at org.apache.felix.scr.impl.manager.SingleComponentManager.createImplementationObject(SingleComponentManager.java:302)         at org.apache.felix.scr.impl.manager.SingleComponentManager.createComponent(SingleComponentManager.java:113)         at org.apache.felix.scr.impl.manager.SingleComponentManager.getService(SingleComponentManager.java:832)         at org.apache.felix.scr.impl.manager.SingleComponentManager.getServiceInternal(SingleComponentManager.java:799)         at org.apache.felix.scr.impl.manager.AbstractComponentManager.activateInternal(AbstractComponentManager.java:724)         at org.apache.felix.scr.impl.manager.DependencyManager SingleStaticCustomizer.addedService(DependencyManager.java:927)         at org.apache.felix.scr.impl.manager.DependencyManager SingleStaticCustomizer.addedService(DependencyManager.java:891)         at org.apache.felix.scr.impl.manager.ServiceTracker Tracked.customizerAdded(ServiceTracker.java:1492)         at org.apache.felix.scr.impl.manager.ServiceTracker Tracked.customizerAdded(ServiceTracker.java:1413)         at org.apache.felix.scr.impl.manager.ServiceTracker AbstractTracked.trackAdding(ServiceTracker.java:1222)         at org.apache.felix.scr.impl.manager.ServiceTracker AbstractTracked.track(ServiceTracker.java:1158)         at org.apache.felix.scr.impl.manager.ServiceTracker Tracked.serviceChanged(ServiceTracker.java:1444)         at org.apache.felix.framework.util.EventDispatcher.invokeServiceListenerCallback(EventDispatcher.java:987)         at org.apache.felix.framework.util.EventDispatcher.fireEventImmediately(EventDispatcher.java:838)         at org.apache.felix.framework.util.EventDispatcher.fireServiceEvent(EventDispatcher.java:545)         at org.apache.felix.framework.Felix.fireServiceEvent(Felix.java:4547)         at org.apache.felix.framework.Felix.registerService(Felix.java:3521)         at org.apache.felix.framework.BundleContextImpl.registerService(BundleContextImpl.java:348)         at org.apache.sling.commons.threads.impl.Activator.start(Activator.java:55)         at org.apache.felix.framework.util.SecureAction.startActivator(SecureAction.java:697)         at org.apache.felix.framework.Felix.activateBundle(Felix.java:2223)         at org.apache.felix.framework.Felix.startBundle(Felix.java:2141)         at org.apache.felix.framework.Felix.setActiveStartLevel(Felix.java:1368)         at org.apache.felix.framework.FrameworkStartLevelImpl.run(FrameworkStartLevelImpl.java:308)         at java.lang.Thread.run(Thread.java:745) {noformat}  Above exception happens at  {code} for (String path : indexPaths) {             try {                 indexNode = tracker.acquireIndexNode(path);                  if (indexNode != null) {                     IndexPlan plan = new IndexPlanner(indexNode, path, filter, sortOrder).getPlan();                     if (plan != null) {                         plans.add(plan);                     }                 }             } finally {                 if (indexNode != null) {                     indexNode.release();                 }             }         } {code}  It has been ensured that if indexNode is initialized then it has been acquired. So only way for such an exception to happen is that in a loop of say 2 paths {{indexNode}} got initialized for Loop 1 and then while acquiring in Loop 2 the indexNode still refers to old released value and that would cause the exception. The fix should be simply to null the variable once released
OAK-ff81ef72$$NodeDocument.getNodeAtRevision can go into property history traversal when latest rev on current doc isn't committed$${{NodeDocument.getNodeAtRevision}} tried to look at latest revisions entries for each property in current document. But it just looks at the *last* entry for a given property. In case this last entry isn't committed, the code would go into previous documents to look for a committed value.  (cc [~mreutegg])
OAK-01f5a26f$$Troublesome ExternalIdentityRef.equals(Object) implementation$$in the light of OAK-3508 i looked at the {{ExternalIdentifyRef}} class and found the following implementation of {{Object.equals(Object)}}:  {code} public boolean equals(Object o) {         try {             // assuming that we never compare other types of classes             return this == o || string.equals(((ExternalIdentityRef) o).string);         } catch (Exception e) {             return false;         }     } {code}  since this class is public and exported as part of a public API, i don't think the assumption made in the code is justified. also i would argue that catching {{Exception}} is bad style as is exception driven development. in this particular case it was IMHO perfectly trivial to just get rid of the catch clause altogether.
OAK-5138a1e2$$Test failure: CompactionMapTest.removeSome$$Said test fails sporadically:  {noformat} at org.junit.Assert.assertNull(Assert.java:562) at org.apache.jackrabbit.oak.plugins.segment.CompactionMapTest.removeSome(CompactionMapTest.java:156) {noformat}  This is a regression introduced with OAK-3501: the {{recent}} map gets not cleared when {{segmentIdMap}} is empty. This can happen when a recent key is removed again while there are no other changes.
OAK-24f7f60a$$Node.addNode(String, String) may check nt-mgt-permission against the wrong node$$While I was troubleshooting an issue we're having in AEM 6.1, I've noticed an "impossible" access denied exception in the logs: the user had permission to add nodes under the node in question but still got an error.  Some testing narrowed the issue down to a difference in behavior between the following two invocations: {{someNode.getNode("child").addNode("grandchild", "nt:unstructured");}} {{someNode.addNode("child/grandchild", "nt:unstructured");}}  As far as I can tell, both should behave identically per the JCR spec, but the second one fails if the user doesn't have node type management permission to someNode, even if they have that permission to someNode/child.  I believe the issue is in line 283 of [NodeImpl|https://svn.apache.org/repos/asf/jackrabbit/oak/trunk/oak-jcr/src/main/java/org/apache/jackrabbit/oak/jcr/session/NodeImpl.java]: it is checking permissions against dlg.getTree(), but it should really check against parent.getTree(), or if possible, the path of the node that's about to be created (so glob restrictions can be evaluated).
OAK-4d231938$$TreeTypeProvider returns wrong type for version related node type definitions$$the following paths with result in type {{VERSION}} instead of {{DEFAULT}} and might lead to unexpected results wrt read access:  - /jcr:system/jcr:nodeTypes/rep:system/rep:namedChildNodeDefinitions/jcr:versionStorage - /jcr:system/jcr:nodeTypes/rep:system/rep:namedChildNodeDefinitions/jcr:activities - /jcr:system/jcr:nodeTypes/rep:system/rep:namedChildNodeDefinitions/jcr:configurations
OAK-9772f5b2$$Initial read of _lastRev creates incorrect RevisionComparator$$The logic in backgroundRead(false) orders the local lastRev  before external lastRev. This the last change done by the local cluster node will look as if it happend before a potentially older external change.
OAK-2565d74a$$BackgroundLeaseUpdate not scheduled when asyncDelay=0$$The BackgroundLeaseUpdate extends from NodeStoreTask, which returns from the run() method when asyncDelay is 0. This is fine for the background read and update tasks. However, the lease update task must run even when asyncDelay is set to zero.
OAK-fcd64766$$Mixin based rules not working for relative properties$$If an indexing rule is defined for mixin then it does not work as expected for relative properties.  Issue here being that most of logic in Aggregate class (which is used for relative property handling also) relies on nodes primaryType and does not account for mixin type
OAK-90ad50da$$RDB/MongoDocumentStore may return stale documents$$It appears that the implementations of the {{update}} method sometimes populate the memory cache with documents that do not reflect any current or previous state in the persistence (that is, miss changes made by another node).  (will attach test)
OAK-4e245a76$$missing support for relative path consisting of parent-element$$could not reopen OAK-95 -> cloning. during testing of user-mgt api found that relpath consisting of a single parent element doesn't work (but used to):  {code} @Test     public void getNode3() throws RepositoryException {         Node node = getNode("/foo");         Node root = node.getNode("..");         assertNotNull(root);         assertEquals("", root.getName());         assertTrue("/".equals(root.getPath()));     } :  {code}
OAK-a5ff019e$$Sometimes hierarchy conflict between concurrent add/delete isn't detected$$I'm not sure of exact set of event that led to an incident on one of our test clusters. The cluster is running 3 AEM instances based on oak build at 1.3.10.r1713699 backed by a single mongo 3 instance.  Unfortunately, we found the issue too late and logs had rolled over. Here's the exception that showed over and over as workflow jobs were (trying to) being processed: {noformat} ....         at java.lang.Thread.run(Thread.java:745) Caused by: javax.jcr.InvalidItemStateException: OakMerge0004: OakMerge0004: The node 8:/oak:index/event.job.topic/:index/com%2Fadobe%2Fgranite%2Fworkflow%2Ftransient%2Fjob%2Fetc%2Fworkflow%2Fmodels%2Fdam-xmp-writeback%2Fjcr_content%2Fmodel/var/eventing/jobs/assigned was already added in revision r151233e54e1-0-4, before r15166378b6a-0-2 (retries 5, 6830 ms)         at org.apache.jackrabbit.oak.api.CommitFailedException.asRepositoryException(CommitFailedException.java:239)         at org.apache.jackrabbit.oak.api.CommitFailedException.asRepositoryException(CommitFailedException.java:212)         at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.newRepositoryException(SessionDelegate.java:669)         at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.save(SessionDelegate.java:495)         at org.apache.jackrabbit.oak.jcr.session.SessionImpl 8.performVoid(SessionImpl.java:419)         at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.performVoid(SessionDelegate.java:273)         at org.apache.jackrabbit.oak.jcr.session.SessionImpl.save(SessionImpl.java:416)         at org.apache.sling.jcr.resource.internal.helper.jcr.JcrResourceProvider.commit(JcrResourceProvider.java:634)         ... 16 common frames omitted Caused by: org.apache.jackrabbit.oak.api.CommitFailedException: OakMerge0004: OakMerge0004: The node 8:/oak:index/event.job.topic/:index/com%2Fadobe%2Fgranite%2Fworkflow%2Ftransient%2Fjob%2Fetc%2Fworkflow%2Fmodels%2Fdam-xmp-writeback%2Fjcr_content%2Fmodel/var/eventing/jobs/assigned was already added in revision r151233e54e1-0-4, before r15166378b6a-0-2 (retries 5, 6830 ms)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.merge0(DocumentNodeStoreBranch.java:200)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.merge(DocumentNodeStoreBranch.java:123)         at org.apache.jackrabbit.oak.plugins.document.DocumentRootBuilder.merge(DocumentRootBuilder.java:158)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.merge(DocumentNodeStore.java:1497)         at org.apache.jackrabbit.oak.core.MutableRoot.commit(MutableRoot.java:247)         at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.commit(SessionDelegate.java:346)         at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.save(SessionDelegate.java:493)         ... 20 common frames omitted Caused by: org.apache.jackrabbit.oak.plugins.document.ConflictException: The node 8:/oak:index/event.job.topic/:index/com%2Fadobe%2Fgranite%2Fworkflow%2Ftransient%2Fjob%2Fetc%2Fworkflow%2Fmodels%2Fdam-xmp-writeback%2Fjcr_content%2Fmodel/var/eventing/jobs/assigned was already added in revision r151233e54e1-0-4, before r15166378b6a-0-2         at org.apache.jackrabbit.oak.plugins.document.Commit.checkConflicts(Commit.java:582)         at org.apache.jackrabbit.oak.plugins.document.Commit.createOrUpdateNode(Commit.java:487)         at org.apache.jackrabbit.oak.plugins.document.Commit.applyToDocumentStore(Commit.java:371)         at org.apache.jackrabbit.oak.plugins.document.Commit.applyToDocumentStore(Commit.java:265)         at org.apache.jackrabbit.oak.plugins.document.Commit.applyInternal(Commit.java:234)         at org.apache.jackrabbit.oak.plugins.document.Commit.apply(Commit.java:219)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.persist(DocumentNodeStoreBranch.java:290)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.persist(DocumentNodeStoreBranch.java:260)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.access 300(DocumentNodeStoreBranch.java:54)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch InMemory.merge(DocumentNodeStoreBranch.java:498)         at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.merge0(DocumentNodeStoreBranch.java:180)         ... 26 common frames omitted .... {noformat}  Doing following removed repo corruption and restored w/f processing: {noformat} oak.removeDescendantsAndSelf("/oak:index/event.job.topic/:index/com%2Fadobe%2Fgranite%2Fworkflow%2Ftransient%2Fjob%2Fetc%2Fworkflow%2Fmodels%2Fdam-xmp-writeback%2Fjcr_content%2Fmodel/var/eventing/jobs/assigned") {noformat}  Attaching [mongoexport output|^mongoexport.zip] for {{/oak:index/event.job.topic/:index/com%2Fadobe%2Fgranite%2Fworkflow%2Ftransient%2Fjob%2Fetc%2Fworkflow%2Fmodels%2Fdam-xmp-writeback%2Fjcr_content%2Fmodel/var/eventing/jobs/assigned/6a389a6a-a8bf-4038-b57b-cb441c6ac557/com.adobe.granite.workflow.transient.job.etc.workflow.models.dam-xmp-writeback.jcr_content.model/2015/11/19/23/54/6a389a6a-a8bf-4038-b57b-cb441c6ac557_10}} (the hierarchy created at {{r151233e54e1-0-4}}). I've renamed a few path elements to make it more reable though (e.g. {{:index/com%2Fadobe%2Fgranite%2Fworkflow%2Ftransient%2Fjob%2Fetc%2Fworkflow%2Fmodels%2Fdam-xmp-writeback%2Fjcr_content%2Fmodel}} -> {{enc_value}}).  [~mreutegg], I'm assigning it to myself for now, but I think this would require your expertise all the way :).
OAK-ab1a0cc2$$EmptyNodeState.equals() broken$$EmptyNodeState.equals() returns incorrect results when the other node state is not of type EmptyNodeState and the two states have differing exists() flags.
OAK-306a9e00$$QueryParse exception when fulltext search performed with term having '/'$$Running the below query, results in Exception pointed by [1]  /jcr:root/content/dam//element(*,dam:Asset)[jcr:contains(jcr:content/metadata/@cq:tags, 'stockphotography:business/business_abstract')] order by @jcr:created descending  Also if you remove the node at /oak:index/damAssetLucene/indexRules/dam:Asset/properties/cqTags  and re-index the /oak:index/damAssetLucene index, the query works.  Seems '/' is special character and needs to be escaped by Oak.  [1] {noformat} Caused by: org.apache.lucene.queryparser.flexible.core.QueryNodeParseException: Syntax Error, cannot parse stockphotography\:business/business_abstract: Lexical error at line 1, column 45.  Encountered: <EOF> after : "/business_abstract"  at org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.parse(StandardSyntaxParser.java:74) at org.apache.lucene.queryparser.flexible.core.QueryParserHelper.parse(QueryParserHelper.java:250) at org.apache.lucene.queryparser.flexible.standard.StandardQueryParser.parse(StandardQueryParser.java:168) at org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex.tokenToQuery(LucenePropertyIndex.java:1260) ... 138 common frames omitted Caused by: org.apache.lucene.queryparser.flexible.standard.parser.TokenMgrError: Lexical error at line 1, column 45.  Encountered: <EOF> after : "/business_abstract" at org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParserTokenManager.getNextToken(StandardSyntaxParserTokenManager.java:937) at org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.jj_scan_token(StandardSyntaxParser.java:945) at org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.jj_3R_4(StandardSyntaxParser.java:827) at org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.jj_3_2(StandardSyntaxParser.java:739) at org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.jj_2_2(StandardSyntaxParser.java:730) at org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.Clause(StandardSyntaxParser.java:318) at org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.ModClause(StandardSyntaxParser.java:303) at org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.ConjQuery(StandardSyntaxParser.java:234) at org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.DisjQuery(StandardSyntaxParser.java:204) at org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.Query(StandardSyntaxParser.java:166) at org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.TopLevelQuery(StandardSyntaxParser.java:147) at org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.parse(StandardSyntaxParser.java:65) ... 141 common frames omitted {noformat}
OAK-94110f21$$Provide Simple Exception Name in Credentials Attribute for PW Expiry$$currently upon encountering a pw history exception while changing the password of a user, the credential attribute is set with the FQ class name, instead of the simple name. this requires consumers (e.g. sling) to use oak package names instead of a simple class name to react to the situation.
OAK-2ac1dccd$$NodeDocument.getNewestRevision() incorrect when there are previous documents$$The method may incorrectly return null when there are previous documents and the base revision is lower than all local changes.  This is most likely caused by changes done for OAK-3388.
OAK-621a5101$$Query test failures on buildbot$$Since revision 1398915 various query tests fail on [buildbot|http://ci.apache.org/builders/oak-trunk/builds/784]:  {code} sql1(org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexQueryTest): No LoginModules configured for jackrabbit.oak sql2(org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexQueryTest): No LoginModules configured for jackrabbit.oak xpath(org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexQueryTest): No LoginModules configured for jackrabbit.oak bindVariableTest(org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexQueryTest): No LoginModules configured for jackrabbit.oak sql1(org.apache.jackrabbit.oak.plugins.index.property.PropertyIndexQueryTest): No LoginModules configured for jackrabbit.oak sql2(org.apache.jackrabbit.oak.plugins.index.property.PropertyIndexQueryTest): No LoginModules configured for jackrabbit.oak xpath(org.apache.jackrabbit.oak.plugins.index.property.PropertyIndexQueryTest): No LoginModules configured for jackrabbit.oak bindVariableTest(org.apache.jackrabbit.oak.plugins.index.property.PropertyIndexQueryTest): No LoginModules configured for jackrabbit.oak sql2Explain(org.apache.jackrabbit.oak.plugins.index.old.QueryTest): No LoginModules configured for jackrabbit.oak sql1(org.apache.jackrabbit.oak.plugins.index.old.QueryTest): No LoginModules configured for jackrabbit.oak xpath(org.apache.jackrabbit.oak.plugins.index.old.QueryTest): No LoginModules configured for jackrabbit.oak bindVariableTest(org.apache.jackrabbit.oak.plugins.index.old.QueryTest): No LoginModules configured for jackrabbit.oak sql1(org.apache.jackrabbit.oak.query.index.TraversingIndexQueryTest): No LoginModules configured for jackrabbit.oak sql2(org.apache.jackrabbit.oak.query.index.TraversingIndexQueryTest): No LoginModules configured for jackrabbit.oak xpath(org.apache.jackrabbit.oak.query.index.TraversingIndexQueryTest): No LoginModules configured for jackrabbit.oak bindVariableTest(org.apache.jackrabbit.oak.query.index.TraversingIndexQueryTest): No LoginModules configured for jackrabbit.oak {code}
OAK-2a02a138$$Hidden properties (one prefixed with ':') in lucene's analyzer configuration fail to construct analyzers$$This is similar to OAK-2524 in the sense that lucene doesn't like extra arguments sent its way while constructing analyzers. In some cases (like node move adds {{:source-path}}) we have hidden properties added to index definition nodes and they get passed along to lucene analyzer factories which complaint and fail.
OAK-c13708e3$$[RDB] Updated blob still deleted even if deletion interval lower$$If an existing blob is uploaded again, the timestamp of the existing entry is updated in the meta table. Subsequently if a call to delete (RDBBlobStore#countDeleteChunks) is made with {{maxLastModifiedTime}} parameter of less than the updated time above, the entry in the meta table is not touched but the data table entry is wiped out.   Refer https://github.com/apache/jackrabbit-oak/blob/trunk/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/document/rdb/RDBBlobStore.java#L510
OAK-4faf31e3$$Lucene index / compatVersion 2: search for 'abc!' does not work$$When using a Lucene fulltext index with compatVersion 2, then the following query does not return any results. When using compatVersion 1, the correct result is returned.  {noformat} SELECT * FROM [nt:unstructured] AS c  WHERE CONTAINS(c.[jcr:description], 'abc!')  AND ISDESCENDANTNODE(c, '/content') {noformat}  With compatVersion 1 and 2, searching for just 'abc' works. Also, searching with '=' instead of 'contains' works.
OAK-94c6c575$$Branch reset does not revert all changes$$This is caused by recent changes done for OAK-3646.
OAK-690fb9f4$$Commit fails even though change made it to the DocumentStore$$In some rare cases it may happen that the DocumentNodeStore considers a commit as failed even though the changes were applied entirely to the DocumentStore. The issue happens when the update of the commit root is applied to the storage of a DocumentStore but then shortly after the communication between Oak the the storage system fails. On the Oak side the call will be considered as failed, but the change was actually applied.  The issue can be reproduced with the test attached to OAK-1641 and a replica-set with 3 nodes. Killing the primary node and restarting it a after a while in a loop will eventually lead to a commit that conflicts itself.
OAK-99996c25$$OakDirectory not usable in readOnly mode with a readOnly builder$$When using {{OakDirectory}} with a read only builder say in LuceneCommand in oak-console following error is seen  {noformat} lc info /oak:index/users ERROR java.lang.UnsupportedOperationException: This builder is read-only.        at org.apache.jackrabbit.oak.spi.state.ReadOnlyBuilder.unsupported (ReadOnlyBuilder.java:45)        at org.apache.jackrabbit.oak.spi.state.ReadOnlyBuilder.child (ReadOnlyBuilder.java:190)        at org.apache.jackrabbit.oak.spi.state.ReadOnlyBuilder.child (ReadOnlyBuilder.java:35)        at org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory.<init> (OakDirectory.java:93)        at org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory.<init> (OakDirectory.java:87)        at org.apache.jackrabbit.oak.console.commands.LuceneCommand.getDirectory (LuceneCommand.groovy:128)        at org.apache.jackrabbit.oak.console.commands.LuceneCommand.this 4 getDirectory (LuceneCommand.groovy)        at org.apache.jackrabbit.oak.console.commands.LuceneCommand _closure1.doCall (LuceneCommand.groovy:55) {noformat}
OAK-b939aa6e$$Sysview import of single valued mv property creates sv property$$See test in filevault [0].  it imports a multivalue property that only has 1 value, via [1]. the same test succeeds in jackrabbit 2.0, but fails in oak 1.3.14  [0] https://github.com/apache/jackrabbit-filevault/blob/jackrabbit-filevault-3.1.26/vault-core/src/test/java/org/apache/jackrabbit/vault/packaging/integration/TestUserContentPackage.java#L297-L326 [1] https://github.com/apache/jackrabbit-filevault/blob/jackrabbit-filevault-3.1.26/vault-core/src/main/java/org/apache/jackrabbit/vault/fs/impl/io/JcrSysViewTransformer.java#L146-L148
OAK-4ed7bc8e$$Inconsistency in Node#setProperty in case of null value$$Setting a null value to a single valued property will result in 'null' being returned while executing the same on a multivalued property will return the removed property.  jr2 returned the removed property in both cases as far as i  remember and i would suggest that we don't change that behavior. in particular since the specification IMO doesn't allow to return null-values for these methods.
OAK-e6c31270$$Inconsistency in Node#setProperty in case of null value$$Setting a null value to a single valued property will result in 'null' being returned while executing the same on a multivalued property will return the removed property.  jr2 returned the removed property in both cases as far as i  remember and i would suggest that we don't change that behavior. in particular since the specification IMO doesn't allow to return null-values for these methods.
OAK-f4324736$$LuceneIndexProviderService may miss on registering PreExtractedTextProvider$${{LuceneIndexProviderService}} has an optional dependency on {{PreExtractedTextProvider}}. In such a case it can happen that bind for the provided is invoked before the activate is called. In such a case the provider would not be registered.
OAK-557eec4f$$o.a.j.o.spi.query.Filter exposes unexported class o.a.j.o.query.ast.SelectorImpl$$The interface {{o.a.j.o.spi.query.Filter}} uses in its public API the class {{o.a.j.o.query.ast.SelectorImpl}}, but while the former is contained in an exported package, the latter is not.
OAK-52ca008c$$SplitOperations may not retain most recent committed _commitRoot entry$$In some rare cases it may happen that SplitOperations does not retain the most recent committed _commitRoot entry on a document. This may result in an undetected hierarchy conflict.
OAK-9a109aa3$$Suggestion dictionary don't update after suggestUpdateFrequencyMinutes unless something else causes index update$$Currently, suggestions building is tied at the end of indexing cycle. Along with that we check if diff between currTime and lastSugguestionBuildTime is more than {{suggestUpdateFrequencyMinutes}} before deciding to build suggestions or not.  This allows for suggestions not getting updated if: * At T1 suggestions are built * At T2 an index update takes place but suggestions aren't rebuilt because not enough time has passed since T1 * Now at T3 (after sufficient time), changes at T2 won't show up for suggestions until some other index change happens.  We should probably see track about last changes in index (at T2) and use that too while running indexing cycle at T3.
OAK-56accddf$$AssertionError thrown for Lucene index with empty suggest disctionary$$Create an index where one field is enabled for suggestion but no content is indexed for that index i.e. no matching content. Then while performing any query following exception is thrown  {noformat} java.lang.AssertionError 	at org.apache.lucene.search.suggest.analyzing.AnalyzingInfixSuggester.<init>(AnalyzingInfixSuggester.java:167) 	at org.apache.jackrabbit.oak.plugins.index.lucene.util.SuggestHelper 2.<init>(SuggestHelper.java:127) 	at org.apache.jackrabbit.oak.plugins.index.lucene.util.SuggestHelper.getLookup(SuggestHelper.java:127) 	at org.apache.jackrabbit.oak.plugins.index.lucene.util.SuggestHelper.getLookup(SuggestHelper.java:123) 	at org.apache.jackrabbit.oak.plugins.index.lucene.IndexNode.<init>(IndexNode.java:109) 	at org.apache.jackrabbit.oak.plugins.index.lucene.IndexNode.open(IndexNode.java:69) 	at org.apache.jackrabbit.oak.plugins.index.lucene.IndexTracker.findIndexNode(IndexTracker.java:162) 	at org.apache.jackrabbit.oak.plugins.index.lucene.IndexTracker.acquireIndexNode(IndexTracker.java:137) 	at org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex.getPlans(LucenePropertyIndex.java:249) 	at org.apache.jackrabbit.oak.query.QueryImpl.getBestSelectorExecutionPlan(QueryImpl.java:1016) 	at org.apache.jackrabbit.oak.query.QueryImpl.getBestSelectorExecutionPlan(QueryImpl.java:949) 	at org.apache.jackrabbit.oak.query.ast.SelectorImpl.prepare(SelectorImpl.java:288) {noformat}  This happens with {{-ea}} flag i.e. java assertions enabled. It caused [here|https://github.com/apache/lucene-solr/blob/releases/lucene-solr/4.7.1/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java#L167]
OAK-9120fd1b$$segment's compareAgainstBaseState wont call childNodeDeleted when deleting last and adding n nodes$${{SegmentNodeState.compareAgainstBaseState}} fails to call {{NodeStateDiff.childNodeDeleted}} when for the same parent the only child is deleted and at the same time multiple new, different children are added.  Reason is that the [current code|https://github.com/apache/jackrabbit-oak/blob/a9ce70b61567ffe27529dad8eb5d38ced77cf8ad/oak-segment/src/main/java/org/apache/jackrabbit/oak/plugins/segment/SegmentNodeState.java#L558] for '{{afterChildName == MANY_CHILD_NODES}}' *and* '{{beforeChildName == ONE_CHILD_NODE}}' does not handle all cases: it assumes that 'after' contains the 'before' child and doesn't handle the situation where the 'before' child has gone.
OAK-374e3f3d$$Simple versionable nodes are invalid after migration$$OAK-3836 introduces a support for migrating {{mix:simpleVersionable}} nodes from JCR2 to {{mix:versionable}} nodes in Oak. It changes the mixin type, however it doesn't add required properties: {{jcr:versionHistory}}, {{jcr:baseVersion}} and {{jcr:predecessors}}. As a result, versioning-related methods invoked on such nodes doesn't work correctly.
OAK-2a489d05$$QueryEngine adding invalid property restriction for fulltext query$$QueryEngine inserts a property restriction of "is not null" for any property used in fulltext constraint. For e.g. for query  {noformat} select * from [nt:unstructured] where CONTAINS([jcr:content/metadata/comment], 'december') {noformat}  A property restriction would be added for {{jcr:content/metadata/comment}}. However currently due to bug in {{FulltextSearchImpl}} [1] the property name generated is {{comment/jcr:content/metadata}}.  {code} @Override     public void restrict(FilterImpl f) {         if (propertyName != null) {             if (f.getSelector().equals(selector)) {                 String p = propertyName;                 if (relativePath != null) {                     p = PathUtils.concat(p, relativePath);                 }                                 p = normalizePropertyName(p);                 restrictPropertyOnFilter(p, f);             }         }         f.restrictFulltextCondition(fullTextSearchExpression.currentValue().getValue(Type.STRING));     } {code}  This happens because {{relativePath}} is passed as second param to {{PathUtils.concat}}. It should be first param  [1] https://github.com/apache/jackrabbit-oak/blob/1.4/oak-core/src/main/java/org/apache/jackrabbit/oak/query/ast/FullTextSearchImpl.java#L275-L286
OAK-36e70bd7$$NodeBuilder.reset might lead to inconsistent builder$$The following test fails: {code} NodeBuilder root = new MemoryNodeBuilder(BASE); NodeBuilder x = root.child("x"); NodeBuilder y = x.child("y");  root.reset(BASE); assertTrue(root.hasChildNode("x")); assertFalse(x.hasChildNode("y"));  // fails {code}
OAK-916cd92f$$Binaries might get removed by garbage collection while still referenced$$The [Microkernel contract|http://svn.apache.org/repos/asf/jackrabbit/oak/trunk/oak-mk-api/src/main/java/org/apache/jackrabbit/mk/api/MicroKernel.java] specifies a specific format for references to binaries: ":blobId:<blobId>". Currently oak-core uses a different format and thus risks premature garbage collection of such binaries.
OAK-cdb34ffc$$FileStore.flush prone to races leading to corruption$$There is a small window in {{FileStore.flush}} that could lead to data corruption: if we crash right after setting the persisted head but before any delay-flushed {{SegmentBufferWriter}} instance flushes (see {{SegmentBufferWriterPool.returnWriter()}}) then that data is lost although it might already be referenced from the persisted head.  We need to come up with a test case for this.   A possible fix would be to return a future from {{SegmentWriter.flush}} and rely on a completion callback. Such a change would most likely also be useful for OAK-3690.
OAK-c02ecef8$$MemoryPropertyBuilder.assignFrom leads to ClassCastException on getPropertyState with date properties$$None
OAK-06c367af$$Cost per entry for Lucene index of type v1 should be higher than that of v2$$Currently default cost per entry for Lucene index of type # v1 - which uses query time aggregation # v2 - which uses index time aggregation  Are same. However given that query time aggregation would require more effort it should result in a higher cost per entry.  This fact impacts the result in cases like OAK-2081 (see last few comments) where with usage of limits both index are currently considered equals
OAK-f303c916$$SegmentWriter saves references to external blobs$$The new {{SegmentWriteOperation#internalWriteStream}} method checks whether the input stream to write is a {{SegmentStream}}. If it's, writer will reuse existing block ids, rather than storing the whole stream.  It should also check whether the blocks in {{SegmentStream}} comes from the same tracker / segment store. Otherwise this will create invalid references if someone invokes the {{internalWriteStream()}} method with a {{SegmentStream}} created externally.
OAK-59a83d23$$Non-root lucene index throws exception if query constraints match root of sub-tree$$LucenePropetyIndexProvider returns incorrect (normalized) path for root of sub-tree if index is defined on the sub-tree. e.g. {{/jcr:root/test//element(*, nt:base)\[@foo='bar']}} would fail with following defn {noformat} + /test     - foo="bar"     + test1           - foo="bar"     + oak:index            - indexRules/nt:base/properties/foo/propertyIndex="true" {noformat}
OAK-b0014b7d$$IndexOutOfBoundsException in FileStore.writeStream$$When writing streams of specific length I get  {code} java.lang.IndexOutOfBoundsException at java.nio.Buffer.checkIndex(Buffer.java:538) at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:359) at org.apache.jackrabbit.oak.segment.Segment.getGcGen(Segment.java:318) at org.apache.jackrabbit.oak.segment.file.FileStore.writeSegment(FileStore.java:1371) at org.apache.jackrabbit.oak.segment.SegmentWriter SegmentWriteOperation.internalWriteStream(SegmentWriter.java:661) {code}
OAK-74cbba24$$Stale cluster ids can potentially lead to lots of previous docs traversal in NodeDocument.getNewestRevision$$When some (actual test case and conditions still being investigated) of the following conditions are met: * There are property value changes from different cluster id * There are very old and stale cluster id (probably older incarnations of current node itself) * A parallel background split removes all _commitRoot, _revision entries such that the latest one (which is less that baseRev) is very old  , finding newest revision traverses a lot of previous docs. Since root document gets split a lot and is a very common commitRoot (thus participating during checkConflicts in lot of commits), the issue can slow down commits by a lot
OAK-002c5845$$Lucene index / compatVersion 2: search for 'a=b=c' does not work$$Similar to OAK-3879, we need to escape '=' (althoug lucene [escape()|https://github.com/apache/lucene-solr/blob/releases/lucene-solr/4.7.1/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java#L988] apparently doesn't escape it).  Due to this searching for {{a=b=c}} throws parse exception from lucene query parser. Also, searching for {{a=b}} gives incorrect result.
OAK-037dea72$$XPath: queries starting with "//" are not always converted correctly$$XPath queries starting with "//" are not always converted to the expected SQL-2 query. Examples:  {noformat} //element(*, oak:QueryIndexDefinition)/* select [jcr:path], [jcr:score], * from [oak:QueryIndexDefinition] as a  //element(*, oak:QueryIndexDefinition)//* select [jcr:path], [jcr:score], * from [oak:QueryIndexDefinition] as a {noformat}  This is wrong. Instead, a join should be used.
OAK-ca05fd06$$XPath: querying for nodes named "text", "element", and "rep:excerpt" fails$$Queries that contain "text" or "element" as a node name currently fail, because the the parser assumes "text()" / "element(...)". Example query that fails:  {noformat} /jcr:root/content/text/jcr:content//element(*,nt:unstructured) {noformat}  A workaround is to use the escape mechanism, that is:  {noformat} /jcr:root/tmp/_x0074_ext/jcr:content//element(*,nt:unstructured) {noformat}  It looks like '(' and ')' are valid characters in node names, but to query for those characters, they need to be escaped.
OAK-e33516d5$$DefaultSyncContext.syncMembership may sync group of a foreign IDP$$With the following scenario the {{DefaultSyncContext.syncMembership}} may end up synchronizing (i.e. updating) a group defined by an foreign IDP and even add the user to be synchronized as a new member:  - configuration with different IDPs - foreign IDP synchronizes a given external group 'groupA' => rep:externalID points to foreign-IDP (e.g. rep:externalId = 'groupA;foreignIDP') - my-IDP contains a group with the same ID (but obviously with a different rep:externalID) and user that has declared group membership pointing to 'groupA' from my IDP  if synchronizing my user first the groupA will be created with a rep:externalId = 'groupA;myIDP'. however, if the group has been synced before by the foreignIDP the code fails to verify that an existing group 'groupA' really belongs to the same IDP and thus may end up synchronizing the group and updating it's members.  IMHO that's a critical issue as it violates the IDP boundaries. the fix is pretty trivial as it only requires testing for the IDP of the existing group as we do it in other places (even in the same method).
OAK-668f08f2$$Incomplete journal when move and copy operations are involved$$Given a node at /source:  {code} head = mk.commit("/",     ">\"source\" : \"moved\"" +     "*\"moved\" : \"copy\"",     head, ""); {code}  results in the following journal:  {code} >"/source":"/copy" {code}  where the freshly created node at /moved is missing.  Similarly   {code} head = mk.commit("/",     "*\"source\" : \"copy\"" +     ">\"copy\" : \"moved\"",     head, ""); {code}  results in  {code} +"/moved":{} {code}  where moved away node at /source is missing.
OAK-d645112f$$RepositorySidegrade: oak-segment to oak-segment-tar should migrate checkpoint info$$The sidegrade from {{oak-segment}} to {{oak-segment-tar}} should also take care of moving the checkpoint data and meta. This will save a very expensive full-reindex.
OAK-08f0b280$$Possible overflow in checkpoint creation$$Creating a checkpoint with {{Long.MAX_VALUE}} lifetime will overflow the value, allowing the store to immediately release the checkpoint.
OAK-275eca83$$Possible overflow in checkpoint creation$$Creating a checkpoint with {{Long.MAX_VALUE}} lifetime will overflow the value, allowing the store to immediately release the checkpoint.
OAK-7441a3d5$$Index path property should be considered optional for copy on read logic$$As part of changes done for OAK-4347 logic assumes that indexPath is always non null. This works fine for fresh setup where the indexPath would have been set by the initial indexing. However for upgraded setup this assumption would break as it might happen that index does not get updated with new approach and before that a read is performed.  Currently with updated code on upgraded setup following exception is seen   {noformat} Caused by: javax.security.auth.login.LoginException: java.lang.NullPointerException: Index path property [:indexPath] not found         at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:236)         at org.apache.jackrabbit.oak.plugins.index.lucene.IndexDefinition.getIndexPathFromConfig(IndexDefinition.java:664)         at org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier.getSharedWorkingSet(IndexCopier.java:242)         at org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier.wrapForRead(IndexCopier.java:140)         at org.apache.jackrabbit.oak.plugins.index.lucene.IndexNode.open(IndexNode.java:53)         at org.apache.jackrabbit.oak.plugins.index.lucene.IndexTracker.findIndexNode(IndexTracker.java:179)         at org.apache.jackrabbit.oak.plugins.index.lucene.IndexTracker.acquireIndexNode(IndexTracker.java:154)         at org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex.getPlans(LucenePropertyIndex.java:250) {noformat}  For this specific flow the indexPath can be passed in and not looked up from IndexDefinition
OAK-c9765c21$$Ignore files in the root directory of the FileDataStore in #getAllIdentifiers$$The call to OakFileDataStore#getAllIdentifiers should ignore the the files directly at the root of the DataStore (These files are used for SharedDataStore etc). This does not cause any functional problems but leads to logging warning in the logs.  There is already a check but it fails when the data store root is specified as a relative path.
OAK-00df38d2$$Adding a node with the name of a removed node can lead to an inconsistent hierarchy of node builders$$None
OAK-999097e1$$Node builder for existing node return null for base state$${{MemoryNodeBuilder.getBaseState()}} returns null on builder for an existing node.
OAK-a7f0e808$$NPE in the TypeValidator when using the Lucene Index$$None
OAK-3270e761$$Adding a node to a node that doesn't accept children doesn't fail with ConstraintViolationException$$More node type fun!  I ran into this via the tck test {{org.apache.jackrabbit.test.api.query.SaveTest#testConstraintViolationException}}.  It seems adding a node to a node that doesn't accept children (like for example {{nt:query}}) fails with a {{RepositoryException}} that wraps a {{CommitFailedException}} with a message along the lines of: {{Cannot add node 'q2' at /q1}}, further wrapping a weird-looking {{RepositoryException: No matching node definition found for org.apache.jackrabbit.oak.plugins.nodetype.ValidatingNodeTypeManager@257f1b}}  While this seems ok enough, the tck test expects a {{ConstraintViolationException}}, so that's why I created this bug.   I'll attach a test case shortly.  Trace  {code} javax.jcr.RepositoryException 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) 	at java.lang.reflect.Constructor.newInstance(Constructor.java:513) 	at org.apache.jackrabbit.oak.api.CommitFailedException.throwRepositoryException(CommitFailedException.java:57) 	at org.apache.jackrabbit.oak.jcr.SessionDelegate.save(SessionDelegate.java:244) 	at org.apache.jackrabbit.oak.jcr.SessionImpl.save(SessionImpl.java:283) 	at org.apache.jackrabbit.oak.jcr.nodetype.NodeTypeTest.illegalAddNode(NodeTypeTest.java:39) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:597) 	at org.junit.runners.model.FrameworkMethod 1.runReflectiveCall(FrameworkMethod.java:45) 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42) 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20) 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28) 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30) 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) 	at org.junit.runners.ParentRunner 3.run(ParentRunner.java:231) 	at org.junit.runners.ParentRunner 1.schedule(ParentRunner.java:60) 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) 	at org.junit.runners.ParentRunner.access 000(ParentRunner.java:50) 	at org.junit.runners.ParentRunner 2.evaluate(ParentRunner.java:222) 	at org.junit.runners.ParentRunner.run(ParentRunner.java:300) 	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50) 	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197) Caused by: org.apache.jackrabbit.oak.api.CommitFailedException: Cannot add node 'q2' at /q1 	at org.apache.jackrabbit.oak.plugins.nodetype.TypeValidator.childNodeAdded(TypeValidator.java:134) 	at org.apache.jackrabbit.oak.spi.commit.CompositeValidator.childNodeAdded(CompositeValidator.java:68) 	at org.apache.jackrabbit.oak.spi.commit.ValidatingHook ValidatorDiff.childNodeAdded(ValidatingHook.java:155) 	at org.apache.jackrabbit.oak.spi.state.AbstractNodeState.compareAgainstBaseState(AbstractNodeState.java:157) 	at org.apache.jackrabbit.oak.kernel.KernelNodeState.compareAgainstBaseState(KernelNodeState.java:243) 	at org.apache.jackrabbit.oak.spi.commit.ValidatingHook ValidatorDiff.validate(ValidatingHook.java:110) 	at org.apache.jackrabbit.oak.spi.commit.ValidatingHook ValidatorDiff.validate(ValidatingHook.java:101) 	at org.apache.jackrabbit.oak.spi.commit.ValidatingHook ValidatorDiff.childNodeAdded(ValidatingHook.java:157) 	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState 3.childNodeAdded(ModifiedNodeState.java:292) 	at org.apache.jackrabbit.oak.spi.state.AbstractNodeState.compareAgainstBaseState(AbstractNodeState.java:157) 	at org.apache.jackrabbit.oak.kernel.KernelNodeState.compareAgainstBaseState(KernelNodeState.java:243) 	at org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:269) 	at org.apache.jackrabbit.oak.spi.commit.ValidatingHook ValidatorDiff.validate(ValidatingHook.java:110) 	at org.apache.jackrabbit.oak.spi.commit.ValidatingHook ValidatorDiff.validate(ValidatingHook.java:101) 	at org.apache.jackrabbit.oak.spi.commit.ValidatingHook.processCommit(ValidatingHook.java:73) 	at org.apache.jackrabbit.oak.spi.commit.CompositeHook.processCommit(CompositeHook.java:59) 	at org.apache.jackrabbit.oak.kernel.KernelNodeStoreBranch.merge(KernelNodeStoreBranch.java:127) 	at org.apache.jackrabbit.oak.core.RootImpl 2.run(RootImpl.java:239) 	at org.apache.jackrabbit.oak.core.RootImpl 2.run(RootImpl.java:1) 	at java.security.AccessController.doPrivileged(Native Method) 	at javax.security.auth.Subject.doAs(Subject.java:337) 	at org.apache.jackrabbit.oak.core.RootImpl.commit(RootImpl.java:234) 	at org.apache.jackrabbit.oak.jcr.SessionDelegate.save(SessionDelegate.java:241) 	... 27 more Caused by: javax.jcr.RepositoryException: No matching node definition found for org.apache.jackrabbit.oak.plugins.nodetype.ValidatingNodeTypeManager@257f1b 	at org.apache.jackrabbit.oak.plugins.nodetype.ReadOnlyNodeTypeManager.getDefinition(ReadOnlyNodeTypeManager.java:406) 	at org.apache.jackrabbit.oak.plugins.nodetype.TypeValidator EffectiveNodeType.getDefinition(TypeValidator.java:302) 	at org.apache.jackrabbit.oak.plugins.nodetype.TypeValidator EffectiveNodeType.checkAddChildNode(TypeValidator.java:249) 	at org.apache.jackrabbit.oak.plugins.nodetype.TypeValidator.childNodeAdded(TypeValidator.java:127) 	... 49 more {code}
OAK-b62f1c26$$Wrong results and NPE with copy operation$$The following code either results in an NPE or in a wrong result depending on which Microkernel instance is used.   {code}     mk.commit("", "+\"/root\":{}", mk.getHeadRevision(), "");     mk.commit("", "+\"/root/N0\":{}*\"/root/N0\":\"/root/N1\"+\"/root/N0/N4\":{}",             mk.getHeadRevision(), ""); {code}  The wrong result is  {code} {     ":childNodeCount": 2,     "N0": {         ":childNodeCount": 1,         "N4": {             ":childNodeCount": 0         }     },     "N1": {         ":childNodeCount": 1,         "N4": {             ":childNodeCount": 0         }     } } {code}  The expected result is {code} {     ":childNodeCount": 2,     "N0": {         ":childNodeCount": 1,         "N4": {             ":childNodeCount": 0         }     },     "N1": {         ":childNodeCount": 0     } } {code}  simple:fs:target/temp: wrong result fs:{homeDir}/target: NPE http-bridge:fs:{homeDir}/target: NPE simple: wrong result
OAK-f2a2edec$$NamePathMapper should fail on absolute paths escaping root$$The name path mapper should no accept invalid paths of type  {code} /.. {code}  I.e. paths which escape beyond the root of the hierarchy.
OAK-61381ea2$$SQL-2 query parser doesn't detect some illegal statements$$The SQL-2 query parser doesn't detect some illegal statements, for example  {code} select * from [nt:base] where name =+ 'Hello' select * from [nt:base] where name => 'Hello' {code}  Both are currently interpreted as "name = 'Hello'", which is wrong.
OAK-b896c926$$Item names starting with '{X}' cause RepositoryException$$The exception is RepositoryException: Invalid name or path: {0} foo  E.g. for an item named '{0} foo'.  I guess oak-jcr tries to interpret it as a name in expanded form but does not find a namespace uri for '0'. IIRC these names are valid in Jackrabbit 2.x.
OAK-f63d745a$$Multivalued properties with array size 0 forget their type$$thought i remember that i have seen a related TODO or issue before, i couldn't find it any more... sorry for that.  while cleaning up the node type code i found that one FIXME in the  ReadOnlyNodeTypeManager related to definition generation was only needed because the TypeValidator failed upon validation of an empty jcr:supertypes definition. not storing the super types if none has be declared solved the problem for the time being.  however, it seems to me that the underlying problem is in a completely different area: namely that mv properties with an empty value array forget their type.  this can be verified with the following test: {code}     @Test     public void addEmptyMultiValueName() throws RepositoryException {         Node parentNode = getNode(TEST_PATH);         Value[] values = new Value[0];          parentNode.setProperty("multi name", values);         parentNode.getSession().save();          Session session2 = createAnonymousSession();         try {             Property property = session2.getProperty(TEST_PATH + "/multi name");             assertTrue(property.isMultiple());             assertEquals(PropertyType.NAME, property.getType());             Value[] values2 = property.getValues();             assertEquals(values.length, values2.length);             assertEquals(values[0], values2[0]);             assertEquals(values[1], values2[1]);         } finally {             session2.logout();         }     } {code}
OAK-ec961a38$$IllegalStateException in MemoryNodeBuilder$${{AuthorizablePropertyTest.testSetPropertyByRelPath()}} sometimes causes an IllegalStateException in {{MemoryNodeBuilder}}. This might be a problem with the latter uncovered by the recent switch to the p2 index mechanism (OAK-511).  {code} java.lang.IllegalStateException     at com.google.common.base.Preconditions.checkState(Preconditions.java:133)     at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.read(MemoryNodeBuilder.java:205)     at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.getChildNodeNames(MemoryNodeBuilder.java:379)     at org.apache.jackrabbit.oak.plugins.index.p2.strategy.ContentMirrorStoreStrategy.remove(ContentMirrorStoreStrategy.java:66)     at org.apache.jackrabbit.oak.plugins.index.p2.Property2IndexUpdate.apply(Property2IndexUpdate.java:143)     at org.apache.jackrabbit.oak.plugins.index.p2.Property2IndexDiff.apply(Property2IndexDiff.java:232)     at org.apache.jackrabbit.oak.plugins.index.IndexHookManager.apply(IndexHookManager.java:71)     at org.apache.jackrabbit.oak.plugins.index.IndexHookManager.processCommit(IndexHookManager.java:61)     at org.apache.jackrabbit.oak.spi.commit.CompositeHook.processCommit(CompositeHook.java:59)     at org.apache.jackrabbit.oak.kernel.KernelNodeStoreBranch.merge(KernelNodeStoreBranch.java:127)     at org.apache.jackrabbit.oak.core.RootImpl 2.run(RootImpl.java:240)     at org.apache.jackrabbit.oak.core.RootImpl 2.run(RootImpl.java:236)     at java.security.AccessController.doPrivileged(Native Method)     at javax.security.auth.Subject.doAs(Subject.java:337)     at org.apache.jackrabbit.oak.core.RootImpl.commit(RootImpl.java:235)     at org.apache.jackrabbit.oak.jcr.SessionDelegate.save(SessionDelegate.java:255)     at org.apache.jackrabbit.oak.jcr.SessionImpl.save(SessionImpl.java:283)     at org.apache.jackrabbit.oak.jcr.security.user.AbstractUserTest.tearDown(AbstractUserTest.java:72)     at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:456)     at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)     at org.junit.runner.JUnitCore.run(JUnitCore.java:157)     at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:76)     at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:195)     at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:63)     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)     at com.intellij.rt.execution.application.AppMain.main(AppMain.java:120)  {code}
OAK-90c45a02$$NodeBuilder deleted child nodes can come back$$While working on OAK-520, I've noticed a problem with the NodeBuilder: when we delete an entire hierarchy of nodes and then recreate a part of it, some of the previously deleted nodes can come back.  This only happens when there are more than 3 levels of nodes.  So given a hierarchy of nodes: /x/y/z deleted 'x' and simply use the NodeBuilder to traverse down on the same path: .child('x').child('y'). At this point the 'z' child reappears even though it was deleted before.   I'll attach a test case shortly.
OAK-a8493efc$$The Property2Index eagerly and unnecessarily fetches all data$$Currently, the Property2Index (as well as the PropertyIndex and the NodeTypeIndex) loads all paths into a hash set. This is even the case for the getCost operation, which should be fast and therefore not load too much data.  This strategy can cause ouf-of-memory if the result is too big. Also, loading all data is not necessary unless the user reads all rows.  Instead, the index should only load data on demand. Also, the getCost operation should only estimate the number of read nodes, and not actually read the data.
OAK-ffa818f3$$Wrong compareTo in micro-kernel Id class$$CompareTo method in Id class fails in some cases.  {code}  // this works final Id id1 = Id.fromString( "0000000000000007" ); final Id id2 = Id.fromString( "000000000000000c" );  assertTrue( id1 + " should be less than " + id2, id1.compareTo( id2 ) < 0 );  // but this doesn't final Id id1 = Id.fromString( "0000000000000070" ); final Id id2 = Id.fromString( "00000000000000c0" );  assertTrue( id1 + " should be less than " + id2, id1.compareTo( id2 ) < 0 ); {code}
OAK-3ce758b7$$PutTokenImpl not thread safe$${{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated.
OAK-428e32c6$$Query: unexpected result on negative limit / offset$$Currently, running a query with limit of -1 never returns any rows, the same as when using limit = 0.  Either the query engine should fail with a negative limit or offset (IllegalArgumentException), or it should ignore negative values (unlimited result rows for limit, probably no offset for offset = -1).  I would prefer IllegalArgumentException, but I can also live with -1 = unlimited, at least for "limit".
OAK-717186d6$$Moving larger trees cause OutOfMemoryError$${{LargeMoveTest.moveTest}} test runs out of heap space when moving roughly 100000 nodes (128M heap):  {code} java.lang.OutOfMemoryError: Java heap space 	at java.util.Arrays.copyOf(Arrays.java:2786) 	at java.lang.StringCoding.safeTrim(StringCoding.java:64) 	at java.lang.StringCoding.access 300(StringCoding.java:34) 	at java.lang.StringCoding StringEncoder.encode(StringCoding.java:251) 	at java.lang.StringCoding.encode(StringCoding.java:272) 	at java.lang.String.getBytes(String.java:946) 	at org.apache.jackrabbit.mk.util.IOUtils.writeString(IOUtils.java:84) 	at org.apache.jackrabbit.mk.store.BinaryBinding.writeMap(BinaryBinding.java:98) 	at org.apache.jackrabbit.mk.model.ChildNodeEntriesMap.serialize(ChildNodeEntriesMap.java:196) 	at org.apache.jackrabbit.mk.model.AbstractNode.serialize(AbstractNode.java:169) 	at org.apache.jackrabbit.mk.persistence.InMemPersistence.writeNode(InMemPersistence.java:76) 	at org.apache.jackrabbit.mk.store.DefaultRevisionStore.putNode(DefaultRevisionStore.java:276) 	at org.apache.jackrabbit.mk.model.StagedNodeTree StagedNode.persist(StagedNodeTree.java:568) 	at org.apache.jackrabbit.mk.model.StagedNodeTree StagedNode.persist(StagedNodeTree.java:563) 	at org.apache.jackrabbit.mk.model.StagedNodeTree StagedNode.persist(StagedNodeTree.java:563) 	at org.apache.jackrabbit.mk.model.StagedNodeTree StagedNode.persist(StagedNodeTree.java:563) 	at org.apache.jackrabbit.mk.model.StagedNodeTree StagedNode.persist(StagedNodeTree.java:563) 	at org.apache.jackrabbit.mk.model.StagedNodeTree StagedNode.persist(StagedNodeTree.java:563) 	at org.apache.jackrabbit.mk.model.StagedNodeTree StagedNode.persist(StagedNodeTree.java:563) 	at org.apache.jackrabbit.mk.model.StagedNodeTree.persist(StagedNodeTree.java:80) 	at org.apache.jackrabbit.mk.model.CommitBuilder.doCommit(CommitBuilder.java:126) 	at org.apache.jackrabbit.mk.model.CommitBuilder.doCommit(CommitBuilder.java:94) 	at org.apache.jackrabbit.mk.core.MicroKernelImpl.commit(MicroKernelImpl.java:496) 	at org.apache.jackrabbit.oak.kernel.KernelNodeStoreBranch.commit(KernelNodeStoreBranch.java:178) 	at org.apache.jackrabbit.oak.kernel.KernelNodeStoreBranch.setRoot(KernelNodeStoreBranch.java:78) 	at org.apache.jackrabbit.oak.core.RootImpl.purgePendingChanges(RootImpl.java:355) 	at org.apache.jackrabbit.oak.core.RootImpl.commit(RootImpl.java:234) 	at org.apache.jackrabbit.oak.core.LargeMoveTest.moveTest(LargeMoveTest.java:78) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) {code}  This is caused by the inefficient rebase implementation in oak-core as discussed at length in OAK-464.
OAK-3f51fb09$$PropertyStates#createProperty ignores namespace mappings when creating states of type NAME and PATH$$as far as i saw we use PropertyStates#createProperty to create and set an OAK property from a given JCR value or a list of JCR values.  this works well for all types of values except for NAME, PATH which  may contain values with remapped namespaces which will not be converted back to oak-values during the state creation:  {code}      List<String> vals = Lists.newArrayList();      for (Value value : values) {          vals.add(value.getString());      }      return new MultiGenericPropertyState(name, vals, Type.fromTag(type, true)); {code}  if am not mistaken {code}value.getString(){code} will return the JCR representation of the value instead of the oak representation as it would be needed here.  possible solutions include: - passing namepathmapper to the create method - only accept oak Value implementation that allows to retrieve the   internal representation, which is present in the ValueImpl afaik.
OAK-7d72e6ed$$Query: for joins, sometimes no or the wrong index is used$$Currently, no index is used for the join condition. For example, the query:  {code} select * from [nodeTypeA] as a  inner join [nodeTypeB] as b on isdescendantnode(b, a)  where lower(a.x) = 'y' and b.[property] is not null {code}  currently doesn't take into account that the path of the selector 'a' is known at the time selector 'b' is accessed (given that selector 'a' is executed first). So in this case, the query would use an index on the property b.[property], even if this index has a very bad selectivity (many nodes with this problem), or the query would use the node type index on [nodeTypeB], even if there are many nodes of this type.  Instead, most likely the query should do a traversal, using the isdescendantnode(b, a) join condition.
OAK-9b268da0$$Microkernel.diff returns empty diff when there are differences$${code} String rev1 = mk.commit("/", "+\"node1\":{\"node2\":{\"prop1\":\"val1\",\"prop2\":\"val2\"}}", null, null); String rev2 = mk.commit("/", "^\"node1/node2/prop1\":\"val1 new\" ^\"node1/node2/prop2\":null", null, null); String diff = mk.diff(rev1, rev2, "/node1/node2", 0); {code}  Here {{diff}} is empty although there are clearly differences between {{rev1}} and {{rev2}} at depth 0.
OAK-f0fbacab$$Node becomes invalid after Session#move()$$moving or renaming an existing (saved) node renders that node instance invalid and any access on that node instance will throw IllegalStateException.
OAK-df9e6913$$Calling addNode on a node that has orderable child nodes violates specification$$it seems to me that the current behavior of Node.addNode for a node that  has orderable child nodes violates the specification (section 23.3):  {quote} 23.3 Adding a New Child Node When a child node is added to a node that has orderable child nodes it is added to the end of the list. {quote}  however, the following test will fail:  {code} @Test     public void testAddNode() throws Exception {         new TestContentLoader().loadTestContent(getAdminSession());          Session session = getAdminSession();         Node test = session.getRootNode().addNode("test", "test:orderableFolder");         assertTrue(test.getPrimaryNodeType().hasOrderableChildNodes());          Node n1 = test.addNode("a");         Node n2 = test.addNode("b");         session.save();          NodeIterator it = test.getNodes();         assertEquals("a", it.nextNode().getName());         assertEquals("b", it.nextNode().getName());     } {code}
OAK-6feacf6b$$AssertionError in MemoryNodeBuilder$${code}     NodeBuilder root = ...     NodeBuilder child = root.child("new");      root.removeNode("new");     child.getChildNodeCount(); {code}  The last line throws an {{AssertionError}} when no node named "new" existed initially. It throws an {{IllegalStateException}} as expected otherwise.
OAK-00b4b8a0$$Moving or deleting tree instances with status NEW doesn't change its status to DISCONNECTED$$Further fall out from OAK-606:  {code}         Tree t = tree.addChild("new");          root.move("/x", "/y/x");         assertEquals(Status.DISCONNECTED, t.getStatus()); {code}  The assertion fails.
OAK-7a84b3a8$$NPE trying to add a node to an nt:folder node$$The following code throws a NPE:  {code} Session s = getAdminSession(); s.getRootNode().addNode("a", "nt:folder").addNode("b"); s.save();         {code}  Stack trace: {code} java.lang.NullPointerException at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:191) at org.apache.jackrabbit.oak.namepath.LocalNameMapper.getOakNameOrNull(LocalNameMapper.java:82) at org.apache.jackrabbit.oak.namepath.GlobalNameMapper.getOakName(GlobalNameMapper.java:64) at org.apache.jackrabbit.oak.namepath.NamePathMapperImpl.getOakName(NamePathMapperImpl.java:62) at org.apache.jackrabbit.oak.plugins.nodetype.ReadOnlyNodeTypeManager.getOakName(ReadOnlyNodeTypeManager.java:92) at org.apache.jackrabbit.oak.plugins.nodetype.ReadOnlyNodeTypeManager.getNodeType(ReadOnlyNodeTypeManager.java:186) at org.apache.jackrabbit.oak.jcr.NodeImpl 5.perform(NodeImpl.java:265) at org.apache.jackrabbit.oak.jcr.NodeImpl 5.perform(NodeImpl.java:1) at org.apache.jackrabbit.oak.jcr.SessionDelegate.perform(SessionDelegate.java:136) at org.apache.jackrabbit.oak.jcr.NodeImpl.addNode(NodeImpl.java:219) at org.apache.jackrabbit.oak.jcr.NodeImpl.addNode(NodeImpl.java:210) at org.apache.jackrabbit.oak.jcr.CRUDTest.nodeType(CRUDTest.java:122) {code}
OAK-55a4f738$$Revisit PrivilegeDefinitionStore's use of null as a child name parameter$$As discussed on OAK-635, I'm extracting the PrivilegeDefinitionStore code&patch into a dedicated issue.  Following the discussion on the dev list, I've filed it as a bug, as nulls are not considered valid input parameters.
OAK-6c54045d$$Access to disconnected MemoryNodeBuilder should throw IllegalStateException$$None
OAK-8ed779dc$$Assertion error when adding node with expanded name$${code} node.addNode("{http://foo}new"); {code}  results in an assertion error
OAK-35a7f014$$Malformed solr delete query$$Following OAK-734 the solr query tests are failing because of a parsing error on the wildcard delete query.  The exact query is 'path_exact:/test*', which apparently upsets the lucene parser somehow.  Full trace:  {code} SEVERE: org.apache.solr.common.SolrException: org.apache.lucene.queryparser.classic.ParseException: Cannot parse 'path_exact:/test*': Lexical error at line 1, column 18.  Encountered: <EOF> after : "/test*" 	at org.apache.solr.update.DirectUpdateHandler2.getQuery(DirectUpdateHandler2.java:328) 	at org.apache.solr.update.DirectUpdateHandler2.deleteByQuery(DirectUpdateHandler2.java:340) 	at org.apache.solr.update.processor.RunUpdateProcessor.processDelete(RunUpdateProcessorFactory.java:72) 	at org.apache.solr.update.processor.UpdateRequestProcessor.processDelete(UpdateRequestProcessor.java:55) 	at org.apache.solr.update.processor.DistributedUpdateProcessor.doLocalDelete(DistributedUpdateProcessor.java:437) 	at org.apache.solr.update.processor.DistributedUpdateProcessor.doDeleteByQuery(DistributedUpdateProcessor.java:835) 	at org.apache.solr.update.processor.DistributedUpdateProcessor.processDelete(DistributedUpdateProcessor.java:657) 	at org.apache.solr.update.processor.LogUpdateProcessor.processDelete(LogUpdateProcessorFactory.java:121) 	at org.apache.solr.handler.loader.XMLLoader.processDelete(XMLLoader.java:330) 	at org.apache.solr.handler.loader.XMLLoader.processUpdate(XMLLoader.java:261) 	at org.apache.solr.handler.loader.XMLLoader.load(XMLLoader.java:157) 	at org.apache.solr.handler.UpdateRequestHandler 1.load(UpdateRequestHandler.java:92) 	at org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:74) 	at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:129) 	at org.apache.solr.core.SolrCore.execute(SolrCore.java:1699) 	at org.apache.solr.client.solrj.embedded.EmbeddedSolrServer.request(EmbeddedSolrServer.java:150) 	at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:117) 	at org.apache.solr.client.solrj.SolrServer.deleteByQuery(SolrServer.java:285) 	at org.apache.solr.client.solrj.SolrServer.deleteByQuery(SolrServer.java:271) 	at org.apache.jackrabbit.oak.plugins.index.solr.index.SolrIndexUpdate.deleteSubtreeWriter(SolrIndexUpdate.java:161) 	at org.apache.jackrabbit.oak.plugins.index.solr.index.SolrIndexUpdate.apply(SolrIndexUpdate.java:98) 	at org.apache.jackrabbit.oak.plugins.index.solr.index.SolrIndexDiff.leave(SolrIndexDiff.java:202) 	at org.apache.jackrabbit.oak.spi.commit.CompositeEditor.leave(CompositeEditor.java:74) 	at org.apache.jackrabbit.oak.plugins.index.IndexHookManagerDiff.leave(IndexHookManagerDiff.java:117) 	at org.apache.jackrabbit.oak.spi.commit.EditorHook EditorDiff.process(EditorHook.java:115) 	at org.apache.jackrabbit.oak.spi.commit.EditorHook.process(EditorHook.java:80) 	at org.apache.jackrabbit.oak.spi.commit.EditorHook.processCommit(EditorHook.java:54) 	at org.apache.jackrabbit.oak.kernel.KernelNodeStoreBranch.merge(KernelNodeStoreBranch.java:144) 	at org.apache.jackrabbit.oak.core.RootImpl 2.run(RootImpl.java:266) 	at org.apache.jackrabbit.oak.core.RootImpl 2.run(RootImpl.java:1) 	at java.security.AccessController.doPrivileged(Native Method) 	at javax.security.auth.Subject.doAs(Subject.java:337) 	at org.apache.jackrabbit.oak.core.RootImpl.commit(RootImpl.java:261) 	at org.apache.jackrabbit.oak.query.AbstractQueryTest.test(AbstractQueryTest.java:236) 	at org.apache.jackrabbit.oak.plugins.index.solr.query.SolrIndexQueryTest.sql2(SolrIndexQueryTest.java:79) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 	at java.lang.reflect.Method.invoke(Method.java:597) 	at org.junit.runners.model.FrameworkMethod 1.runReflectiveCall(FrameworkMethod.java:44) 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41) 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20) 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28) 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50) 	at org.junit.runners.ParentRunner 3.run(ParentRunner.java:193) 	at org.junit.runners.ParentRunner 1.schedule(ParentRunner.java:52) 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191) 	at org.junit.runners.ParentRunner.access 000(ParentRunner.java:42) 	at org.junit.runners.ParentRunner 2.evaluate(ParentRunner.java:184) 	at org.junit.runners.ParentRunner.run(ParentRunner.java:236) 	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50) 	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197) Caused by: org.apache.lucene.queryparser.classic.ParseException: Cannot parse 'path_exact:/test*': Lexical error at line 1, column 18.  Encountered: <EOF> after : "/test*" 	at org.apache.lucene.queryparser.classic.QueryParserBase.parse(QueryParserBase.java:130) 	at org.apache.solr.search.LuceneQParser.parse(LuceneQParserPlugin.java:72) 	at org.apache.solr.search.QParser.getQuery(QParser.java:143) 	at org.apache.solr.update.DirectUpdateHandler2.getQuery(DirectUpdateHandler2.java:310) 	... 58 more Caused by: org.apache.lucene.queryparser.classic.TokenMgrError: Lexical error at line 1, column 18.  Encountered: <EOF> after : "/test*" 	at org.apache.lucene.queryparser.classic.QueryParserTokenManager.getNextToken(QueryParserTokenManager.java:1048) 	at org.apache.lucene.queryparser.classic.QueryParser.jj_ntk(QueryParser.java:638) 	at org.apache.lucene.queryparser.classic.QueryParser.Clause(QueryParser.java:246) 	at org.apache.lucene.queryparser.classic.QueryParser.Query(QueryParser.java:181) 	at org.apache.lucene.queryparser.classic.QueryParser.TopLevelQuery(QueryParser.java:170) 	at org.apache.lucene.queryparser.classic.QueryParserBase.parse(QueryParserBase.java:120) 	... 61 more {code}
OAK-503451c1$$ContentMirrorStoreStrategy #insert fails to enforce uniqueness and is slow$$Following OAK-734 I've noticed that the _ContentMirrorStoreStrategy_ fails to enforce the uniqueness constraints assumed on the #insert method.  It is also responsible for a slowdown on the #insert method because of the behavior change of the Property2Index (very frequent saves instead of a bulk one).
OAK-6fc5ea9d$$TreeImpl#*Location: unable retrieve child location if access to parent is denied$$as a consequence of OAK-709 we now have an issue with the way SessionDelegate and Root#getLocation access a node in the hierarchy which has an ancestor which is not accessible.  specifically RootImpl#getLocation will be served a NullLocation for the first ancestor which is not accessible and consequently any accessible child node cannot be accessed.  in order to reproduce the issue you may:  - change AccessControlConfigurationImpl to use PermissionProviderImpl instead   of the tmp solution - and run o.a.j.oak.jcr.security.authorization.ReadTest#testReadDenied
OAK-45b110e1$$MemoryNodeBuilder.setNode() loses property values$${code} builder.setNode("a", nodeA); builder.child("a").setProperty(...); {code}  After the 2nd line executed, properties initially present on {{nodeA}} are gone on {{builder.getNodeState()}}.
OAK-7acb091a$$Branch conflicts not detected by MongoMK$$MongoMK does not correctly detect conflicts when changes are committed into multiple branches concurrently and then merged back.  ConflictTest already covers conflict detection for non-branch commits and mixed branch/non-branch changes, but is missing tests for conflicting branches. I'll commit an ignored test to illustrate the problem.
OAK-65aa40dd$$Condition check broken in MemoryDocumentStore$$The Operation.CONTAINS_MAP_ENTRY condition check does not work correctly in the MemoryDocumentStore and may return false even when the condition is not met.
OAK-6d82cb64$$PathUtils#getDepth returns 1 for empty path$$PathUtils#getDepths that the root path / has depth 0. however, passing in a empty string is accepted and returns 1.  according to the API contract getDepth is counting the number of elements in the path which for "" should IMO be zero.
OAK-e1ae968c$$MongoMK: split documents when they are too large$$Currently, the MongoMK stores all revisions of a node in the same document. Once there are many revisions, the document gets very large.  The plan is to split the document when it gets big.  It looks like this isn't just a "nice to have", but also a problem for some use cases. Example stack trace:  {code} 21.07.2013 12:35:47.554 *ERROR* ... Caused by: java.lang.IllegalArgumentException: 'ok' should never be null... 	at com.mongodb.CommandResult.ok(CommandResult.java:48) 	at com.mongodb.DBCollection.findAndModify(DBCollection.java:375) 	at org.apache.jackrabbit.oak.plugins.mongomk.MongoDocumentStore.findAndModify(MongoDocumentStore.java:302) 	... 32 more {code}  at the same time in the MongoDB log:  {code} Sun Jul 21 12:35:47.334 [conn7] warning: log line attempted (159k) over max size(10k),  printing beginning and end ...  Assertion: 10334:BSONObj size: 16795219 (0x53460001) is invalid.  Size must be between 0 and 16793600(16MB)  First element: :childOrder: { r1400279f22d-0-1: "[]", ... {code}
OAK-0be7e8f0$$Tree has wrong parent after move$$After a move operation Tree.getParent() still returns the old parent.  {code} Tree x = r.getChild("x"); Tree y = r.getChild("y");  root.move("x", "y/x"); assertEquals("y", x.getParent().getName());  // Fails {code}
