Math-1$$Fraction specified with maxDenominator and a value very close to a simple fraction should not throw an overflow exception$$An overflow exception is thrown when a Fraction is initialized with a maxDenominator from a double that is very close to a simple fraction.  For example: double d = 0.5000000001; Fraction f = new Fraction(d, 10); Patch with unit test on way.
Math-2$$HypergeometricDistribution.sample suffers from integer overflow$$Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values – the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean() – instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.
Math-3$$ArrayIndexOutOfBoundsException in MathArrays.linearCombination$$When MathArrays.linearCombination is passed arguments with length 1, it throws an ArrayOutOfBoundsException. This is caused by this line: double prodHighNext = prodHigh[1]; linearCombination should check the length of the arguments and fall back to simple multiplication if length == 1.
Math-4$$NPE when calling SubLine.intersection() with non-intersecting lines$$When calling SubLine.intersection() with two lines that not intersect, then a NullPointerException is thrown in Line.toSubSpace(). This bug is in the twod and threed implementations. The attached patch fixes both implementations and adds the required test cases.
Math-5$$Complex.ZERO.reciprocal() returns NaN but should return INF.$$Complex.ZERO.reciprocal() returns NaN but should return INF. Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version  Id: Complex.java 1416643 2012-12-03 19:37:14Z tn  
Math-6$$LevenbergMarquardtOptimizer reports 0 iterations$$The method LevenbergMarquardtOptimizer.getIterations() does not report the correct number of iterations; It always returns 0. A quick look at the code shows that only SimplexOptimizer calls BaseOptimizer.incrementEvaluationsCount() I've put a test case below. Notice how the evaluations count is correctly incremented, but the iterations count is not.      @Test     public void testGetIterations() {         // setup         LevenbergMarquardtOptimizer otim = new LevenbergMarquardtOptimizer();          // action         otim.optimize(new MaxEval(100), new Target(new double[] { 1 }),                 new Weight(new double[] { 1 }), new InitialGuess(                         new double[] { 3 }), new ModelFunction(                         new MultivariateVectorFunction() {                             @Override                             public double[] value(double[] point)                                     throws IllegalArgumentException {                                 return new double[] { FastMath.pow(point[0], 4) };                             }                         }), new ModelFunctionJacobian(                         new MultivariateMatrixFunction() {                             @Override                             public double[][] value(double[] point)                                     throws IllegalArgumentException {                                 return new double[][] { { 0.25 * FastMath.pow(                                         point[0], 3) } };                             }                         }));          // verify         assertThat(otim.getEvaluations(), greaterThan(1));         assertThat(otim.getIterations(), greaterThan(1));     }
Math-7$$event state not updated if an unrelated event triggers a RESET_STATE during ODE integration$$When an ODE solver manages several different event types, there are some unwanted side effects. If one event handler asks for a RESET_STATE (for integration state) when its eventOccurred method is called, the other event handlers that did not trigger an event in the same step are not updated correctly, due to an early return. As a result, when the next step is processed with a reset integration state, the forgotten event still refer to the start date of the previous state. This implies that when these event handlers will be checked for In some cases, the function defining an event g(double t, double[] y) is called with state parameters y that are completely wrong. In one case when the y array should have contained values between -1 and +1, one function call got values up to 1.0e20. The attached file reproduces the problem.
Math-8$$DiscreteDistribution.sample(int) may throw an exception if first element of singletons of sub-class type$$Creating an array with Array.newInstance(singletons.get(0).getClass(), sampleSize) in DiscreteDistribution.sample(int) is risky. An exception will be thrown if:  singleons.get(0) is of type T1, an sub-class of T, and DiscreteDistribution.sample() returns an object which is of type T, but not of type T1.  To reproduce:  List<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>(); list.add(new Pair<Object, Double>(new Object() {}, new Double(0))); list.add(new Pair<Object, Double>(new Object() {}, new Double(1))); new DiscreteDistribution<Object>(list).sample(1);   Attaching a patch.
Math-9$$Line.revert() is imprecise$$Line.revert() only maintains ~10 digits for the direction. This becomes an issue when the line's position is evaluated far from the origin. A simple fix would be to use Vector3D.negate() for the direction. Also, is there a reason why Line is not immutable? It is just comprised of two vectors.
Math-10$$DerivativeStructure.atan2(y,x) does not handle special cases properly$$The four special cases +/-0 for both x and y should give the same values as Math.atan2 and FastMath.atan2. However, they give NaN for the value in all cases.
Math-11$$MultivariateNormalDistribution.density(double[]) returns wrong value when the dimension is odd$$To reproduce:  Assert.assertEquals(0.398942280401433, new MultivariateNormalDistribution(new double[]{0}, new double[][]{{1}}).density(new double[]{0}), 1e-15);
Math-12$$GammaDistribution cloning broken$$Serializing a GammaDistribution and deserializing it, does not result in a cloned distribution that produces the same samples. Cause: GammaDistribution inherits from AbstractRealDistribution, which implements Serializable. AbstractRealDistribution has random, in which we have a Well19937c instance, which inherits from AbstractWell. AbstractWell implements Serializable. AbstractWell inherits from BitsStreamGenerator, which is not Serializable, but does have a private field 'nextGaussian'. Solution: Make BitStreamGenerator implement Serializable as well. This probably affects other distributions as well.
Math-13$$new multivariate vector optimizers cannot be used with large number of weights$$When using the Weigth class to pass a large number of weights to multivariate vector optimizers, an nxn full matrix is created (and copied) when a n elements vector is used. This exhausts memory when n is large. This happens for example when using curve fitters (even simple curve fitters like polynomial ones for low degree) with large number of points. I encountered this with curve fitting on 41200 points, which created a matrix with 1.7 billion elements.
Math-14$$new multivariate vector optimizers cannot be used with large number of weights$$When using the Weigth class to pass a large number of weights to multivariate vector optimizers, an nxn full matrix is created (and copied) when a n elements vector is used. This exhausts memory when n is large. This happens for example when using curve fitters (even simple curve fitters like polynomial ones for low degree) with large number of points. I encountered this with curve fitting on 41200 points, which created a matrix with 1.7 billion elements.
Math-15$$FastMath.pow deviates from Math.pow for negative, finite base values with an exponent 2^52 < y < 2^53$$As reported by Jeff Hain: pow(double,double): Math.pow(-1.0,5.000000000000001E15) = -1.0 FastMath.pow(-1.0,5.000000000000001E15) = 1.0 ===> This is due to considering that power is an even integer if it is >= 2^52, while you need to test that it is >= 2^53 for it. ===> replace "if (y >= TWO_POWER_52 || y <= -TWO_POWER_52)" with "if (y >= 2*TWO_POWER_52 || y <= -2*TWO_POWER_52)" and that solves it.
Math-16$$FastMath.[cosh, sinh] do not support the same range of values as the Math counterparts$$As reported by Jeff Hain: cosh(double) and sinh(double): Math.cosh(709.783) = 8.991046692770538E307 FastMath.cosh(709.783) = Infinity Math.sinh(709.783) = 8.991046692770538E307 FastMath.sinh(709.783) = Infinity ===> This is due to using exp( x )/2 for values of |x| above 20: the result sometimes should not overflow, but exp( x ) does, so we end up with some infinity. ===> for values of |x| >= StrictMath.log(Double.MAX_VALUE), exp will overflow, so you need to use that instead: for x positive: double t = exp(x*0.5); return (0.5*t)*t; for x negative: double t = exp(-x*0.5); return (-0.5*t)*t;
Math-17$$Dfp Dfp.multiply(int x) does not comply with the general contract FieldElement.multiply(int n)$$In class org.apache.commons.math3.Dfp,  the method multiply(int n) is limited to 0 <= n <= 9999. This is not consistent with the general contract of FieldElement.multiply(int n), where there should be no limitation on the values of n.
Math-18$$CMAESOptimizer with bounds fits finely near lower bound and coarsely near upper bound.$$When fitting with bounds, the CMAESOptimizer fits finely near the lower bound and coarsely near the upper bound.  This is because it internally maps the fitted parameter range into the interval [0,1].  The unit of least precision (ulp) between floating point numbers is much smaller near zero than near one.  Thus, fits have much better resolution near the lower bound (which is mapped to zero) than the upper bound (which is mapped to one).  I will attach a example program to demonstrate.
Math-19$$Wide bounds to CMAESOptimizer result in NaN parameters passed to fitness function$$If you give large values as lower/upper bounds (for example -Double.MAX_VALUE as a lower bound), the optimizer can call the fitness function with parameters set to NaN.  My guess is this is due to FitnessFunction.encode/decode generating NaN when normalizing/denormalizing parameters.  For example, if the difference between the lower and upper bound is greater than Double.MAX_VALUE, encode could divide infinity by infinity.
Math-20$$CMAESOptimizer does not enforce bounds$$The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.
Math-21$$Correlated random vector generator fails (silently) when faced with zero rows in covariance matrix$$The following three matrices (which are basically permutations of each other) produce different results when sampling a multi-variate Gaussian with the help of CorrelatedRandomVectorGenerator (sample covariances calculated in R, based on 10,000 samples): Array2DRowRealMatrix { {0.0,0.0,0.0,0.0,0.0} , {0.0,0.013445532,0.01039469,0.009881156,0.010499559} , {0.0,0.01039469,0.023006616,0.008196856,0.010732709} , {0.0,0.009881156,0.008196856,0.019023866,0.009210099} , {0.0,0.010499559,0.010732709,0.009210099,0.019107243}} > cov(data1)    V1 V2 V3 V4 V5 V1 0 0.000000000 0.00000000 0.000000000 0.000000000 V2 0 0.013383931 0.01034401 0.009913271 0.010506733 V3 0 0.010344006 0.02309479 0.008374730 0.010759306 V4 0 0.009913271 0.00837473 0.019005488 0.009187287 V5 0 0.010506733 0.01075931 0.009187287 0.019021483 Array2DRowRealMatrix { {0.013445532,0.01039469,0.0,0.009881156,0.010499559} , {0.01039469,0.023006616,0.0,0.008196856,0.010732709} , {0.0,0.0,0.0,0.0,0.0}, {0.009881156,0.008196856,0.0,0.019023866,0.009210099}, {0.010499559,0.010732709,0.0,0.009210099,0.019107243}}  > cov(data2)             V1 V2 V3 V4 V5 V1 0.006922905 0.010507692 0 0.005817399 0.010330529 V2 0.010507692 0.023428918 0 0.008273152 0.010735568 V3 0.000000000 0.000000000 0 0.000000000 0.000000000 V4 0.005817399 0.008273152 0 0.004929843 0.009048759 V5 0.010330529 0.010735568 0 0.009048759 0.018683544   Array2DRowRealMatrix{ {0.013445532,0.01039469,0.009881156,0.010499559}, {0.01039469,0.023006616,0.008196856,0.010732709}, {0.009881156,0.008196856,0.019023866,0.009210099}, {0.010499559,0.010732709,0.009210099,0.019107243}}  > cov(data3)             V1          V2          V3          V4 V1 0.013445047 0.010478862 0.009955904 0.010529542 V2 0.010478862 0.022910522 0.008610113 0.011046353 V3 0.009955904 0.008610113 0.019250975 0.009464442 V4 0.010529542 0.011046353 0.009464442 0.019260317   I've traced this back to the RectangularCholeskyDecomposition, which does not seem to handle the second matrix very well (decompositions in the same order as the matrices above):  CorrelatedRandomVectorGenerator.getRootMatrix() =  Array2DRowRealMatrix{{0.0,0.0,0.0,0.0,0.0} , {0.0759577418122063,0.0876125188474239,0.0,0.0,0.0} , {0.07764443622513505,0.05132821221460752,0.11976381821791235,0.0,0.0} , {0.06662930527909404,0.05501661744114585,0.0016662506519307997,0.10749324207653632,0.0} ,{0.13822895138139477,0.0,0.0,0.0,0.0}} CorrelatedRandomVectorGenerator.getRank() = 5 CorrelatedRandomVectorGenerator.getRootMatrix() =  Array2DRowRealMatrix{{0.0759577418122063,0.034512751379448724,0.0}, {0.07764443622513505,0.13029949164628746,0.0} , {0.0,0.0,0.0} , {0.06662930527909404,0.023203936694855674,0.0} ,{0.13822895138139477,0.0,0.0}} CorrelatedRandomVectorGenerator.getRank() = 3 CorrelatedRandomVectorGenerator.getRootMatrix() =  Array2DRowRealMatrix{{0.0759577418122063,0.034512751379448724,0.033913748226348225,0.07303890149947785}, {0.07764443622513505,0.13029949164628746,0.0,0.0} , {0.06662930527909404,0.023203936694855674,0.11851573313229945,0.0} ,{0.13822895138139477,0.0,0.0,0.0}} CorrelatedRandomVectorGenerator.getRank() = 4 Clearly, the rank of each of these matrices should be 4. The first matrix does not lead to incorrect results, but the second one does. Unfortunately, I don't know enough about the Cholesky decomposition to find the flaw in the implementation, and I could not find documentation for the "rectangular" variant (also not at the links provided in the javadoc).
Math-22$$Fix and then deprecate isSupportXxxInclusive in RealDistribution interface$$The conclusion from [1] was never implemented. We should deprecate these properties from the RealDistribution interface, but since removal will have to wait until 4.0, we should agree on a precise definition and fix the code to match it in the mean time. The definition that I propose is that isSupportXxxInclusive means that when the density function is applied to the upper or lower bound of support returned by getSupportXxxBound, a finite (i.e. not infinite), not NaN value is returned. [1] http://markmail.org/message/dxuxh7eybl7xejde
Math-23$$"BrentOptimizer" not always reporting the best point$$BrentOptimizer (package "o.a.c.m.optimization.univariate") does not check that the point it is going to return is indeed the best one it has encountered. Indeed, the last evaluated point might be slightly worse than the one before last.
Math-24$$"BrentOptimizer" not always reporting the best point$$BrentOptimizer (package "o.a.c.m.optimization.univariate") does not check that the point it is going to return is indeed the best one it has encountered. Indeed, the last evaluated point might be slightly worse than the one before last.
Math-25$$"HarmonicFitter.ParameterGuesser" sometimes fails to return sensible values$$The inner class "ParameterGuesser" in "HarmonicFitter" (package "o.a.c.m.optimization.fitting") fails to compute a usable guess for the "amplitude" parameter.
Math-26$$Fraction(double, int) constructor strange behaviour$$The Fraction constructor Fraction(double, int) takes a double value and a int maximal denominator, and approximates a fraction. When the double value is a large, negative number with many digits in the fractional part, and the maximal denominator is a big, positive integer (in the 100'000s), two distinct bugs can manifest: 1: the constructor returns a positive Fraction. Calling Fraction(-33655.1677817278, 371880) returns the fraction 410517235/243036, which both has the wrong sign, and is far away from the absolute value of the given value 2: the constructor does not manage to reduce the Fraction properly. Calling Fraction(-43979.60679604749, 366081) returns the fraction -1651878166/256677, which should have* been reduced to -24654898/3831. I have, as of yet, not found a solution. The constructor looks like this: public Fraction(double value, int maxDenominator)         throws FractionConversionException     {        this(value, 0, maxDenominator, 100);     }  Increasing the 100 value (max iterations) does not fix the problem for all cases. Changing the 0-value (the epsilon, maximum allowed error) to something small does not work either, as this breaks the tests in FractionTest.  The problem is not neccissarily that the algorithm is unable to approximate a fraction correctly. A solution where a FractionConversionException had been thrown in each of these examples would probably be the best solution if an improvement on the approximation algorithm turns out to be hard to find. This bug has been found when trying to explore the idea of axiom-based testing (http://bldl.ii.uib.no/testing.html). Attached is a java test class FractionTestByAxiom (junit, goes into org.apache.commons.math3.fraction) which shows these bugs through a simplified approach to this kind of testing, and a text file describing some of the value/maxDenominator combinations which causes one of these failures.  It is never specified in the documentation that the Fraction class guarantees that completely reduced rational numbers are constructed, but a comment inside the equals method claims that "since fractions are always in lowest terms, numerators and can be compared directly for equality", so it seems like this is the intention.
Math-27$$Fraction percentageValue rare overflow$$The percentageValue() method of the Fraction class works by first multiplying the Fraction by 100, then converting the Fraction to a double. This causes overflows when the numerator is greater than Integer.MAX_VALUE/100, even when the value of the fraction is far below this value. The patch changes the method to first convert to a double value, and then multiply this value by 100 - the result should be the same, but with less overflows. An addition to the test for the method that covers this bug is also included.
Math-28$$Not expected UnboundedSolutionException$$SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR
Math-29$$Bugs in RealVector.ebeMultiply(RealVector) and ebeDivide(RealVector)$$OpenMapRealVector.ebeMultiply(RealVector) and OpenMapRealVector.ebeDivide(RealVector) return wrong values when one entry of the specified RealVector is nan or infinity. The bug is easy to understand. Here is the current implementation of ebeMultiply      public OpenMapRealVector ebeMultiply(RealVector v) {         checkVectorDimensions(v.getDimension());         OpenMapRealVector res = new OpenMapRealVector(this);         Iterator iter = entries.iterator();         while (iter.hasNext()) {             iter.advance();             res.setEntry(iter.key(), iter.value() * v.getEntry(iter.key()));         }         return res;     }   The assumption is that for any double x, x * 0d == 0d holds, which is not true. The bug is easy enough to identify, but more complex to solve. The only solution I can come up with is to loop through all entries of v (instead of those entries which correspond to non-zero entries of this). I'm afraid about performance losses.
Math-30$$Mann-Whitney U Test Suffers From Integer Overflow With Large Data Sets$$When performing a Mann-Whitney U Test on large data sets (the attached test uses two 1500 element sets), intermediate integer values used in calculateAsymptoticPValue can overflow, leading to invalid results, such as p-values of NaN, or incorrect calculations. Attached is a patch, including a test, and a fix, which modifies the affected code to use doubles
Math-31$$inverseCumulativeProbability of BinomialDistribution returns wrong value for large trials.$$The inverseCumulativeProbability method of the BinomialDistributionImpl class returns wrong value for large trials.  Following code will be reproduce the problem. System.out.println(new BinomialDistributionImpl(1000000, 0.5).inverseCumulativeProbability(0.5)); This returns 499525, though it should be 499999. I'm not sure how it should be fixed, but the cause is that the cumulativeProbability method returns Infinity, not NaN.  As the result the checkedCumulativeProbability method doesn't work as expected.
Math-32$$BSPTree class and recovery of a Euclidean 3D BRep$$New to the work here. Thanks for your efforts on this code. I create a BSPTree from a BoundaryRep (Brep) my test Brep is a cube as represented by a float array containing 8 3D points in(x,y,z) order and an array of indices (12 triplets for the 12 faces of the cube). I construct a BSPMesh() as shown in the code below. I can construct the PolyhedronsSet() but have problems extracting the faces from the BSPTree to reconstruct the BRep. The attached code (BSPMesh2.java) shows that a small change to 1 of the vertex positions causes/corrects the problem. Any ideas?
Math-33$$SimplexSolver gives bad results$$Methode SimplexSolver.optimeze(...) gives bad results with commons-math3-3.0 in a simple test problem. It works well in commons-math-2.2.
Math-34$$ListPopulation Iterator allows you to remove chromosomes from the population.$$Calling the iterator method of ListPopulation returns an iterator of the protected modifiable list. Before returning the iterator we should wrap it in an unmodifiable list.
Math-35$$Need range checks for elitismRate in ElitisticListPopulation constructors.$$There is a range check for setting the elitismRate via ElitisticListPopulation's setElitismRate method, but not via the constructors.
Math-36$$BigFraction.doubleValue() returns Double.NaN for large numerators or denominators$$The current implementation of doubleValue() divides numerator.doubleValue() / denominator.doubleValue().  BigInteger.doubleValue() fails for any number greater than Double.MAX_VALUE.  So if the user has 308-digit numerator or denominator, the resulting quotient fails, even in cases where the result would be well inside Double's range. I have a patch to fix it, if I can figure out how to attach it here I will.
Math-37$$[math] Complex Tanh for "big" numbers$$Hi, In Complex.java the tanh is computed with the following formula: tanh(a + bi) = sinh(2a)/(cosh(2a)+cos(2b)) + [sin(2b)/(cosh(2a)+cos(2b))]i The problem that I'm finding is that as soon as "a" is a "big" number, both sinh(2a) and cosh(2a) are infinity and then the method tanh returns in the real part NaN (infinity/infinity) when it should return 1.0. Wouldn't it be appropiate to add something as in the FastMath library??: if (real>20.0){       return createComplex(1.0, 0.0); } if (real<-20.0){       return createComplex(-1.0, 0.0); } Best regards, JBB
Math-38$$Errors in BOBYQAOptimizer when numberOfInterpolationPoints is greater than 2*dim+1$$I've been having trouble getting BOBYQA to minimize a function (actually a non-linear least squares fit) so as one change I increased the number of interpolation points.  It seems that anything larger than 2*dim+1 causes an error (typically at line 1662                    interpolationPoints.setEntry(nfm, ipt, interpolationPoints.getEntry(ipt, ipt)); I'm guessing there is an off by one error in the translation from FORTRAN.  Changing the BOBYQAOptimizerTest as follows (increasing number of interpolation points by one) will cause failures. Bruce Index: src/test/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizerTest.java =================================================================== — src/test/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizerTest.java	(revision 1221065) +++ src/test/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizerTest.java	(working copy) @@ -258,7 +258,7 @@  //        RealPointValuePair result = optim.optimize(100000, func, goal, startPoint);          final double[] lB = boundaries == null ? null : boundaries[0];          final double[] uB = boundaries == null ? null : boundaries[1];  BOBYQAOptimizer optim = new BOBYQAOptimizer(2 * dim + 1); +        BOBYQAOptimizer optim = new BOBYQAOptimizer(2 * dim + 2);          RealPointValuePair result = optim.optimize(maxEvaluations, func, goal, startPoint, lB, uB);  //        System.out.println(func.getClass().getName() + " = "   //              + optim.getEvaluations() + " f(");
Math-39$$too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)$$Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.
Math-40$$BracketingNthOrderBrentSolver exceeds maxIterationCount while updating always the same boundary$$In some cases, the aging feature in BracketingNthOrderBrentSolver fails. It attempts to balance the bracketing points by targeting a non-zero value instead of the real root. However, the chosen target is too close too zero, and the inverse polynomial approximation is always on the same side, thus always updates the same bracket. In the real used case for a large program, I had a bracket point xA = 12500.0, yA = 3.7e-16, agingA = 0, which is the (really good) estimate of the zero on one side of the root and xB = 12500.03, yB = -7.0e-5, agingB = 97. This shows that the bracketing interval is completely unbalanced, and we never succeed to rebalance it as we always updates (xA, yA) and never updates (xB, yB).
Math-41$$One of Variance.evaluate() methods does not work correctly$$The method org.apache.commons.math.stat.descriptive.moment.Variance.evaluate(double[] values, double[] weights, double mean, int begin, int length) does not work properly. Looks loke it ignores the length parameter and grabs the whole dataset. Similar method in Mean class seems to work. I did not check other methods taking the part of the array; they may have the same problem. Workaround: I had to shrink my arrays and use the method without the length.
Math-42$$Negative value with restrictNonNegative$$Problem: commons-math-2.2 SimplexSolver. A variable with 0 coefficient may be assigned a negative value nevertheless restrictToNonnegative flag in call: SimplexSolver.optimize(function, constraints, GoalType.MINIMIZE, true); Function 1 * x + 1 * y + 0 Constraints: 1 * x + 0 * y = 1 Result: x = 1; y = -1; Probably variables with 0 coefficients are omitted at some point of computation and because of that the restrictions do not affect their values.
Math-43$$Statistics.setVarianceImpl makes getStandardDeviation produce NaN$$Invoking SummaryStatistics.setVarianceImpl(new Variance(true/false) makes getStandardDeviation produce NaN. The code to reproduce it:  int[] scores = {1, 2, 3, 4}; SummaryStatistics stats = new SummaryStatistics(); stats.setVarianceImpl(new Variance(false)); //use "population variance" for(int i : scores) {   stats.addValue(i); } double sd = stats.getStandardDeviation(); System.out.println(sd);   A workaround suggested by Mikkel is:    double sd = FastMath.sqrt(stats.getSecondMoment() / stats.getN());
Math-44$$Incomplete reinitialization with some events handling$$I get a bug with event handling: I track 2 events that occur in the same step, when the first one is accepted, it resets the state but the reinitialization is not complete and the second one becomes unable to find its way. I can't give my context, which is rather large, but I tried a patch that works for me, unfortunately it breaks the unit tests.
Math-45$$Integer overflow in OpenMapRealMatrix$$computeKey() has an integer overflow. Since it is a sparse matrix, this is quite easily encountered long before heap space is exhausted. The attached code demonstrates the problem, which could potentially be a security vulnerability (for example, if one was to use this matrix to store access control information). Workaround: never create an OpenMapRealMatrix with more cells than are addressable with an int.
Math-46$$Division by zero$$In class Complex, division by zero always returns NaN. I think that it should return NaN only when the numerator is also ZERO, otherwise the result should be INF. See here.
Math-47$$Division by zero$$In class Complex, division by zero always returns NaN. I think that it should return NaN only when the numerator is also ZERO, otherwise the result should be INF. See here.
Math-48$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.
Math-49$$MathRuntimeException with simple ebeMultiply on OpenMapRealVector$$The following piece of code  import org.apache.commons.math.linear.OpenMapRealVector; import org.apache.commons.math.linear.RealVector;  public class DemoBugOpenMapRealVector {     public static void main(String[] args) {         final RealVector u = new OpenMapRealVector(3, 1E-6);         u.setEntry(0, 1.);         u.setEntry(1, 0.);         u.setEntry(2, 2.);         final RealVector v = new OpenMapRealVector(3, 1E-6);         v.setEntry(0, 0.);         v.setEntry(1, 3.);         v.setEntry(2, 0.);         System.out.println(u);         System.out.println(v);         System.out.println(u.ebeMultiply(v));     } }   raises an exception  org.apache.commons.math.linear.OpenMapRealVector@7170a9b6 Exception in thread "main" org.apache.commons.math.MathRuntimeException 6: map has been modified while iterating 	at org.apache.commons.math.MathRuntimeException.createConcurrentModificationException(MathRuntimeException.java:373) 	at org.apache.commons.math.util.OpenIntToDoubleHashMap Iterator.advance(OpenIntToDoubleHashMap.java:564) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:372) 	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:1) 	at DemoBugOpenMapRealVector.main(DemoBugOpenMapRealVector.java:17)
Math-50$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.
Math-51$$"RegulaFalsiSolver" failure$$The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.
Math-52$$numerical problems in rotation creation$$building a rotation from the following vector pairs leads to NaN: u1 = -4921140.837095533, -2.1512094250440013E7, -890093.279426377 u2 = -2.7238580938724895E9, -2.169664921341876E9, 6.749688708885301E10 v1 = 1, 0, 0 v2 = 0, 0, 1 The constructor first changes the (v1, v2) pair into (v1', v2') ensuring the following scalar products hold:  <v1'|v1'> == <u1|u1>  <v2'|v2'> == <u2|u2>  <u1 |u2>  == <v1'|v2'> Once the (v1', v2') pair has been computed, we compute the cross product:   k = (v1' - u1)^(v2' - u2) and the scalar product:   c = <k | (u1^u2)> By construction, c is positive or null and the quaternion axis we want to build is q = k/[2*sqrt(c)]. c should be null only if some of the vectors are aligned, and this is dealt with later in the algorithm. However, there are numerical problems with the vector above with the way these computations are done, as shown by the following comparisons, showing the result we get from our Java code and the result we get from manual computation with the same formulas but with enhanced precision: commons math:   k = 38514476.5,            -84.,                           -1168590144 high precision: k = 38514410.36093388...,  -0.374075245201180409222711..., -1168590152.10599715208... and it becomes worse when computing c because the vectors are almost orthogonal to each other, hence inducing additional cancellations. We get: commons math    c = -1.2397173627587605E20 high precision: c =  558382746168463196.7079627... We have lost ALL significant digits in cancellations, and even the sign is wrong!
Math-53$$Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same$$For both Complex add and subtract, the javadoc states that       * If either this or <code>rhs</code> has a NaN value in either part,      * {@link #NaN} is returned; otherwise Inifinite and NaN values are      * returned in the parts of the result according to the rules for      * {@link java.lang.Double} arithmetic   Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).
Math-54$$class Dfp toDouble method return -inf whan Dfp value is 0 "zero"$$I found a bug in the toDouble() method of the Dfp class. If the Dfp's value is 0 "zero", the toDouble() method returns a  negative infini. This is because the double value returned has an exposant equal to 0xFFF  and a significand is equal to 0. In the IEEE754 this is a -inf. To be equal to zero, the exposant and the significand must be equal to zero. A simple test case is : ---------------------------------------------- import org.apache.commons.math.dfp.DfpField; public class test { 	/**  @param args 	 */ 	public static void main(String[] args)  { 		DfpField field = new DfpField(100); 		System.out.println("toDouble value of getZero() ="+field.getZero().toDouble()+ 				"\ntoDouble value of newDfp(0.0) ="+ 				field.newDfp(0.0).toDouble()); 	} }  May be the simplest way to fix it is to test the zero equality at the begin of the toDouble() method, to be able to return the correctly signed zero ?
Math-55$$Vector3D.crossProduct is sensitive to numerical cancellation$$Cross product implementation uses the naive formulas (y1 z2 - y2 z1, ...). These formulas fail when vectors are almost colinear, like in the following example:  Vector3D v1 = new Vector3D(9070467121.0, 4535233560.0, 1); Vector3D v2 = new Vector3D(9070467123.0, 4535233561.0, 1); System.out.println(Vector3D.crossProduct(v1, v2));   The previous code displays  { -1, 2, 0 }  instead of the correct answer  { -1, 2, 1 }
Math-56$$MultidimensionalCounter.getCounts(int) returns wrong array of indices$$MultidimensionalCounter counter = new MultidimensionalCounter(2, 4); for (Integer i : counter) {     int[] x = counter.getCounts;     System.out.println(i + " " + Arrays.toString); } Output is: 0 [0, 0] 1 [0, 1] 2 [0, 2] 3 [0, 2]   <=== should be [0, 3] 4 [1, 0] 5 [1, 1] 6 [1, 2] 7 [1, 2]   <=== should be [1, 3]
Math-57$$Truncation issue in KMeansPlusPlusClusterer$$The for loop inside KMeansPlusPlusClusterer.chooseInitialClusters defines a variable   int sum = 0; This variable should have type double, rather than int.  Using an int causes the method to truncate the distances between points to (square roots of) integers.  It's especially bad when the distances between points are typically less than 1. As an aside, in version 2.2, this bug manifested itself by making the clusterer return empty clusters.  I wonder if the EmptyClusterStrategy would still be necessary if this bug were fixed.
Math-58$$GaussianFitter Unexpectedly Throws NotStrictlyPositiveException$$Running the following:     	double[] observations =   {      			1.1143831578403364E-29,      			 4.95281403484594E-28,      			 1.1171347211930288E-26,      			 1.7044813962636277E-25,      			 1.9784716574832164E-24,      			 1.8630236407866774E-23,      			 1.4820532905097742E-22,      			 1.0241963854632831E-21,      			 6.275077366673128E-21,      			 3.461808994532493E-20,      			 1.7407124684715706E-19,      			 8.056687953553974E-19,      			 3.460193945992071E-18,      			 1.3883326374011525E-17,      			 5.233894983671116E-17,      			 1.8630791465263745E-16,      			 6.288759227922111E-16,      			 2.0204433920597856E-15,      			 6.198768938576155E-15,      			 1.821419346860626E-14,      			 5.139176445538471E-14,      			 1.3956427429045787E-13,      			 3.655705706448139E-13,      			 9.253753324779779E-13,      			 2.267636001476696E-12,      			 5.3880460095836855E-12,      			 1.2431632654852931E-11      	} ;     	GaussianFitter g =      		new GaussianFitter(new LevenbergMarquardtOptimizer());     	for (int index = 0; index < 27; index++)     	{     		g.addObservedPoint(index, observations[index]);     	}        	g.fit(); Results in: org.apache.commons.math.exception.NotStrictlyPositiveException: -1.277 is smaller than, or equal to, the minimum (0) 	at org.apache.commons.math.analysis.function.Gaussian Parametric.validateParameters(Gaussian.java:184) 	at org.apache.commons.math.analysis.function.Gaussian Parametric.value(Gaussian.java:129) I'm guessing the initial guess for sigma is off.
Math-59$$FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f$$FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f. This is because the wrong variable is returned. The bug was not detected by the test case "testMinMaxFloat()" because that has a bug too - it tests doubles, not floats.
Math-60$$ConvergenceException in NormalDistributionImpl.cumulativeProbability()$$I get a ConvergenceException in  NormalDistributionImpl.cumulativeProbability() for very large/small parameters including Infinity, -Infinity. For instance in the following code: 	@Test 	public void testCumulative() { 		final NormalDistribution nd = new NormalDistributionImpl(); 		for (int i = 0; i < 500; i++) { 			final double val = Math.exp; 			try  { 				System.out.println("val = " + val + " cumulative = " + nd.cumulativeProbability(val)); 			}  catch (MathException e)  { 				e.printStackTrace(); 				fail(); 			} 		} 	} In version 2.0, I get no exception.  My suggestion is to change in the implementation of cumulativeProbability(double) to catch all ConvergenceException (and return for very large and very small values), not just MaxIterationsExceededException.
Math-61$$Dangerous code in "PoissonDistributionImpl"$$In the following excerpt from class "PoissonDistributionImpl": PoissonDistributionImpl.java     public PoissonDistributionImpl(double p, NormalDistribution z) {         super();         setNormal(z);         setMean(p);     }   (1) Overridable methods are called within the constructor. (2) The reference "z" is stored and modified within the class. I've encountered problem (1) in several classes while working on issue 348. In those cases, in order to remove potential problems, I copied/pasted the body of the "setter" methods inside the constructor but I think that a more elegant solution would be to remove the "setters" altogether (i.e. make the classes immutable). Problem (2) can also create unexpected behaviour. Is it really necessary to pass the "NormalDistribution" object; can't it be always created within the class?
Math-62$$Miscellaneous issues concerning the "optimization" package$$Revision 990792 contains changes triggered the following issues:  MATH-394 MATH-397 MATH-404  This issue collects the currently still unsatisfactory code (not necessarily sorted in order of annoyance):  "BrentOptimizer": a specific convergence checker must be used. "LevenbergMarquardtOptimizer" also has specific convergence checks. Trying to make convergence checking independent of the optimization algorithm creates problems (conceptual and practical): 	 See "BrentOptimizer" and "LevenbergMarquardtOptimizer", the algorithm passes "points" to the convergence checker, but the actual meaning of the points can very well be different in the caller (optimization algorithm) and the callee (convergence checker). In "PowellOptimizer" the line search ("BrentOptimizer") tolerances depend on the tolerances within the main algorithm. Since tolerances come with "ConvergenceChecker" and so can be changed at any time, it is awkward to adapt the values within the line search optimizer without exposing its internals ("BrentOptimizer" field) to the enclosing class ("PowellOptimizer").   Given the numerous changes, some Javadoc comments might be out-of-sync, although I did try to update them all. Class "DirectSearchOptimizer" (in package "optimization.direct") inherits from class "AbstractScalarOptimizer" (in package "optimization.general"). Some interfaces are defined in package "optimization" but their base implementations (abstract class that contain the boiler-plate code) are in package "optimization.general" (e.g. "DifferentiableMultivariateVectorialOptimizer" and "BaseAbstractVectorialOptimizer"). No check is performed to ensure the the convergence checker has been set (see e.g. "BrentOptimizer" and "PowellOptimizer"); if it hasn't there will be a NPE. The alternative is to initialize a default checker that will never be used in case the user had intended to explicitly sets the checker. "NonLinearConjugateGradientOptimizer": Ugly workaround for the checked "ConvergenceException". Everywhere, we trail the checked "FunctionEvaluationException" although it is never used. There remains some duplicate code (such as the "multi-start loop" in the various "MultiStart..." implementations). The "ConvergenceChecker" interface is very general (the "converged" method can take any number of "...PointValuePair"). However there remains a "semantic" problem: One cannot be sure that the list of points means the same thing for the caller of "converged" and within the implementation of the "ConvergenceChecker" that was independently set. It is not clear whether it is wise to aggregate the counter of gradient evaluations to the function evaluation counter. In "LevenbergMarquartdOptimizer" for example, it would be unfair to do so. Currently I had to remove all tests referring to gradient and Jacobian evaluations. In "AbstractLeastSquaresOptimizer" and "LevenbergMarquardtOptimizer", occurences of "OptimizationException" were replaced by the unchecked "ConvergenceException" but in some cases it might not be the most appropriate one. "MultiStartUnivariateRealOptimizer": in the other classes ("MultiStartMultivariate...") similar to this one, the randomization is on the firts-guess value while in this class, it is on the search interval. I think that here also we should randomly choose the start value (within the user-selected interval). The Javadoc utility raises warnings (see output of "mvn site") which I couldn't figure out how to correct. Some previously existing classes and interfaces have become no more than a specialisation of new "generics" classes; it might be interesting to remove them in order to reduce the number of classes and thus limit the potential for confusion.
Math-63$$NaN in "equals" methods$$In "MathUtils", some "equals" methods will return true if both argument are NaN. Unless I'm mistaken, this contradicts the IEEE standard. If nobody objects, I'm going to make the changes.
Math-64$$Inconsistent result from Levenberg-Marquardt$$Levenberg-Marquardt (its method doOptimize) returns a VectorialPointValuePair.  However, the class holds the optimum point, the vector of the objective function, the cost and residuals.  The value returns by doOptimize does not always corresponds to the point which leads to the residuals and cost
Math-65$$weight versus sigma in AbstractLeastSquares$$In AbstractLeastSquares, residualsWeights contains the WEIGHTS assigned to each observation.  In the method getRMS(), these weights are multiplicative as they should. unlike in getChiSquare() where it appears at the denominator!   If the weight is really the weight of the observation, it should multiply the square of the residual even in the computation of the chi2.  Once corrected, getRMS() can even reduce  public double getRMS()  {return Math.sqrt(getChiSquare()/rows);}
Math-66$$Bugs in "BrentOptimizer"$$I apologize for having provided a buggy implementation of Brent's optimization algorithm (class "BrentOptimizer" in package "optimization.univariate"). The unit tests didn't show that there was something wrong, although (from the "changes.xml" file) I discovered that, at the time, Luc had noticed something weird in the implementation's behaviour. Comparing with an implementation in Python, I could figure out the fixes. I'll modify "BrentOptimizer" and add a test. I also propose to change the name of the unit test class from "BrentMinimizerTest" to "BrentOptimizerTest".
Math-67$$Method "getResult()" in "MultiStartUnivariateRealOptimizer"$$In "MultiStartUnivariateRealOptimizer" (package "optimization"), the method "getResult" returns the result of the last run of the "underlying" optimizer; this last result might not be the best one, in which case it will not correspond to the value returned by the "optimize" method. This is confusing and does not seem very useful. I think that "getResult" should be defined as    public double getResult() {     return optima[0]; }   and similarly  public double getFunctionValue() {     return optimaValues[0]; }
Math-68$$LevenbergMarquardtOptimizer ignores the VectorialConvergenceChecker parameter passed to it$$LevenbergMarquardtOptimizer ignores the VectorialConvergenceChecker parameter passed to it. This makes it hard to specify custom stopping criteria for the optimizer.
Math-69$$PearsonsCorrelation.getCorrelationPValues() precision limited by machine epsilon$$Similar to the issue described in MATH-201, using PearsonsCorrelation.getCorrelationPValues() with many treatments results in p-values that are continuous down to 2.2e-16 but that drop to 0 after that. In MATH-201, the problem was described as such: > So in essence, the p-value returned by TTestImpl.tTest() is: >  > 1.0 - (cumulativeProbability(t) - cumulativeProbabily(-t)) >  > For large-ish t-statistics, cumulativeProbabilty(-t) can get quite small, and cumulativeProbabilty(t) can get very close to 1.0. When  > cumulativeProbability(-t) is less than the machine epsilon, we get p-values equal to zero because: >  > 1.0 - 1.0 + 0.0 = 0.0 The solution in MATH-201 was to modify the p-value calculation to this: > p = 2.0 * cumulativeProbability(-t) Here, the problem is similar.  From PearsonsCorrelation.getCorrelationPValues():   p = 2 * (1 - tDistribution.cumulativeProbability(t)); Directly calculating the p-value using identical code as PearsonsCorrelation.getCorrelationPValues(), but with the following change seems to solve the problem:   p = 2 * (tDistribution.cumulativeProbability(-t));
Math-70$$BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException$$Method      BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)   invokes      BisectionSolver.solve(double min, double max)  which throws NullPointerException, as member variable     UnivariateRealSolverImpl.f  is null. Instead the method:     BisectionSolver.solve(final UnivariateRealFunction f, double min, double max) should be called. Steps to reproduce: invoke:      new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5); NullPointerException will be thrown.
Math-71$$ODE integrator goes past specified end of integration range$$End of integration range in ODE solving is handled as an event. In some cases, numerical accuracy in events detection leads to error in events location. The following test case shows the end event is not handled properly and an integration that should cover a 60s range in fact covers a 160s range, more than twice the specified range.    public void testMissedEvent() throws IntegratorException, DerivativeException {           final double t0 = 1878250320.0000029;           final double t =  1878250379.9999986;           FirstOrderDifferentialEquations ode = new FirstOrderDifferentialEquations() {                          public int getDimension() {                 return 1;             }                          public void computeDerivatives(double t, double[] y, double[] yDot)                 throws DerivativeException {                 yDot[0] = y[0] * 1.0e-6;             }         };          DormandPrince853Integrator integrator = new DormandPrince853Integrator(0.0, 100.0,                                                                                1.0e-10, 1.0e-10);          double[] y = { 1.0 };         integrator.setInitialStepSize(60.0);         double finalT = integrator.integrate(ode, t0, y, t, y);         Assert.assertEquals(t, finalT, 1.0e-6);     }
Math-72$$Brent solver returns the wrong value if either bracket endpoint is root$$The solve(final UnivariateRealFunction f, final double min, final double max, final double initial) function returns yMin or yMax if min or max are deemed to be roots, respectively, instead of min or max.
Math-73$$Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign$$Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.
Math-74$$Wrong parameter for first step size guess for Embedded Runge Kutta methods$$In a space application using DOP853 i detected what seems to be a bad parameter in the call to the method  initializeStep of class AdaptiveStepsizeIntegrator. Here, DormandPrince853Integrator is a subclass for EmbeddedRungeKuttaIntegrator which perform the call to initializeStep at the beginning of its method integrate(...) The problem comes from the array "scale" that is used as a parameter in the call off initializeStep(..) Following the theory described by Hairer in his book "Solving Ordinary Differential Equations 1 : Nonstiff Problems", the scaling should be : sci = Atol i + |y0i| * Rtoli Whereas EmbeddedRungeKuttaIntegrator uses :  sci = Atoli Note that the Gragg-Bulirsch-Stoer integrator uses the good implementation "sci = Atol i + |y0i| * Rtoli  " when he performs the call to the same method initializeStep(..) In the method initializeStep, the error leads to a wrong step size h used to perform an  Euler step. Most of the time it is unvisible for the user. But in my space application the Euler step with this wrong step size h (much bigger than it should be)  makes an exception occur (my satellite hits the ground...) To fix the bug, one should use the same algorithm as in the rescale method in GraggBulirschStoerIntegrator For exemple :  final double[] scale= new double[y0.length];;           if (vecAbsoluteTolerance == null) {               for (int i = 0; i < scale.length; ++i)  {                 final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));                 scale[i] = scalAbsoluteTolerance + scalRelativeTolerance * yi;               }             } else {               for (int i = 0; i < scale.length; ++i)  {                 final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));                 scale[i] = vecAbsoluteTolerance[i] + vecRelativeTolerance[i] * yi;               }             }           hNew = initializeStep(equations, forward, getOrder(), scale,                            stepStart, y, yDotK[0], yTmp, yDotK[1]); Sorry for the length of this message, looking forward to hearing from you soon Vincent Morand
Math-75$$In stat.Frequency, getPct(Object) uses getCumPct(Comparable) instead of getPct(Comparable)$$Drop in Replacement of 1.2 with 2.0 not possible because all getPct calls will be cummulative without code change Frequency.java    /**  Returns the percentage of values that are equal to v @deprecated replaced by  {@link #getPct(Comparable)}  as of 2.0      */     @Deprecated     public double getPct(Object v)  {         return getCumPct((Comparable<?>) v);     }
Math-76$$NaN singular value from SVD$$The following jython code Start code from org.apache.commons.math.linear import * Alist = [[1.0, 2.0, 3.0],[2.0,3.0,4.0],[3.0,5.0,7.0]] A = Array2DRowRealMatrix(Alist) decomp = SingularValueDecompositionImpl(A) print decomp.getSingularValues() End code prints array('d', [11.218599757513008, 0.3781791648535976, nan]) The last singular value should be something very close to 0 since the matrix is rank deficient.  When i use the result from getSolver() to solve a system, i end  up with a bunch of NaNs in the solution.  I assumed i would get back a least squares solution. Does this SVD implementation require that the matrix be full rank?  If so, then i would expect an exception to be thrown from the constructor or one of the methods.
Math-77$$getLInfNorm() uses wrong formula in both ArrayRealVector and OpenMapRealVector (in different ways)$$the L_infinity norm of a finite dimensional vector is just the max of the absolute value of its entries. The current implementation in ArrayRealVector has a typo:      public double getLInfNorm() {         double max = 0;         for (double a : data) {             max += Math.max(max, Math.abs(a));         }         return max;     }   the += should just be an =. There is sadly a unit test assuring us that this is the correct behavior (effectively a regression-only test, not a test for correctness). Worse, the implementation in OpenMapRealVector is not even positive semi-definite:          public double getLInfNorm() {         double max = 0;         Iterator iter = entries.iterator();         while (iter.hasNext()) {             iter.advance();             max += iter.value();         }         return max;     }   I would suggest that this method be moved up to the AbstractRealVector superclass and implemented using the sparseIterator():    public double getLInfNorm() {     double norm = 0;     Iterator<Entry> it = sparseIterator();     Entry e;     while(it.hasNext() && (e = it.next()) != null) {       norm = Math.max(norm, Math.abs(e.getValue()));     }     return norm;   }   Unit tests with negative valued vectors would be helpful to check for this kind of thing in the future.
Math-78$$during ODE integration, the last event in a pair of very close event may not be detected$$When an events follows a previous one very closely, it may be ignored. The occurrence of the bug depends on the side of the bracketing interval that was selected. For example consider a switching function that is increasing around first event around t = 90, reaches its maximum and is decreasing around the second event around t = 135. If an integration step spans from 67.5 and 112.5, the switching function values at start and end of step will  have opposite signs, so the first event will be detected. The solver will find the event really occurs at 90.0 and will therefore truncate the step at 90.0. The next step will start from where the first step ends, i.e. it will start at 90.0. Let's say this step spans from 90.0 to 153.0. The switching function switches once again in this step. If the solver for the first event converged to a value slightly before 90.0 (say 89.9999999), then the switch will not be detected because g(89.9999999) and g(153.0) are both negative. This bug was introduced as of r781157 (2009-06-02) when special handling of events very close to step start was added.
Math-79$$NPE in  KMeansPlusPlusClusterer unittest$$When running this unittest, I am facing this NPE: java.lang.NullPointerException 	at org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer.assignPointsToClusters(KMeansPlusPlusClusterer.java:91) This is the unittest: package org.fao.fisheries.chronicles.calcuation.cluster; import static org.junit.Assert.assertEquals; import static org.junit.Assert.assertTrue; import java.util.Arrays; import java.util.List; import java.util.Random; import org.apache.commons.math.stat.clustering.Cluster; import org.apache.commons.math.stat.clustering.EuclideanIntegerPoint; import org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer; import org.fao.fisheries.chronicles.input.CsvImportProcess; import org.fao.fisheries.chronicles.input.Top200Csv; import org.junit.Test; public class ClusterAnalysisTest { 	@Test 	public void testPerformClusterAnalysis2() { 		KMeansPlusPlusClusterer<EuclideanIntegerPoint> transformer = new KMeansPlusPlusClusterer<EuclideanIntegerPoint>( 				new Random(1746432956321l)); 		EuclideanIntegerPoint[] points = new EuclideanIntegerPoint[] { 				new EuclideanIntegerPoint(new int[]  { 1959, 325100 } ), 				new EuclideanIntegerPoint(new int[]  { 1960, 373200 } ), }; 		List<Cluster<EuclideanIntegerPoint>> clusters = transformer.cluster(Arrays.asList(points), 1, 1); 		assertEquals(1, clusters.size()); 	} }
Math-80$$wrong result in eigen decomposition$$Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {         	  7484.860960227216, 18405.28129035345, 13855.225609560746,         	 10016.708722343366, 559.8117399576674, 6750.190788301587,          	    71.21428769782159         };         double[] secondaryTridiagonal = {         	 -4175.088570476366,1975.7955858241994,5193.178422374075,          	  1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {         		20654.744890306974412,16828.208208485466457,         		6893.155912634994820,6757.083016675340332,         		5887.799885688558788,64.309089923240379,         		57.992628792736340         };         RealVector[] refEigenVectors = {         		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),         		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),         		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),         		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),         		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),         		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),         		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }
Math-81$$ArrayIndexOutOfBoundException in EigenDecompositionImpl$$The following test triggers an ArrayIndexOutOfBoundException:      public void testMath308() {          double[] mainTridiagonal = {             22.330154644539597, 46.65485522478641, 17.393672330044705, 54.46687435351116, 80.17800767709437         };         double[] secondaryTridiagonal = {             13.04450406501361, -5.977590941539671, 2.9040909856707517, 7.1570352792841225         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {             14.138204224043099, 18.847969733754262, 52.536278520113882, 53.456697699894512, 82.044413207204002         };         RealVector[] refEigenVectors = {             new ArrayRealVector(new double[] {  0.584677060845929, -0.367177264979103, -0.721453187784497,  0.052971054621812, -0.005740715188257 }),             new ArrayRealVector(new double[] {  0.713933751051495, -0.190582113553930,  0.671410443368332, -0.056056055955050,  0.006541576993581 }),             new ArrayRealVector(new double[] {  0.222368839324646,  0.514921891363332, -0.021377019336614,  0.801196801016305, -0.207446991247740 }),             new ArrayRealVector(new double[] {  0.314647769490148,  0.750806415553905, -0.167700312025760, -0.537092972407375,  0.143854968127780 }),             new ArrayRealVector(new double[] { -0.000462690386766, -0.002118073109055,  0.011530080757413,  0.252322434584915,  0.967572088232592 })         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-6);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             }         }      }   Running the previous method as a Junit test triggers the exception when the EigenDecompositionImpl instance is built. The first few lines of the stack trace are:  java.lang.ArrayIndexOutOfBoundsException: -1 	at org.apache.commons.math.linear.EigenDecompositionImpl.computeShiftIncrement(EigenDecompositionImpl.java:1545) 	at org.apache.commons.math.linear.EigenDecompositionImpl.goodStep(EigenDecompositionImpl.java:1072) 	at org.apache.commons.math.linear.EigenDecompositionImpl.processGeneralBlock(EigenDecompositionImpl.java:894) 	at org.apache.commons.math.linear.EigenDecompositionImpl.findEigenvalues(EigenDecompositionImpl.java:658) 	at org.apache.commons.math.linear.EigenDecompositionImpl.decompose(EigenDecompositionImpl.java:246) 	at org.apache.commons.math.linear.EigenDecompositionImpl.<init>(EigenDecompositionImpl.java:205) 	at org.apache.commons.math.linear.EigenDecompositionImplTest.testMath308(EigenDecompositionImplTest.java:136)   I'm currently investigating this bug. It is not a simple index translation error between the original fortran (Lapack) and commons-math implementation.
Math-82$$SimplexSolver not working as expected 2$$SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).
Math-83$$SimplexSolver not working as expected?$$I guess (but I could be wrong) that SimplexSolver does not always return the optimal solution, nor satisfies all the constraints... Consider this LP: max: 0.8 x0 + 0.2 x1 + 0.7 x2 + 0.3 x3 + 0.6 x4 + 0.4 x5; r1: x0 + x2 + x4 = 23.0; r2: x1 + x3 + x5 = 23.0; r3: x0 >= 10.0; r4: x2 >= 8.0; r5: x4 >= 5.0; LPSolve returns 25.8, with x0 = 10.0, x1 = 0.0, x2 = 8.0, x3 = 0.0, x4 = 5.0, x5 = 23.0; The same LP expressed in Apache commons math is: LinearObjectiveFunction f = new LinearObjectiveFunction(new double[]  { 0.8, 0.2, 0.7, 0.3, 0.6, 0.4 } , 0 ); Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint>(); constraints.add(new LinearConstraint(new double[]  { 1, 0, 1, 0, 1, 0 } , Relationship.EQ, 23.0)); constraints.add(new LinearConstraint(new double[]  { 0, 1, 0, 1, 0, 1 } , Relationship.EQ, 23.0)); constraints.add(new LinearConstraint(new double[]  { 1, 0, 0, 0, 0, 0 } , Relationship.GEQ, 10.0)); constraints.add(new LinearConstraint(new double[]  { 0, 0, 1, 0, 0, 0 } , Relationship.GEQ, 8.0)); constraints.add(new LinearConstraint(new double[]  { 0, 0, 0, 0, 1, 0 } , Relationship.GEQ, 5.0)); RealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true); that returns 22.20, with x0 = 15.0, x1 = 23.0, x2 = 8.0, x3 = 0.0, x4 = 0.0, x5 = 0.0; Is it possible SimplexSolver is buggy that way? The returned value is 22.20 instead of 25.8, and the last constraint (x4 >= 5.0) is not satisfied... Am I using the interface wrongly?
Math-84$$MultiDirectional optimzation loops forver if started at the correct solution$$MultiDirectional.iterateSimplex loops forever if the starting point is the correct solution. see the attached test case (testMultiDirectionalCorrectStart) as an example.
Math-85$$bug in inverseCumulativeProbability() for Normal Distribution$$@version  Revision: 617953    Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008)    */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version  Revision: 506600    Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007)    */ public abstract class AbstractContinuousDistribution  This code:         	DistributionFactory factory = app.getDistributionFactory();         	NormalDistribution normal = factory.createNormalDistribution(0,1);         	double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0 	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103) 	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)
Math-86$$testing for symmetric positive definite matrix in CholeskyDecomposition$$I used this matrix:         double[][] cv = {  {0.40434286, 0.09376327, 0.30328980, 0.04909388} ,  {0.09376327, 0.10400408, 0.07137959, 0.04762857} ,  {0.30328980, 0.07137959, 0.30458776, 0.04882449},             {0.04909388, 0.04762857, 0.04882449, 0.07543265}         };  And it works fine, because it is symmetric positive definite  I tried this matrix:          double[][] cv = {             {0.40434286, -0.09376327, 0.30328980, 0.04909388},             {-0.09376327, 0.10400408, 0.07137959, 0.04762857},             {0.30328980, 0.07137959, 0.30458776, 0.04882449} ,             {0.04909388, 0.04762857, 0.04882449, 0.07543265}         }; And it should throw an exception but it does not.  I tested the matrix in R and R's cholesky decomposition method returns that the matrix is not symmetric positive definite. Obviously your code is not catching this appropriately. By the way (in my opinion) the use of exceptions to check these conditions is not the best design or use for exceptions.  If you are going to force the use to try and catch these exceptions at least provide methods  to test the conditions prior to the possibility of the exception.
Math-87$$Basic variable is not found correctly in simplex tableau$$The last patch to SimplexTableau caused an automated test suite I'm running at work to go down a new code path and uncover what is hopefully the last bug remaining in the Simplex code. SimplexTableau was assuming an entry in the tableau had to be nonzero to indicate a basic variable, which is incorrect - the entry should have a value equal to 1.
Math-88$$Simplex Solver arrives at incorrect solution$$I have reduced the problem reported to me down to a minimal test case which I will attach.
Math-89$$Bugs in Frequency API$$I think the existing Frequency API has some bugs in it. The addValue(Object v) method allows one to add a plain Object, but one cannot add anything further to the instance, as the second add fails with IllegalArgumentException. In fact, the problem is with the first call to addValue(Object) which should not allow a plain Object to be added - it should only allow Comparable objects. This could be fixed by checking that the object is Comparable. Similar considerations apply to the getCumFreq(Object) and getCumPct(Object) methods - they will only work with objects that implement Comparable. The getCount(Object) and getPct(Object) methods don't fail when given a non-Comparable object (because the class cast exception is caught), however they just return 0 as if the object was not present:          final Object OBJ = new Object();         f.addValue(OBJ); // This ought to fail, but doesn't, causing the unexpected behaviour below         System.out.println(f.getCount(OBJ)); // 0         System.out.println(f.getPct(OBJ)); // 0.0   Rather than adding extra checks for Comparable, it seems to me that the API would be much improved by using Comparable instead of Object. Also, it should make it easier to implement generics. However, this would cause compilation failures for some programs that pass Object rather than Comparable to the class. These would need recoding, but I think they would continue to run OK against the new API. It would also affect the run-time behaviour slightly, as the first attempt to add a non-Comparable object would fail, rather than the second add of a possibly valid object. But is that a viable program? It can only add one object, and any attempt to get statistics will either return 0 or an Exception, and applying the instanceof fix would also cause it to fail.
Math-90$$Bugs in Frequency API$$I think the existing Frequency API has some bugs in it. The addValue(Object v) method allows one to add a plain Object, but one cannot add anything further to the instance, as the second add fails with IllegalArgumentException. In fact, the problem is with the first call to addValue(Object) which should not allow a plain Object to be added - it should only allow Comparable objects. This could be fixed by checking that the object is Comparable. Similar considerations apply to the getCumFreq(Object) and getCumPct(Object) methods - they will only work with objects that implement Comparable. The getCount(Object) and getPct(Object) methods don't fail when given a non-Comparable object (because the class cast exception is caught), however they just return 0 as if the object was not present:          final Object OBJ = new Object();         f.addValue(OBJ); // This ought to fail, but doesn't, causing the unexpected behaviour below         System.out.println(f.getCount(OBJ)); // 0         System.out.println(f.getPct(OBJ)); // 0.0   Rather than adding extra checks for Comparable, it seems to me that the API would be much improved by using Comparable instead of Object. Also, it should make it easier to implement generics. However, this would cause compilation failures for some programs that pass Object rather than Comparable to the class. These would need recoding, but I think they would continue to run OK against the new API. It would also affect the run-time behaviour slightly, as the first attempt to add a non-Comparable object would fail, rather than the second add of a possibly valid object. But is that a viable program? It can only add one object, and any attempt to get statistics will either return 0 or an Exception, and applying the instanceof fix would also cause it to fail.
Math-91$$Fraction.comparTo returns 0 for some differente fractions$$If two different fractions evaluate to the same double due to limited precision, the compareTo methode returns 0 as if they were identical.  // value is roughly PI - 3.07e-18 Fraction pi1 = new Fraction(1068966896, 340262731);  // value is roughly PI + 1.936e-17 Fraction pi2 = new Fraction( 411557987, 131002976);  System.out.println(pi1.doubleValue() - pi2.doubleValue()); // exactly 0.0 due to limited IEEE754 precision System.out.println(pi1.compareTo(pi2)); // display 0 instead of a negative value
Math-92$$MathUtils.binomialCoefficient(n,k) fails for large results$$Probably due to rounding errors, MathUtils.binomialCoefficient(n,k) fails for results near Long.MAX_VALUE. The existence of failures can be demonstrated by testing the recursive property:           assertEquals(MathUtils.binomialCoefficient(65,32) + MathUtils.binomialCoefficient(65,33),                  MathUtils.binomialCoefficient(66,33));   Or by directly using the (externally calculated and hopefully correct) expected value:           assertEquals(7219428434016265740L, MathUtils.binomialCoefficient(66,33));   I suggest a nonrecursive test implementation along the lines of MathUtilsTest.java     /**      * Exact implementation using BigInteger and the explicit formula      * (n, k) == ((k-1)*...*n) / (1*...*(n-k))      */ 	public static long binomialCoefficient(int n, int k) { 		if (k == 0 || k == n) 			return 1; 		BigInteger result = BigInteger.ONE; 		for (int i = k + 1; i <= n; i++) { 			result = result.multiply(BigInteger.valueOf(i)); 		} 		for (int i = 1; i <= n - k; i++) { 			result = result.divide(BigInteger.valueOf(i)); 		} 		if (result.compareTo(BigInteger.valueOf(Long.MAX_VALUE)) > 0) { 			throw new ArithmeticException(                                 "Binomial coefficient overflow: " + n + ", " + k); 		} 		return result.longValue(); 	}   Which would allow you to test the expected values directly:           assertEquals(binomialCoefficient(66,33), MathUtils.binomialCoefficient(66,33));
Math-93$$MathUtils.factorial(n) fails for n >= 17$$The result of MathUtils.factorial( n ) for n = 17, 18, 19 is wrong, probably because of rounding errors in the double calculations. Replace the first line of MathUtilsTest.testFactorial() by         for (int i = 1; i <= 20; i++) { to check all valid arguments for the long result and see the failure. I suggest implementing a simple loop to multiply the long result - or even using a precomputed long[] - instead of adding logarithms.
Math-94$$MathUtils.gcd(u, v) fails when u and v both contain a high power of 2$$The test at the beginning of MathUtils.gcd(u, v) for arguments equal to zero fails when u and v contain high enough powers of 2 so that their product overflows to zero.         assertEquals(3 * (1<<15), MathUtils.gcd(3 * (1<<20), 9 * (1<<15))); Fix: Replace the test at the start of MathUtils.gcd()         if (u * v == 0) { by         if (u == 0 || v == 0) {
Math-95$$denominatorDegreeOfFreedom in FDistribution leads to IllegalArgumentsException in UnivariateRealSolverUtils.bracket$$We are using the FDistributionImpl from the commons.math project to do some statistical calculations, namely receiving the upper and lower boundaries of a confidence interval. Everything is working fine and the results are matching our reference calculations. However, the FDistribution behaves strange if a denominatorDegreeOfFreedom of 2 is used, with an alpha-value of 0.95. This results in an IllegalArgumentsException, stating: Invalid endpoint parameters:  lowerBound=0.0 initial=Infinity upperBound=1.7976931348623157E308 coming from org.apache.commons.math.analysis.UnivariateRealSolverUtils.bracket The problem is the 'initial' parameter to that function, wich is POSITIVE_INFINITY and therefore not within the boundaries. I already pinned down the problem to the FDistributions getInitialDomain()-method, wich goes like:         return getDenominatorDegreesOfFreedom() /                     (getDenominatorDegreesOfFreedom() - 2.0); Obviously, in case of denominatorDegreesOfFreedom == 2, this must lead to a division-by-zero, resulting in POSTIVE_INFINITY. The result of this operation is then directly passed into the UnivariateRealSolverUtils.bracket() - method as second argument.
Math-96$$Result of multiplying and equals for complex numbers is wrong$$Hi. The bug relates on complex numbers. The methods "multiply" and "equals" of the class Complex are involved. mathematic background:  (0,i) * (-1,0i) = (0,-i). little java program + output that shows the bug: -----------------------------------------------------------------------  import org.apache.commons.math.complex.*; public class TestProg {         public static void main(String[] args) {                  ComplexFormat f = new ComplexFormat();                 Complex c1 = new Complex(0,1);                 Complex c2 = new Complex(-1,0);                  Complex res = c1.multiply(c2);                 Complex comp = new Complex(0,-1);                  System.out.println("res:  "+f.format(res));                 System.out.println("comp: "+f.format(comp));                  System.out.println("res=comp: "+res.equals(comp));         } }   ----------------------------------------------------------------------- res:  -0 - 1i comp: 0 - 1i res=comp: false ----------------------------------------------------------------------- I think the "equals" should return "true". The problem could either be the "multiply" method that gives (-0,-1i) instead of (0,-1i), or if you think thats right, the equals method has to be modified. Good Luck Dieter
Math-97$$BrentSolver throws IllegalArgumentException$$I am getting this exception: java.lang.IllegalArgumentException: Function values at endpoints do not have different signs.  Endpoints: [-100000.0,1.7976931348623157E308]  Values: [0.0,-101945.04630982173] at org.apache.commons.math.analysis.BrentSolver.solve(BrentSolver.java:99) at org.apache.commons.math.analysis.BrentSolver.solve(BrentSolver.java:62) The exception should not be thrown with values  [0.0,-101945.04630982173] because 0.0 is positive. According to Brent Worden, the algorithm should stop and return 0 as the root instead of throwing an exception. The problem comes from this method:     public double solve(double min, double max) throws MaxIterationsExceededException,          FunctionEvaluationException {         clearResult();         verifyInterval(min, max);         double yMin = f.value(min);         double yMax = f.value(max);         // Verify bracketing         if (yMin * yMax >= 0)  {             throw new IllegalArgumentException             ("Function values at endpoints do not have different signs." +                     "  Endpoints: [" + min + "," + max + "]" +                      "  Values: [" + yMin + "," + yMax + "]");                }          // solve using only the first endpoint as initial guess         return solve(min, yMin, max, yMax, min, yMin);     } One way to fix it would be to add this code after the assignment of yMin and yMax:         if (yMin ==0 || yMax == 0)  {         	return 0;        	}
Math-98$$RealMatrixImpl#operate gets result vector dimensions wrong$$org.apache.commons.math.linear.RealMatrixImpl#operate tries to create a result vector that always has the same length as the input vector. This can result in runtime exceptions if the matrix is non-square and it always yields incorrect results if the matrix is non-square. The correct behaviour would of course be to create a vector with the same length as the row dimension of the matrix. Thus line 640 in RealMatrixImpl.java should read double[] out = new double[nRows]; instead of double[] out = new double[v.length];
Math-99$$MathUtils.gcd(Integer.MIN_VALUE, 0) should throw an Exception instead of returning Integer.MIN_VALUE$$The gcd method should throw an Exception for gcd(Integer.MIN_VALUE, 0), like for gcd(Integer.MIN_VALUE, Integer.MIN_VALUE). The method should only return nonnegative results.
Math-100$$AbstractEstimator: getCovariances() and guessParametersErrors() crash when having bound parameters$$the two methods getCovariances() and guessParametersErrors() from org.apache.commons.math.estimation.AbstractEstimator crash with ArrayOutOfBounds exception when some of the parameters are bound. The reason is that the Jacobian is calculated only for the unbound parameters. in the code you loop through all parameters. line #166: final int cols = problem.getAllParameters().length; should be replaced by:  final int cols = problem.getUnboundParameters().length; (similar changes could be done in guessParametersErrors()) the dissadvantage of the above bug fix is that what is returned to the user is an array with smaller size than the number of all parameters. Alternatively, you can have some logic in the code which writes zeros for the elements of the covariance matrix corresponding to the bound parameters
Math-101$$java.lang.StringIndexOutOfBoundsException in ComplexFormat.parse(String source, ParsePosition pos)$$The parse(String source, ParsePosition pos) method in the ComplexFormat class does not check whether the imaginary character is set or not which produces StringIndexOutOfBoundsException in the substring method : (line 375 of ComplexFormat) ...         // parse imaginary character         int n = getImaginaryCharacter().length();         startIndex = pos.getIndex();         int endIndex = startIndex + n;         if (source.substring(startIndex, endIndex).compareTo(             getImaginaryCharacter()) != 0) { ... I encoutered this exception typing in a JTextFied with ComplexFormat set to look up an AbstractFormatter. If only the user types the imaginary part of the complex number first, he gets this exception. Solution: Before setting to n length of the imaginary character, check if the source contains it. My proposal: ...         int n = 0;         if (source.contains(getImaginaryCharacter()))         n = getImaginaryCharacter().length(); ...		  F.S.
Math-102$$chiSquare(double[] expected, long[] observed) is returning incorrect test statistic$$ChiSquareTestImpl is returning incorrect chi-squared value. An implicit assumption of public double chiSquare(double[] expected, long[] observed) is that the sum of expected and observed are equal. That is, in the code: for (int i = 0; i < observed.length; i++)  {             dev = ((double) observed[i] - expected[i]);             sumSq += dev * dev / expected[i];         } this calculation is only correct if sum(observed)==sum(expected). When they are not equal then one must rescale the expected value by sum(observed) / sum(expected) so that they are. Ironically, it is an example in the unit test ChiSquareTestTest that highlights the error: long[] observed1 =  { 500, 623, 72, 70, 31 } ;         double[] expected1 =  { 485, 541, 82, 61, 37 } ;         assertEquals( "chi-square test statistic", 16.4131070362, testStatistic.chiSquare(expected1, observed1), 1E-10);         assertEquals("chi-square p-value", 0.002512096, testStatistic.chiSquareTest(expected1, observed1), 1E-9); 16.413 is not correct because the expected values do not make sense, they should be: 521.19403 581.37313  88.11940  65.55224  39.76119 so that the sum of expected equals 1296 which is the sum of observed. Here is some R code (r-project.org) which proves it: > o1 [1] 500 623  72  70  31 > e1 [1] 485 541  82  61  37 > chisq.test(o1,p=e1,rescale.p=TRUE)         Chi-squared test for given probabilities data:  o1  X-squared = 9.0233, df = 4, p-value = 0.06052 > chisq.test(o1,p=e1,rescale.p=TRUE) observed [1] 500 623  72  70  31 > chisq.test(o1,p=e1,rescale.p=TRUE) expected [1] 521.19403 581.37313  88.11940  65.55224  39.76119
Math-103$$ConvergenceException in normal CDF$$NormalDistributionImpl::cumulativeProbability(double x) throws ConvergenceException if x deviates too much from the mean. For example, when x=+/-100, mean=0, sd=1. Of course the value of the CDF is hard to evaluate in these cases, but effectively it should be either zero or one.
Math-104$$Special functions not very accurate$$The Gamma and Beta functions return values in double precision but the default epsilon is set to 10e-9. I think that the default should be set to the highest possible accuracy, as this is what I'd expect to be returned by a double precision routine. Note that the erf function already uses a call to Gamma.regularizedGammaP with an epsilon of 1.0e-15.
Math-105$$[math]  SimpleRegression getSumSquaredErrors$$getSumSquaredErrors returns -ve value. See test below: public void testSimpleRegression() { 		double[] y =  {  8915.102, 8919.302, 8923.502} ; 		double[] x =  { 1.107178495, 1.107264895, 1.107351295} ; 		double[] x2 =  { 1.107178495E2, 1.107264895E2, 1.107351295E2} ; 		SimpleRegression reg = new SimpleRegression(); 		for (int i = 0; i < x.length; i++)  { 			reg.addData(x[i],y[i]); 		} 		assertTrue(reg.getSumSquaredErrors() >= 0.0); // OK 		reg.clear(); 		for (int i = 0; i < x.length; i++)  { 			reg.addData(x2[i],y[i]); 		} 		assertTrue(reg.getSumSquaredErrors() >= 0.0); // FAIL 	}
Math-106$$[math] Function math.fraction.ProperFractionFormat.parse(String, ParsePosition) return illogical result$$Hello, I find illogical returned result from function "Fraction parse(String source,  ParsePostion pos)" (in class ProperFractionFormat of the Fraction Package) of  the Commons Math library. Please see the following code segment for more  details: " ProperFractionFormat properFormat = new ProperFractionFormat(); result = null; String source = "1 -1 / 2"; ParsePosition pos = new ParsePosition(0); //Test 1 : fail  public void testParseNegative(){    String source = "-1 -2 / 3";    ParsePosition pos = new ParsePosition(0);    Fraction actual = properFormat.parse(source, pos);    assertNull(actual); } // Test2: success public void testParseNegative(){    String source = "-1 -2 / 3";    ParsePosition pos = new ParsePosition(0);    Fraction actual = properFormat.parse(source, pos);  // return Fraction 1/3    assertEquals(1, source.getNumerator());    assertEquals(3, source.getDenominator()); } " Note: Similarly, when I passed in the following inputs:    input 2: (source = “1 2 / -3”, pos = 0)   input 3: ( source = ” -1 -2 / 3”, pos = 0) Function "Fraction parse(String, ParsePosition)" returned Fraction 1/3 (means  the result Fraction had numerator = 1 and  denominator = 3)for all 3 inputs  above. I think the function does not handle parsing the numberator/ denominator  properly incase input string provide invalid numerator/denominator.  Thank you!
